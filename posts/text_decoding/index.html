<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Decoding Strategies in Language Models | Rajan Ghimire</title>
<meta name="keywords" content="Natural Language Processing, PyTorch, Large Language Models">
<meta name="description" content="Exploring and implementing text decoding strategies in PyTorch">
<meta name="author" content="Rajan Ghimire">
<link rel="canonical" href="https://R4j4n.github.io/blogs/posts/text_decoding/">
<link crossorigin="anonymous" href="/blogs/assets/css/stylesheet.54ccf0f50e0bb5bc885c6d275474800d82dde88cc22f647be5b4e4ca14d7176f.css" integrity="sha256-VMzw9Q4LtbyIXG0nVHSADYLd6IzCL2R75bTkyhTXF28=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/blogs/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://R4j4n.github.io/blogs/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://R4j4n.github.io/blogs/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://R4j4n.github.io/blogs/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://R4j4n.github.io/blogs/apple-touch-icon.png">
<link rel="mask-icon" href="https://R4j4n.github.io/blogs/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css"
    integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js"
    integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY"
    crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js"
    integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>

>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false }
            ]
        });
    });
</script>
<meta property="og:title" content="Decoding Strategies in Language Models" />
<meta property="og:description" content="Exploring and implementing text decoding strategies in PyTorch" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://R4j4n.github.io/blogs/posts/text_decoding/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-09-15T00:00:00+00:00" />
<meta property="article:modified_time" content="2023-09-15T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Decoding Strategies in Language Models"/>
<meta name="twitter:description" content="Exploring and implementing text decoding strategies in PyTorch"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://R4j4n.github.io/blogs/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Decoding Strategies in Language Models",
      "item": "https://R4j4n.github.io/blogs/posts/text_decoding/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Decoding Strategies in Language Models",
  "name": "Decoding Strategies in Language Models",
  "description": "Exploring and implementing text decoding strategies in PyTorch",
  "keywords": [
    "Natural Language Processing", "PyTorch", "Large Language Models"
  ],
  "articleBody": "The Auto-regression and Decoding Strategies Auto-regressive language generation assumes that the element of the output sequence at time-step $t$ is determined by the input sequence and time-steps before $t$. $$ P\\left(w_{1: T} \\mid W_0\\right)=\\prod_{t=1}^T P\\left(w_t \\mid w_{1: t-1}, W_0\\right), \\text { with } w_{1: 0}=\\emptyset $$\nwhere $W_0$ is the input sequence; $W_t$ is the word at timestep $t$; T is determined by the position of a token. source\nLanguage models, especially those like the GPT and LLaMa, are auto-regressive. This means that they generate sequences one item at a time, using the previously generated items as context for generating the next item. When the Language model is given a series of tokens, it tries to guess what comes next. It does this by creating a list of discrete probability distributions for each potential next token using softmax. The decoding strategy is applied to select the next token(s) from this distribution. Due to the sequential structure of language, tokens must not only be contextually appropriate but also organically flow to create cohesive sentences and paragraphs. Decoding strategies help in selecting tokens that adhere to the patterns and structures of the language. Also, decoding strategies help strike a balance between deterministic outputs and creative, diverse responses.\nThe true beauty of these strategies is best appreciated when they are built from the ground up, understanding each decision and line of code that goes into making them work.\nIn this blog, we aim to demystify these decoding strategies. And how do we plan to do that? By doing everything from scratch! We won’t be relying on pre-built libraries or ready-made functions\nDecoding Strategies Before diving into the theoretical and practical aspects of each of the decoding strategies, let’s write a Sampler base class that abstracts the common utilities and operations, ensuring that subsequent decoding strategies can be implemented in a more streamlined manner. Its core functionalities are encoding text into token IDs, decoding these IDs back into text, and obtaining the next token’s probability.\nAdditionally, the class offers the ability to visually represent scores of tokens through the plot_scores method, displaying a color-coded bar graph of top token probabilities, thus providing an intuitive overview of the model’s predictions.\nimport torch import plotly.graph_objects as go from transformers import AutoTokenizer, AutoModelForCausalLM # torch.manual_seed(0) class Sampler: def __init__(self , model_name : str ='gpt2-medium') -\u003e None: self.device = 'cuda' if torch.cuda.is_available() else 'cpu' self.tokenizer = AutoTokenizer.from_pretrained(model_name) self.model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cpu\").to(self.device) def encode(self, text): return self.tokenizer.encode(text, return_tensors='pt').to(self.device) def decode(self, ids): return self.tokenizer.decode(ids) def get_next_token_prob(self, input_ids: torch.Tensor): with torch.no_grad(): logits = self.model(input_ids=input_ids).logits logits = logits[0, -1, :] return logits def plot_scores(self, scores, title, samples): top_indices = torch.argsort(scores, descending=True)[:samples] tokens = [self.decode(idx) for idx in top_indices] if self.device == \"cpu\": top_probs = scores[top_indices].numpy() else: top_probs = scores[top_indices].cpu().numpy() colors = ['#E95B68', '#C4C956', '#58BB7B', '#CAC1C5', '#87601F', '#F7311B', '#C53D39', '#38658F', '#242ABC', '#9DA52F', '#329018', '#D415C5', '#6DCE59', '#ADF212', '#9CF042'] colors = colors[0:len(top_indices)] fig = go.Figure(data=[ go.Bar(x=tokens, y=top_probs, marker_color=colors, textposition='auto') ]) fig.update_layout(title=title) fig.show() Greedy Search Decoding: Greedy search is the simplest decoding method. It takes the token with the highest conditional probability from the vocabulary V.\nAt each step, it selects the token with the highest probability and adds it to the sequence. It is continued until an end token is met or a maximum sequence length is reached.\nimport torch class GreedySampler(Sampler): def __call__(self, prompt, max_new_tokens=10): predictions = [] result = prompt # generate until max_len for i in range(max_new_tokens): input_ids = self.encode(result) next_token_probs = self.get_next_token_prob(input_ids=input_ids) # choose the token with the highest probability id = torch.argmax(next_token_probs, dim=-1).item() # convert to token and add new token to text result += self.decode(id) predictions.append(next_token_probs[id].item()) return result We will be using the same SAMPLE_TEXT and an equal maximum sequence length when testing each decoding strategy. This we can easily assess the performance of each decoding strategy.\nSAMPLE_TEXT = \"Artificial Intelligence is the intelligence possessed by the” greedy_sampler = GreedySampler() result = greedy_sampler(prompt=SAMPLE_TEXT,max_new_tokens=16) print(result) Output: Artificial Intelligence is the intelligence possessed by the machines that run the world. It is the intelligence that is capable of understanding and Greedy search is advantageous due to its simplicity and computational efficiency. By tracking only the most probable sequence, it requires less memory and computational power. However, there are some major drawbacks to this approach:\nMyopia: Greedy algorithms focus solely on the best immediate option, overlooking potential long-term benefits. It’s like a hiker only choosing the nearest hilltop without aiming for the tallest mountain. Repetitiveness: These algorithms often produce generic and monotonous outputs. By always picking the most probable word, it tends to favor frequently used phrases, leading to predictable results. Error Magnification: Greedy search can’t rectify its mistakes. Once it makes a less-than-ideal choice, every subsequent decision is influenced. Beam Search: Beam search is an advanced decoding algorithm designed to optimize sequence generation.\nUnlike the greedy search that might consider only the most probable word at each step, beam search simultaneously tracks multiple potential sequences, determined by a parameter known as the ‘beam width’.\nAt every stage, it expands each sequence by appending all possible subsequent words. From this pool of new sequences, the top ‘k’ sequences will be selected, where ‘k’ signifies the beam width. This ensures that the algorithm doesn’t just focus on immediate high-probability words, but also on the overall sequence probability. In essence, beam search aims to balance between finding the most probable sequence and computational efficiency by considering multiple sequences but not every possible one.\nWhile the outcome of beam search decoding is notably more fluent, it may contain repeated sequences of the same words. To address this repetition, the concept of “n-gram penalty” can be used. This technique ensures that any given n-gram only appears once. If a n-gram sequence is generated to put in sequence and that n-gram is already present in the sequence, then its probability is set to zero.\nLet’s take an example where we search through the graph to obtain a sequence length of 4, pruning all but the number of parameterized beams which is 2 at each time step. Source\n![2023-09-13_17-35.png] (/blogs/img/decoding/2023-09-13_17-35.png)\nWith the theoretical foundation in place, it is time to transition to the practical side of things. Let’s start by implementing beam search in PyTorch. This code is directly derived from this Repo.\nclass Beam: def __init__(self, device, size, input_ids, score, output=None): self.device = device self.size = size # num_beam self.input_ids = input_ids.to(self.device) self.score = score self.output = output.to(self.device) if output is not None else None # get input_ids def get_current_state(self): return self.input_ids # get probability of the sentence def get_score(self): return self.score # create a new instance of Beam class after the top k selection def extend(self, token_id, score): new_input_ids = torch.cat([self.input_ids, token_id.unsqueeze(0)], dim=-1) new_score = self.score * score new_output = torch.cat([self.output, token_id.unsqueeze(0)], dim=-1) if self.output is not None else new_input_ids return Beam(self.device, self.size, new_input_ids, new_score, new_output) class BeamSampler(Sampler): def beam_decode(self, ids): return self.tokenizer.decode(ids.squeeze().tolist()) # Get the top k id with the greatest probability @staticmethod def get_topk(prob, k=1): scores, token_ids = torch.topk(prob, k=k, dim=-1) return scores, token_ids def __call__(self, prompt, max_new_tokens=10, num_beam=1): input_ids = self.encode(prompt) # initialize the beam # Ensure this initializes only `num_beam` beams beams = [Beam(self.device, num_beam, input_ids, 1) for _ in range(num_beam)] # loop until the maximum length is reached for i in range(max_new_tokens): all_next_token_prob = [] for beam in beams: next_token_prob = self.get_next_token_prob(input_ids=beam.get_current_state()) all_next_token_prob.append(next_token_prob) # With this all_topk_scores = [] all_topk_token_ids = [] for prob in all_next_token_prob: scores, token_ids = self.get_topk(prob, k=num_beam) all_topk_scores.append(scores) all_topk_token_ids.append(token_ids) all_topk_scores = torch.stack(all_topk_scores) all_topk_token_ids = torch.stack(all_topk_token_ids) new_beams = [] # Then, when accessing them: for j, beam in enumerate(beams): for k in range(num_beam): score = all_topk_scores[j][k].item() token_id = all_topk_token_ids[j][k].unsqueeze(0) new_beam = beam.extend(token_id, score) new_beams.append(new_beam) beams = sorted(new_beams, key=lambda b: b.get_score(), reverse=True)[:num_beam] generated_text = self.beam_decode(beams[0].output[:, len(input_ids[0]):]) return prompt + generated_text beam_sampler = BeamSampler() result = beam_sampler(prompt=SAMPLE_TEXT,max_new_tokens=16 , num_beam=1) print(result) result = beam_sampler(prompt=SAMPLE_TEXT,max_new_tokens=16 , num_beam=10) print(result) Output: Artificial Intelligence is the intelligence possessed by the machines that run the world. It is the intelligence that is capable of understanding and Artificial Intelligence is the intelligence possessed by the machines used to create those machines.[1e]] As such we may expect intelligent When we used a beam size of 1, the generated text was more deterministic, relying heavily on the most probable tokens at each step. On the other hand, increasing the beam size to 10 allowed for a broader exploration of possibilities, which resulted in coherent text with a degree of variation.\nHowever, there are a few drawbacks to this approach:\nIt requires more computational resources than greedy search, as it needs to maintain and calculate probabilities for ‘k’ sequences at each step which can amplify inference time. It also doesn’t guarantee finding the most probable sequence, especially if the beam width ‘k’ is too small compared to the size of the vocabulary. Temperature Sampling $$ P\\left(x_{i} \\mid x_{1: i-1}\\right)=\\frac{\\exp \\left(u_{i} / t\\right)}{\\sum_{j} \\exp \\left(u_{j} / t\\right)} $$\nRandom sampling can be very unpredictable. We can enhance the predictability and control over random sampling using temperature. Temperature serves as a mechanism to control the likelihood of selecting certain tokens over others. The temperature acts as a hyperparameter that can either amplify or reduce the randomness in the sampling process, providing a balance between unpredictability and determinism.\nA temperature value set between 0 and 1 can adjust this probability. Specifically, as the temperature approaches 1, it tends to retain the original randomness of sampling. Conversely, as the temperature nears 0, the process becomes more deterministic like greedy decoding. Let’s see how this works on code:\nclass RandomTempSampler(Sampler): def __call__(self, prompt, max_new_tokens=10 , temp : float = 0.5): predictions = [] result = prompt # generate until max_len for i in range(max_new_tokens): input_ids = self.encode(result) next_token_probs = self.get_next_token_prob(input_ids=input_ids) # apply temp before softmax (sharper logits) next_token_probs /= temp # convert logits to scores scores = softmax(next_token_probs, dim=-1) # sample from scores id = torch.multinomial(scores, num_samples=1).item() # convert to token and add new token to text result += self.decode(id) # keep track of scores for next token predictions.append(scores[id].item()) return result def sample_plot(self,prompt,temp:float = 0.5): input_ids = self.encode(prompt) next_token_probs = self.get_next_token_prob(input_ids=input_ids) next_token_probs /= temp scores = softmax(next_token_probs, dim=0) self.plot_scores(scores=scores,title=f\"Tempreature : {temp}\",samples=10) random_tempreature = RandomTempSampler() print(random_tempreature(prompt=SAMPLE_TEXT,max_new_tokens=16,temp=0.1)) print(random_tempreature(prompt=SAMPLE_TEXT,max_new_tokens=16,temp=0.9)) Output: Artificial Intelligence is the intelligence possessed by the machines that run the world. It is the intelligence that is able to understand and Artificial Intelligence is the intelligence possessed by the computer when the parts of it that commands it do the work are no longer useful Let’s look at the probability distribution when we change the temperature of the softmax function.\nWhen Temperature is set to 0.1 :\nrandom_tempreature.sample_plot(prompt=SAMPLE_TEXT,temp=0.1)\nWhen Temperature is set to 0.9 :\nrandom_tempreature.sample_plot(prompt=SAMPLE_TEXT,temp=0.1)\nIt is evident from the plots that by increasing the temperature, a skewed distribution is turned into a more uniform distribution. This will increase entropy and add more randomness. This is why, when we significantly raise the temperature, it introduces more randomness in the model, which can lead to unusual outputs.\nTop-K sampling Top-K sampling is another technique in language generation. It works by ensuring that only the most probable tokens (the top K tokens) have a chance at being selected in the next step.\nThis method narrows down the choices to the K most probable ones, and at each generation step, tokens are selected from this restricted pool. If we set $K=1$, it simply becomes a greedy search, choosing the most probable word each time. Conversely, if $k=len(vocabulary(v))$, it’s the same as pure sampling, considering every word equally. We can also introduce the concept of temperature in top-k sampling, which allows for the adjustment of randomness in top-k selections.\nHowever, it’s crucial to note a limitation of this method. Using a constant value for k is not optimal for all contexts. In some situations, there can be many equally good options for the next word, making the distribution head flat. In other contexts, a few tokens dominate the probability distribution. A small k might result in generic text, and a large k could include unsuitable word candidates.\nAs we have now laid the theoretical groundwork, let’s implement top-k sampling with temperature.\nclass TOPKsampler(Sampler): def __call__(self, prompt, max_new_tokens=10 ,top_k = 1 ,temp : float = 0.5): predictions = [] result = prompt # generate until max_len for i in range(max_new_tokens): # convert words to tokens input_ids = self.encode(result) next_token_probs = self.get_next_token_prob(input_ids=input_ids) next_token_probs = next_token_probs / temp indices_to_remove = next_token_probs \u003c torch.topk(next_token_probs, top_k)[0][..., -1, None] new_logits = torch.clone(next_token_probs) new_logits[indices_to_remove] = float('-inf') # convert logits to scores scores = softmax(new_logits, dim=-1) # Use modified logits # sample from scores id = torch.multinomial(scores, num_samples=1).item() # convert to token and add new token to text result += self.decode(id) # keep track of scores for next token predictions.append(scores[id].item()) return result def sample_plot(self,prompt ,top_k = 5 ,temp : float = 0.5): input_ids = self.encode(prompt) next_token_probs = self.get_next_token_prob(input_ids=input_ids) next_token_probs = next_token_probs / temp # Remove all tokens with a probability less than the last token of the top-k. indices_to_remove = next_token_probs \u003c torch.topk(next_token_probs, top_k)[0][..., -1, None] new_logits = torch.clone(next_token_probs) new_logits[indices_to_remove] = float('-inf') # convert logits to scores scores = softmax(new_logits, dim=-1) # Use modified logits self.plot_scores(scores,title=f\"Tempreature : {temp} Top k : {top_k}\" , samples = top_k + int(math.sqrt(top_k))) topksampler = TOPKsampler() result = topksampler( prompt=SAMPLE_TEXT, max_new_tokens=32, top_k= 10, temp=0.5 ) print(result) Output: Artificial Intelligence is the intelligence possessed by the computers that control our lives. Artificial Intelligence is also the intelligence that runs our economy, our government, and our economy's own intelligence. Let’s look at the probability distribution when we change the temperature of the softmax function among top-k tokens.\ntopksampler.sample_plot(prompt=SAMPLE_TEXT,top_k=10,temp=0.1)\ntopksampler.sample_plot(prompt=SAMPLE_TEXT,top_k=10,temp=0.9)\nNucleus(top-p) sampling Nucleus sampling is similar to Top-K sampling. Instead selecting the most probable K words, nucleus sampling selects the smallest set of words whose combined probabilities surpass a threshold, p. This method allows for a dynamic number of candidate words, which can expand or contract based on the model’s confidence in the vocabulary.\nNucleus sampling first picks a subset of the vocabulary $V^{(p)}$⊂$V$, where $V^{(p)}$ is smallest set of tokens such that\n$$ \\sum_{x_{i} \\in V^{(p)}} P\\left(x_{i} \\mid x_{1: i-1}\\right) \\geq p $$\nThat is, we pick the highest probable tokens until the sum of their probabilities is less than p. Source\nclass NucleusSampler(Sampler): def __call__(self, prompt, max_new_tokens=10 , p : float = 0.7): predictions = [] result = prompt # generate until max_len for i in range(max_new_tokens): # convert words to tokens input_ids = self.encode(result) next_token_probs = self.get_next_token_prob(input_ids=input_ids) sorted_logits, sorted_indices = torch.sort(next_token_probs, descending=True) cumulative_probs = torch.cumsum(softmax(sorted_logits, dim=-1), dim=-1) # Remove tokens with cumulative probability above the threshold sorted_indices_to_remove = cumulative_probs \u003e p \"\"\" When we determine which tokens to remove based on this mask, it's important to note that as soon as the cumulative probability crosses the threshold `p`, all the subsequent tokens will also have cumulative probabilities greater than `p` (because the probabilities are sorted in descending order). The logic here is to also exclude the very first token that caused the cumulative sum to cross the threshold, and this is achieved by shifting the mask to the right. By doing this shift and ensuring the first token that exceeds the threshold is included in the removal list, we're adhering to the true spirit of top-p sampling: we're including in the final consideration only those tokens whose cumulative sum is less than or equal to `p`. \"\"\" # Shift the indices to the right to keep also the first token above the threshold sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone() sorted_indices_to_remove[..., 0] = 0 indices_to_remove = sorted_indices[sorted_indices_to_remove] new_logits = torch.clone(next_token_probs) new_logits[indices_to_remove] = float('-inf') # convert logits to scores scores = softmax(new_logits, dim=-1) # Use modified logits # sample from scores id = torch.multinomial(scores, num_samples=1).item()\\ # convert to token and add new token to text result += self.decode(id) # keep track of scores for next token predictions.append(scores[id].item()) return result def sample_plot(self,prompt, p: float): input_ids = self.encode(prompt) next_token_probs = self.get_next_token_prob(input_ids=input_ids) sorted_logits, sorted_indices = torch.sort(next_token_probs, descending=True) cumulative_probs = torch.cumsum(softmax(sorted_logits, dim=-1), dim=-1) # Remove tokens with cumulative probability above the threshold sorted_indices_to_remove = cumulative_probs \u003e p # Shift the indices to the right to keep also the first token above the threshold sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone() sorted_indices_to_remove[..., 0] = 0 indices_to_remove = sorted_indices[sorted_indices_to_remove] new_logits = torch.clone(next_token_probs) new_logits[indices_to_remove] = float('-inf') # convert logits to scores scores = softmax(new_logits, dim=-1) self.plot_scores(scores,title=f\"P : {p}\", samples=10) nssammpler = NucleusSampler() result = nssammpler(prompt=SAMPLE_TEXT,max_new_tokens=16,p=0.8) print(result) Output: Artificial Intelligence is the intelligence possessed by the sentient, intellectual, or creative faculties, such as the abilities of people with hands Let’s look at the number of candidate words distribution when we change the p value.\nnssammpler.sample_plot(prompt=SAMPLE_TEXT,p=0.8)\nnssammpler.sample_plot(prompt*=SAMPLE_TEXT,p=0.1)\nWe can see from the plots that by increasing the p, the candidate words distribution converts from more uniform distribution to a skewed distribution.\nAll of the codes used in this repository can be found HERE. From the simplicity of Greedy Search Decoding to the more sophisticated approaches like Beam Search and various sampling techniques such as Temperature Sampling, Top-K, and Nucleus Sampling, each offers unique advantages to address specific challenges. By understanding and implementing these methods, we can tailor their model’s output to align closer with the desired outcome.\nReferences https://www.kaggle.com/code/sajjadayobi360/how-to-generate-text-using-language-models https://vitalflux.com/greedy-search-vs-beam-search-decoding-concepts-examples/#:~:text=complex decoding methods.-,Drawbacks of Greedy Search Decoding Method,term implications of its choices. https://medium.com/@jessica_lopez/understanding-greedy-search-and-beam-search-98c1e3cd821d https://towardsdatascience.com/decoding-strategies-that-you-need-to-know-for-response-generation-ba95ee0faadc#:~:text=Random Sampling with Temperature,1%2C there is no effect. https://windysavage.github.io/Decoding-strategies-in-text-generation/ https://peterchng.com/blog/2023/05/02/token-selection-strategies-top-k-top-p-and-temperature/ ",
  "wordCount" : "2855",
  "inLanguage": "en",
  "datePublished": "2023-09-15T00:00:00Z",
  "dateModified": "2023-09-15T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Rajan Ghimire"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://R4j4n.github.io/blogs/posts/text_decoding/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Rajan Ghimire",
    "logo": {
      "@type": "ImageObject",
      "url": "https://R4j4n.github.io/blogs/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://R4j4n.github.io/blogs/" accesskey="h" title="Rajan Ghimire (Alt + H)">Rajan Ghimire</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://R4j4n.github.io/blogs/" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="https://R4j4n.github.io/blogs/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://R4j4n.github.io/blogs/">Home</a>&nbsp;»&nbsp;<a href="https://R4j4n.github.io/blogs/posts/">Posts</a></div>
    <h1 class="post-title">
      Decoding Strategies in Language Models
    </h1>
    <div class="post-description">
      Exploring and implementing text decoding strategies in PyTorch
    </div>
    <div class="post-meta"><span title='2023-09-15 00:00:00 +0000 UTC'>September 15, 2023</span>&nbsp;·&nbsp;14 min&nbsp;·&nbsp;Rajan Ghimire

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#the-auto-regression-and-decoding-strategies" aria-label="The Auto-regression and Decoding Strategies">The Auto-regression and Decoding Strategies</a></li>
                <li>
                    <a href="#decoding-strategies" aria-label="Decoding Strategies">Decoding Strategies</a><ul>
                        
                <li>
                    <a href="#greedy-search-decoding" aria-label="Greedy Search Decoding:">Greedy Search Decoding:</a></li>
                <li>
                    <a href="#beam-search" aria-label="Beam Search:">Beam Search:</a></li>
                <li>
                    <a href="#temperature-sampling" aria-label="Temperature Sampling">Temperature Sampling</a></li>
                <li>
                    <a href="#top-k-sampling" aria-label="Top-K sampling">Top-K sampling</a></li>
                <li>
                    <a href="#nucleustop-p-sampling" aria-label="Nucleus(top-p) sampling">Nucleus(top-p) sampling</a></li></ul>
                </li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="the-auto-regression-and-decoding-strategies">The Auto-regression and Decoding Strategies<a hidden class="anchor" aria-hidden="true" href="#the-auto-regression-and-decoding-strategies">#</a></h2>
<p><img loading="lazy" src="https://www.wallpaperflare.com/static/95/347/681/colorful-gray-yellow-blue-wallpaper.jpg" alt="https://www.wallpaperflare.com/static/95/347/681/colorful-gray-yellow-blue-wallpaper.jpg"  />
</p>
<p>Auto-regressive language generation assumes that the element of the output sequence at time-step $t$ is determined by the input sequence and time-steps before $t$.
$$
P\left(w_{1: T} \mid W_0\right)=\prod_{t=1}^T P\left(w_t \mid w_{1: t-1}, W_0\right), \text { with } w_{1: 0}=\emptyset
$$</p>
<p>where $W_0$ is the input sequence; $W_t$ is the word at timestep $t$; T is determined by the position of a token. <a href="https://windysavage.github.io/Decoding-strategies-in-text-generation/">source</a></p>
<p>Language models, especially those like the GPT and LLaMa, are auto-regressive. This means that they generate sequences one item at a time, using the previously generated items as context for generating the next item. When the Language model is given a series of tokens, it tries to guess what comes next. It does this by creating a list of <strong>discrete probability distributions</strong> for each potential next token using softmax. The decoding strategy is applied to select the next token(s) from this distribution. Due to the sequential structure of language, tokens must not only be contextually appropriate but also organically flow to create cohesive sentences and paragraphs. Decoding strategies help in selecting tokens that adhere to the patterns and structures of the language. Also, decoding strategies help strike a balance between deterministic outputs and creative, diverse responses.</p>
<p>The true beauty of these strategies is best appreciated when they are built from the ground up, understanding each decision and line of code that goes into making them work.</p>
<p>In this blog, we aim to demystify these decoding strategies. And how do we plan to do that? <strong>By doing everything from scratch!</strong> We won&rsquo;t be relying on pre-built libraries or ready-made functions</p>
<h2 id="decoding-strategies">Decoding Strategies<a hidden class="anchor" aria-hidden="true" href="#decoding-strategies">#</a></h2>
<p>Before diving into the theoretical and practical aspects of each of the decoding strategies, let&rsquo;s write a <code>Sampler</code> base class that abstracts the common utilities and operations, ensuring that subsequent decoding strategies can be implemented in a more streamlined manner. Its core functionalities are encoding text into token IDs, decoding these IDs back into text, and obtaining the next token&rsquo;s probability.</p>
<p>Additionally, the class offers the ability to visually represent scores of tokens through the <code>plot_scores</code> method, displaying a color-coded bar graph of top token probabilities, thus providing an intuitive overview of the model&rsquo;s predictions.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> plotly.graph_objects <span style="color:#66d9ef">as</span> go
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> AutoTokenizer, AutoModelForCausalLM
</span></span><span style="display:flex;"><span><span style="color:#75715e"># torch.manual_seed(0)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Sampler</span>:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self , model_name : str <span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gpt2-medium&#39;</span>) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>device <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;cuda&#39;</span> <span style="color:#66d9ef">if</span> torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>is_available() <span style="color:#66d9ef">else</span> <span style="color:#e6db74">&#39;cpu&#39;</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>tokenizer <span style="color:#f92672">=</span> AutoTokenizer<span style="color:#f92672">.</span>from_pretrained(model_name)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>model <span style="color:#f92672">=</span> AutoModelForCausalLM<span style="color:#f92672">.</span>from_pretrained(model_name)<span style="color:#f92672">.</span>to(<span style="color:#e6db74">&#34;cpu&#34;</span>)<span style="color:#f92672">.</span>to(self<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">encode</span>(self, text):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>tokenizer<span style="color:#f92672">.</span>encode(text, return_tensors<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;pt&#39;</span>)<span style="color:#f92672">.</span>to(self<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">decode</span>(self, ids):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>tokenizer<span style="color:#f92672">.</span>decode(ids)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_next_token_prob</span>(self, input_ids: torch<span style="color:#f92672">.</span>Tensor):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>            logits <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>model(input_ids<span style="color:#f92672">=</span>input_ids)<span style="color:#f92672">.</span>logits
</span></span><span style="display:flex;"><span>        logits <span style="color:#f92672">=</span> logits[<span style="color:#ae81ff">0</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, :]
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> logits
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">plot_scores</span>(self, scores, title, samples):
</span></span><span style="display:flex;"><span>        top_indices <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>argsort(scores, descending<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)[:samples]
</span></span><span style="display:flex;"><span>        tokens <span style="color:#f92672">=</span> [self<span style="color:#f92672">.</span>decode(idx) <span style="color:#66d9ef">for</span> idx <span style="color:#f92672">in</span> top_indices]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>device <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;cpu&#34;</span>:
</span></span><span style="display:flex;"><span>            top_probs <span style="color:#f92672">=</span> scores[top_indices]<span style="color:#f92672">.</span>numpy()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            top_probs <span style="color:#f92672">=</span> scores[top_indices]<span style="color:#f92672">.</span>cpu()<span style="color:#f92672">.</span>numpy()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        colors <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;#E95B68&#39;</span>, <span style="color:#e6db74">&#39;#C4C956&#39;</span>, <span style="color:#e6db74">&#39;#58BB7B&#39;</span>, <span style="color:#e6db74">&#39;#CAC1C5&#39;</span>, <span style="color:#e6db74">&#39;#87601F&#39;</span>, <span style="color:#e6db74">&#39;#F7311B&#39;</span>, 
</span></span><span style="display:flex;"><span>                  <span style="color:#e6db74">&#39;#C53D39&#39;</span>, <span style="color:#e6db74">&#39;#38658F&#39;</span>, <span style="color:#e6db74">&#39;#242ABC&#39;</span>, <span style="color:#e6db74">&#39;#9DA52F&#39;</span>, <span style="color:#e6db74">&#39;#329018&#39;</span>, <span style="color:#e6db74">&#39;#D415C5&#39;</span>, 
</span></span><span style="display:flex;"><span>                  <span style="color:#e6db74">&#39;#6DCE59&#39;</span>, <span style="color:#e6db74">&#39;#ADF212&#39;</span>, <span style="color:#e6db74">&#39;#9CF042&#39;</span>]
</span></span><span style="display:flex;"><span>        colors <span style="color:#f92672">=</span> colors[<span style="color:#ae81ff">0</span>:len(top_indices)]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        fig <span style="color:#f92672">=</span> go<span style="color:#f92672">.</span>Figure(data<span style="color:#f92672">=</span>[
</span></span><span style="display:flex;"><span>            go<span style="color:#f92672">.</span>Bar(x<span style="color:#f92672">=</span>tokens, y<span style="color:#f92672">=</span>top_probs, marker_color<span style="color:#f92672">=</span>colors, textposition<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;auto&#39;</span>)
</span></span><span style="display:flex;"><span>        ])
</span></span><span style="display:flex;"><span>        fig<span style="color:#f92672">.</span>update_layout(title<span style="color:#f92672">=</span>title)
</span></span><span style="display:flex;"><span>        fig<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><h3 id="greedy-search-decoding">Greedy Search Decoding:<a hidden class="anchor" aria-hidden="true" href="#greedy-search-decoding">#</a></h3>
<p>Greedy search is the simplest decoding method. It takes the token with the highest conditional probability from the vocabulary V.</p>
<p><img loading="lazy" src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*Nh_Q0_s1KiiFitZVafqb3g.png" alt="https://miro.medium.com/v2/resize:fit:640/format:webp/1*Nh_Q0_s1KiiFitZVafqb3g.png"  />
</p>
<p>At each step, it selects the token with the highest probability and adds it to the sequence. It is continued until an end token is met or a maximum sequence length is reached.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">GreedySampler</span>(Sampler):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __call__(self, prompt, max_new_tokens<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>):
</span></span><span style="display:flex;"><span>        predictions <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>        result <span style="color:#f92672">=</span> prompt
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># generate until max_len</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(max_new_tokens):
</span></span><span style="display:flex;"><span>            input_ids <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>encode(result)
</span></span><span style="display:flex;"><span>            next_token_probs <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>get_next_token_prob(input_ids<span style="color:#f92672">=</span>input_ids)
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># choose the token with the highest probability</span>
</span></span><span style="display:flex;"><span>            id <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>argmax(next_token_probs, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>item()
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># convert to token and add new token to text</span>
</span></span><span style="display:flex;"><span>            result <span style="color:#f92672">+=</span> self<span style="color:#f92672">.</span>decode(id)
</span></span><span style="display:flex;"><span>            predictions<span style="color:#f92672">.</span>append(next_token_probs[id]<span style="color:#f92672">.</span>item())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> result
</span></span></code></pre></div><p>We will be using the same <code>SAMPLE_TEXT</code> and an equal maximum sequence length when testing each decoding strategy. This we can easily assess the performance of each decoding strategy.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>SAMPLE_TEXT <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Artificial Intelligence is the intelligence possessed by the”</span>
</span></span><span style="display:flex;"><span>greedy_sampler <span style="color:#f92672">=</span> GreedySampler()
</span></span><span style="display:flex;"><span>result <span style="color:#f92672">=</span> greedy_sampler(prompt<span style="color:#f92672">=</span>SAMPLE_TEXT,max_new_tokens<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>)
</span></span><span style="display:flex;"><span>print(result)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>Output: 
</span></span><span style="display:flex;"><span>Artificial Intelligence <span style="color:#f92672">is</span> the intelligence possessed by the machines that run the world<span style="color:#f92672">.</span> It <span style="color:#f92672">is</span> the intelligence that <span style="color:#f92672">is</span> capable of understanding <span style="color:#f92672">and</span>
</span></span></code></pre></div><p>Greedy search is advantageous due to its simplicity and computational efficiency. By tracking only the most probable sequence, it requires less memory and computational power. However, there are some major drawbacks to this approach:</p>
<ol>
<li><strong>Myopia</strong>: Greedy algorithms focus solely on the best immediate option, overlooking potential long-term benefits. It&rsquo;s like a hiker only choosing the nearest hilltop without aiming for the tallest mountain.</li>
<li><strong>Repetitiveness</strong>: These algorithms often produce generic and monotonous outputs. By always picking the most probable word, it tends to favor frequently used phrases, leading to predictable results.</li>
<li><strong>Error Magnification</strong>: Greedy search can&rsquo;t rectify its mistakes. Once it makes a less-than-ideal choice, every subsequent decision is influenced.</li>
</ol>
<h3 id="beam-search">Beam Search:<a hidden class="anchor" aria-hidden="true" href="#beam-search">#</a></h3>
<p>Beam search is an advanced decoding algorithm designed to optimize sequence generation.</p>
<p>Unlike the greedy search that might consider only the most probable word at each step, beam search simultaneously tracks multiple potential sequences, determined by a parameter known as the <strong>&lsquo;beam width&rsquo;</strong>.</p>
<p>At every stage, it expands each sequence by appending all possible subsequent words. From this pool of new sequences, the top &lsquo;k&rsquo; sequences will be selected, where &lsquo;k&rsquo; signifies the beam width. This ensures that the algorithm doesn&rsquo;t just focus on immediate high-probability words, but also on the overall sequence probability. In essence, beam search aims to balance between finding the most probable sequence and computational efficiency by considering multiple sequences but not every possible one.</p>
<p>While the outcome of beam search decoding is notably more fluent, it may contain repeated sequences of the same words. To address this repetition, the concept of &ldquo;n-gram penalty&rdquo;  can be used. This technique ensures that any given n-gram only appears once. If a n-gram sequence is generated to put in sequence and that n-gram is already present in the sequence, then its probability is set to zero.</p>
<p>Let&rsquo;s take an example where we search through the graph to obtain a sequence length of 4, pruning all but the number of parameterized beams which is 2 at each time step. <a href="https://cjlovering.github.io/posts/beam-search/index.html">Source</a></p>
<p>![2023-09-13_17-35.png]
(/blogs/img/decoding/2023-09-13_17-35.png)</p>
<p>With the theoretical foundation in place, it is time to transition to the practical side of things. Let&rsquo;s start by implementing beam search in PyTorch. This code is directly derived from this <a href="https://github.com/HiepThanh0510/text-generation-pytorch">Repo</a>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Beam</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, device, size, input_ids, score, output<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>device <span style="color:#f92672">=</span> device
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>size <span style="color:#f92672">=</span> size <span style="color:#75715e"># num_beam </span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>input_ids <span style="color:#f92672">=</span> input_ids<span style="color:#f92672">.</span>to(self<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>score <span style="color:#f92672">=</span> score
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>output <span style="color:#f92672">=</span> output<span style="color:#f92672">.</span>to(self<span style="color:#f92672">.</span>device) <span style="color:#66d9ef">if</span> output <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span> <span style="color:#66d9ef">else</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># get input_ids </span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_current_state</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>input_ids
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># get probability of the sentence         </span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_score</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>score
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># create a new instance of Beam class after the top k selection</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">extend</span>(self, token_id, score):
</span></span><span style="display:flex;"><span>        new_input_ids <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([self<span style="color:#f92672">.</span>input_ids, token_id<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>)], dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        new_score <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>score <span style="color:#f92672">*</span> score
</span></span><span style="display:flex;"><span>        new_output <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([self<span style="color:#f92672">.</span>output, token_id<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>)], dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>) <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>output <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span> <span style="color:#66d9ef">else</span> new_input_ids
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> Beam(self<span style="color:#f92672">.</span>device, self<span style="color:#f92672">.</span>size, new_input_ids, new_score, new_output)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">BeamSampler</span>(Sampler):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">beam_decode</span>(self, ids):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>tokenizer<span style="color:#f92672">.</span>decode(ids<span style="color:#f92672">.</span>squeeze()<span style="color:#f92672">.</span>tolist())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Get the top k id with the greatest probability</span>
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">@staticmethod</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_topk</span>(prob, k<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>):
</span></span><span style="display:flex;"><span>        scores, token_ids <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>topk(prob, k<span style="color:#f92672">=</span>k, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> scores, token_ids
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __call__(self, prompt, max_new_tokens<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>, num_beam<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>):
</span></span><span style="display:flex;"><span>        input_ids <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>encode(prompt)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># initialize the beam</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Ensure this initializes only `num_beam` beams</span>
</span></span><span style="display:flex;"><span>        beams <span style="color:#f92672">=</span> [Beam(self<span style="color:#f92672">.</span>device, num_beam, input_ids, <span style="color:#ae81ff">1</span>) <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(num_beam)]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># loop until the maximum length is reached</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(max_new_tokens):
</span></span><span style="display:flex;"><span>            all_next_token_prob <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">for</span> beam <span style="color:#f92672">in</span> beams:
</span></span><span style="display:flex;"><span>                next_token_prob <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>get_next_token_prob(input_ids<span style="color:#f92672">=</span>beam<span style="color:#f92672">.</span>get_current_state())
</span></span><span style="display:flex;"><span>                all_next_token_prob<span style="color:#f92672">.</span>append(next_token_prob)
</span></span><span style="display:flex;"><span>                
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># With this</span>
</span></span><span style="display:flex;"><span>            all_topk_scores <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>            all_topk_token_ids <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">for</span> prob <span style="color:#f92672">in</span> all_next_token_prob:
</span></span><span style="display:flex;"><span>                scores, token_ids <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>get_topk(prob, k<span style="color:#f92672">=</span>num_beam)
</span></span><span style="display:flex;"><span>                all_topk_scores<span style="color:#f92672">.</span>append(scores)
</span></span><span style="display:flex;"><span>                all_topk_token_ids<span style="color:#f92672">.</span>append(token_ids)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            all_topk_scores <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>stack(all_topk_scores)
</span></span><span style="display:flex;"><span>            all_topk_token_ids <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>stack(all_topk_token_ids)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            new_beams <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Then, when accessing them:</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">for</span> j, beam <span style="color:#f92672">in</span> enumerate(beams):
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">for</span> k <span style="color:#f92672">in</span> range(num_beam):
</span></span><span style="display:flex;"><span>                    score <span style="color:#f92672">=</span> all_topk_scores[j][k]<span style="color:#f92672">.</span>item()
</span></span><span style="display:flex;"><span>                    token_id <span style="color:#f92672">=</span> all_topk_token_ids[j][k]<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>                    new_beam <span style="color:#f92672">=</span> beam<span style="color:#f92672">.</span>extend(token_id, score)
</span></span><span style="display:flex;"><span>                    new_beams<span style="color:#f92672">.</span>append(new_beam)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            beams <span style="color:#f92672">=</span> sorted(new_beams, key<span style="color:#f92672">=</span><span style="color:#66d9ef">lambda</span> b: b<span style="color:#f92672">.</span>get_score(), reverse<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)[:num_beam]
</span></span><span style="display:flex;"><span>        generated_text <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>beam_decode(beams[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>output[:, len(input_ids[<span style="color:#ae81ff">0</span>]):])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> prompt <span style="color:#f92672">+</span> generated_text
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>beam_sampler <span style="color:#f92672">=</span> BeamSampler()
</span></span><span style="display:flex;"><span>result <span style="color:#f92672">=</span> beam_sampler(prompt<span style="color:#f92672">=</span>SAMPLE_TEXT,max_new_tokens<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span> , num_beam<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>print(result)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>result <span style="color:#f92672">=</span> beam_sampler(prompt<span style="color:#f92672">=</span>SAMPLE_TEXT,max_new_tokens<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span> , num_beam<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>print(result)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>Output: 
</span></span><span style="display:flex;"><span>Artificial Intelligence <span style="color:#f92672">is</span> the intelligence possessed by the machines that run the world<span style="color:#f92672">.</span> It <span style="color:#f92672">is</span> the intelligence that <span style="color:#f92672">is</span> capable of understanding <span style="color:#f92672">and</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Artificial Intelligence <span style="color:#f92672">is</span> the intelligence possessed by the machines used to create those machines<span style="color:#f92672">.</span>[<span style="color:#ae81ff">1</span>e]] As such we may expect intelligent
</span></span></code></pre></div><p>When we used a beam size of 1, the generated text was more deterministic, relying heavily on the most probable tokens at each step. On the other hand, increasing the beam size to 10 allowed for a broader exploration of possibilities, which resulted in coherent text with a degree of variation.</p>
<p>However, there are a few drawbacks to this approach:</p>
<ul>
<li>It requires more computational resources than greedy search, as it needs to maintain and calculate probabilities for ‘k’ sequences at each step which can amplify inference time.</li>
<li>It also doesn’t guarantee finding the most probable sequence, especially if the beam width ‘k’ is too small compared to the size of the vocabulary.</li>
</ul>
<h3 id="temperature-sampling">Temperature Sampling<a hidden class="anchor" aria-hidden="true" href="#temperature-sampling">#</a></h3>
<p>$$
P\left(x_{i} \mid x_{1: i-1}\right)=\frac{\exp \left(u_{i} / t\right)}{\sum_{j} \exp \left(u_{j} / t\right)}
$$</p>
<p>Random sampling can be very unpredictable. We can enhance the predictability and control over random sampling using temperature. Temperature serves as a mechanism to control the likelihood of selecting certain tokens over others. The temperature acts as a hyperparameter that can either amplify or reduce the randomness in the sampling process, providing a balance between unpredictability and determinism.</p>
<p>A temperature value set between 0 and 1 can adjust this probability. Specifically, as the temperature approaches 1, it tends to retain the original randomness of sampling. Conversely, as the temperature nears 0, the process becomes more deterministic like greedy decoding. Let&rsquo;s see how this works on code:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">RandomTempSampler</span>(Sampler):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __call__(self, prompt, max_new_tokens<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span> , temp : float <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>):
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>        predictions <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>        result <span style="color:#f92672">=</span> prompt
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># generate until max_len</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(max_new_tokens):
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>            input_ids <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>encode(result)
</span></span><span style="display:flex;"><span>            next_token_probs <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>get_next_token_prob(input_ids<span style="color:#f92672">=</span>input_ids)
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># apply temp before softmax (sharper logits)</span>
</span></span><span style="display:flex;"><span>            next_token_probs <span style="color:#f92672">/=</span> temp
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># convert logits to scores</span>
</span></span><span style="display:flex;"><span>            scores <span style="color:#f92672">=</span> softmax(next_token_probs, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># sample from scores</span>
</span></span><span style="display:flex;"><span>            id <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>multinomial(scores, num_samples<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>item()
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># convert to token and add new token to text</span>
</span></span><span style="display:flex;"><span>            result <span style="color:#f92672">+=</span> self<span style="color:#f92672">.</span>decode(id)
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># keep track of scores for next token</span>
</span></span><span style="display:flex;"><span>            predictions<span style="color:#f92672">.</span>append(scores[id]<span style="color:#f92672">.</span>item())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> result
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sample_plot</span>(self,prompt,temp:float <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>):
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        input_ids <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>encode(prompt)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        next_token_probs <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>get_next_token_prob(input_ids<span style="color:#f92672">=</span>input_ids)
</span></span><span style="display:flex;"><span>        next_token_probs <span style="color:#f92672">/=</span> temp
</span></span><span style="display:flex;"><span>        scores <span style="color:#f92672">=</span> softmax(next_token_probs, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>plot_scores(scores<span style="color:#f92672">=</span>scores,title<span style="color:#f92672">=</span><span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Tempreature : </span><span style="color:#e6db74">{</span>temp<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>,samples<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>random_tempreature <span style="color:#f92672">=</span> RandomTempSampler()
</span></span><span style="display:flex;"><span>print(random_tempreature(prompt<span style="color:#f92672">=</span>SAMPLE_TEXT,max_new_tokens<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>,temp<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(random_tempreature(prompt<span style="color:#f92672">=</span>SAMPLE_TEXT,max_new_tokens<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>,temp<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>))
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>Output: 
</span></span><span style="display:flex;"><span>Artificial Intelligence <span style="color:#f92672">is</span> the intelligence possessed by the machines that run the world<span style="color:#f92672">.</span> It <span style="color:#f92672">is</span> the intelligence that <span style="color:#f92672">is</span> able to understand <span style="color:#f92672">and</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Artificial Intelligence <span style="color:#f92672">is</span> the intelligence possessed by the computer when the parts of it that commands it do the work are no longer useful
</span></span></code></pre></div><p>Let’s look at the probability distribution when we change the temperature of the softmax function.</p>
<p>When Temperature is set to <strong>0.1 :</strong></p>
<p><code>random_tempreature.sample_plot(prompt=SAMPLE_TEXT,temp=0.1)</code></p>
<p><img loading="lazy" src="/blogs/img/decoding/rand0.png" alt="rand0.png"  />
</p>
<p>When Temperature is set to <strong>0.9 :</strong></p>
<p><code>random_tempreature.sample_plot(prompt=SAMPLE_TEXT,temp=0.1)</code></p>
<p><img loading="lazy" src="/blogs/img/decoding/rand1.png" alt="rand1.png"  />
</p>
<p>It is evident from the plots that by increasing the temperature, a skewed distribution is turned into a more uniform distribution. This will increase entropy and add more randomness. This is why, when we significantly raise the temperature, it introduces more randomness in the model, which can lead to unusual outputs.</p>
<h3 id="top-k-sampling">Top-K sampling<a hidden class="anchor" aria-hidden="true" href="#top-k-sampling">#</a></h3>
<p>Top-K sampling is another technique in language generation. It works by ensuring that only the most probable tokens (the top K tokens) have a chance at being selected in the next step.</p>
<p>This method narrows down the choices to the K most probable ones, and at each generation step, tokens are selected from this restricted pool. If we set $K=1$, it simply becomes a greedy search, choosing the most probable word each time. Conversely, if $k=len(vocabulary(v))$, it&rsquo;s the same as pure sampling, considering every word equally. We can also introduce the concept of temperature in top-k sampling, which allows for the adjustment of randomness in top-k selections.</p>
<p>However, it&rsquo;s crucial to note a limitation of this method. Using a constant value for k is not optimal for all contexts. In some situations, there can be many equally good options for the next word, making the distribution head flat. In other contexts, a few tokens dominate the probability distribution. A small k might result in generic text, and a large k could include unsuitable word candidates.</p>
<p>As we have now laid the theoretical groundwork, let’s implement top-k sampling with temperature.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">TOPKsampler</span>(Sampler):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __call__(self, prompt, max_new_tokens<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span> ,top_k <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span> ,temp : float <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>):
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        predictions <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>        result <span style="color:#f92672">=</span> prompt
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># generate until max_len</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(max_new_tokens):
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># convert words to tokens</span>
</span></span><span style="display:flex;"><span>            input_ids <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>encode(result)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            next_token_probs <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>get_next_token_prob(input_ids<span style="color:#f92672">=</span>input_ids)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            next_token_probs <span style="color:#f92672">=</span> next_token_probs <span style="color:#f92672">/</span> temp
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            indices_to_remove <span style="color:#f92672">=</span> next_token_probs <span style="color:#f92672">&lt;</span> torch<span style="color:#f92672">.</span>topk(next_token_probs, top_k)[<span style="color:#ae81ff">0</span>][<span style="color:#f92672">...</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#66d9ef">None</span>]
</span></span><span style="display:flex;"><span>            new_logits <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>clone(next_token_probs)
</span></span><span style="display:flex;"><span>            new_logits[indices_to_remove] <span style="color:#f92672">=</span> float(<span style="color:#e6db74">&#39;-inf&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># convert logits to scores</span>
</span></span><span style="display:flex;"><span>            scores <span style="color:#f92672">=</span> softmax(new_logits, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># Use modified logits</span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># sample from scores</span>
</span></span><span style="display:flex;"><span>            id <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>multinomial(scores, num_samples<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>item()
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># convert to token and add new token to text</span>
</span></span><span style="display:flex;"><span>            result <span style="color:#f92672">+=</span> self<span style="color:#f92672">.</span>decode(id)
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># keep track of scores for next token</span>
</span></span><span style="display:flex;"><span>            predictions<span style="color:#f92672">.</span>append(scores[id]<span style="color:#f92672">.</span>item())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> result
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sample_plot</span>(self,prompt ,top_k <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span> ,temp : float <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        input_ids <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>encode(prompt)
</span></span><span style="display:flex;"><span>        next_token_probs <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>get_next_token_prob(input_ids<span style="color:#f92672">=</span>input_ids)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        next_token_probs <span style="color:#f92672">=</span> next_token_probs <span style="color:#f92672">/</span> temp
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Remove all tokens with a probability less than the last token of the top-k.</span>
</span></span><span style="display:flex;"><span>        indices_to_remove <span style="color:#f92672">=</span> next_token_probs <span style="color:#f92672">&lt;</span> torch<span style="color:#f92672">.</span>topk(next_token_probs, top_k)[<span style="color:#ae81ff">0</span>][<span style="color:#f92672">...</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#66d9ef">None</span>]
</span></span><span style="display:flex;"><span>        new_logits <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>clone(next_token_probs)
</span></span><span style="display:flex;"><span>        new_logits[indices_to_remove] <span style="color:#f92672">=</span> float(<span style="color:#e6db74">&#39;-inf&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># convert logits to scores</span>
</span></span><span style="display:flex;"><span>        scores <span style="color:#f92672">=</span> softmax(new_logits, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># Use modified logits</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>plot_scores(scores,title<span style="color:#f92672">=</span><span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Tempreature : </span><span style="color:#e6db74">{</span>temp<span style="color:#e6db74">}</span><span style="color:#e6db74">  Top k : </span><span style="color:#e6db74">{</span>top_k<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span> , samples <span style="color:#f92672">=</span> top_k <span style="color:#f92672">+</span> int(math<span style="color:#f92672">.</span>sqrt(top_k)))
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>topksampler <span style="color:#f92672">=</span> TOPKsampler()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>result <span style="color:#f92672">=</span> topksampler(
</span></span><span style="display:flex;"><span>    prompt<span style="color:#f92672">=</span>SAMPLE_TEXT,
</span></span><span style="display:flex;"><span>    max_new_tokens<span style="color:#f92672">=</span><span style="color:#ae81ff">32</span>,
</span></span><span style="display:flex;"><span>    top_k<span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>,
</span></span><span style="display:flex;"><span>    temp<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>print(result)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>Output:
</span></span><span style="display:flex;"><span>Artificial Intelligence <span style="color:#f92672">is</span> the intelligence possessed by the computers that control our lives<span style="color:#f92672">.</span> Artificial Intelligence <span style="color:#f92672">is</span> also the intelligence that runs our economy, our government, <span style="color:#f92672">and</span> our economy<span style="color:#e6db74">&#39;s own intelligence.</span>
</span></span></code></pre></div><p>Let’s look at the probability distribution when we change the temperature of the softmax function among top-k tokens.</p>
<p><code>topksampler.sample_plot(prompt=SAMPLE_TEXT,top_k=10,temp=0.1)</code></p>
<p><img loading="lazy" src="/blogs/img/decoding/topk1.png" alt="topk1.png"  />
</p>
<p><code>topksampler.sample_plot(prompt=SAMPLE_TEXT,top_k=10,temp=0.9)</code></p>
<p><img loading="lazy" src="/blogs/img/decoding/topk2.png" alt="topk2.png"  />
</p>
<h3 id="nucleustop-p-sampling">Nucleus(top-p) sampling<a hidden class="anchor" aria-hidden="true" href="#nucleustop-p-sampling">#</a></h3>
<p>Nucleus sampling is similar to Top-K sampling. Instead selecting the most probable K words, nucleus sampling selects the smallest set of words whose combined probabilities surpass a threshold, p. This method allows for a dynamic number of candidate words, which can expand or contract based on the model&rsquo;s confidence in the vocabulary.</p>
<p><em>Nucleus sampling first picks a subset of the vocabulary $V^{(p)}$⊂$V$, where $V^{(p)}$ is smallest set of tokens such that</em></p>
<p><em>$$
\sum_{x_{i} \in V^{(p)}} P\left(x_{i} \mid x_{1: i-1}\right) \geq p
$$</em></p>
<p><em>That is, we pick the highest probable tokens until the sum of their probabilities is less than <strong>p</strong>. <a href="https://nn.labml.ai/sampling/nucleus.html">Source</a></em></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">NucleusSampler</span>(Sampler):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __call__(self, prompt, max_new_tokens<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span> , p : float <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.7</span>):
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        predictions <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>        result <span style="color:#f92672">=</span> prompt
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># generate until max_len</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(max_new_tokens):
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># convert words to tokens</span>
</span></span><span style="display:flex;"><span>            input_ids <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>encode(result)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            next_token_probs <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>get_next_token_prob(input_ids<span style="color:#f92672">=</span>input_ids)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            sorted_logits, sorted_indices <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sort(next_token_probs, descending<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>            cumulative_probs <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cumsum(softmax(sorted_logits, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>), dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Remove tokens with cumulative probability above the threshold</span>
</span></span><span style="display:flex;"><span>            sorted_indices_to_remove <span style="color:#f92672">=</span> cumulative_probs <span style="color:#f92672">&gt;</span> p
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            When we determine which tokens to remove based on this mask, it&#39;s important to note that as soon as the cumulative probability crosses the threshold `p`, 
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            all the subsequent tokens will also have cumulative probabilities greater than `p` (because the probabilities are sorted in descending order). 
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            The logic here is to also exclude the very first token that caused the cumulative sum to cross the threshold, and this is achieved by shifting the mask to the right.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            By doing this shift and ensuring the first token that exceeds the threshold is included in the removal list, 
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            we&#39;re adhering to the true spirit of top-p sampling: we&#39;re including in the final consideration only those tokens whose cumulative sum is less than or equal to `p`.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Shift the indices to the right to keep also the first token above the threshold</span>
</span></span><span style="display:flex;"><span>            sorted_indices_to_remove[<span style="color:#f92672">...</span>, <span style="color:#ae81ff">1</span>:] <span style="color:#f92672">=</span> sorted_indices_to_remove[<span style="color:#f92672">...</span>, :<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>clone()
</span></span><span style="display:flex;"><span>            sorted_indices_to_remove[<span style="color:#f92672">...</span>, <span style="color:#ae81ff">0</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            indices_to_remove <span style="color:#f92672">=</span> sorted_indices[sorted_indices_to_remove]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            new_logits <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>clone(next_token_probs)
</span></span><span style="display:flex;"><span>            new_logits[indices_to_remove] <span style="color:#f92672">=</span> float(<span style="color:#e6db74">&#39;-inf&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># convert logits to scores</span>
</span></span><span style="display:flex;"><span>            scores <span style="color:#f92672">=</span> softmax(new_logits, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># Use modified logits</span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># sample from scores</span>
</span></span><span style="display:flex;"><span>            id <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>multinomial(scores, num_samples<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>item()\
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># convert to token and add new token to text</span>
</span></span><span style="display:flex;"><span>            result <span style="color:#f92672">+=</span> self<span style="color:#f92672">.</span>decode(id)
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># keep track of scores for next token</span>
</span></span><span style="display:flex;"><span>            predictions<span style="color:#f92672">.</span>append(scores[id]<span style="color:#f92672">.</span>item())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> result
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sample_plot</span>(self,prompt, p: float):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        input_ids <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>encode(prompt)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        next_token_probs <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>get_next_token_prob(input_ids<span style="color:#f92672">=</span>input_ids)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        sorted_logits, sorted_indices <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sort(next_token_probs, descending<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        cumulative_probs <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cumsum(softmax(sorted_logits, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>), dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Remove tokens with cumulative probability above the threshold</span>
</span></span><span style="display:flex;"><span>        sorted_indices_to_remove <span style="color:#f92672">=</span> cumulative_probs <span style="color:#f92672">&gt;</span> p
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Shift the indices to the right to keep also the first token above the threshold</span>
</span></span><span style="display:flex;"><span>        sorted_indices_to_remove[<span style="color:#f92672">...</span>, <span style="color:#ae81ff">1</span>:] <span style="color:#f92672">=</span> sorted_indices_to_remove[<span style="color:#f92672">...</span>, :<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>clone()
</span></span><span style="display:flex;"><span>        sorted_indices_to_remove[<span style="color:#f92672">...</span>, <span style="color:#ae81ff">0</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        indices_to_remove <span style="color:#f92672">=</span> sorted_indices[sorted_indices_to_remove]
</span></span><span style="display:flex;"><span>                
</span></span><span style="display:flex;"><span>        new_logits <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>clone(next_token_probs)
</span></span><span style="display:flex;"><span>        new_logits[indices_to_remove] <span style="color:#f92672">=</span> float(<span style="color:#e6db74">&#39;-inf&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># convert logits to scores</span>
</span></span><span style="display:flex;"><span>        scores <span style="color:#f92672">=</span> softmax(new_logits, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>plot_scores(scores,title<span style="color:#f92672">=</span><span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;P : </span><span style="color:#e6db74">{</span>p<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>, samples<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>nssammpler <span style="color:#f92672">=</span> NucleusSampler()
</span></span><span style="display:flex;"><span>result <span style="color:#f92672">=</span> nssammpler(prompt<span style="color:#f92672">=</span>SAMPLE_TEXT,max_new_tokens<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>,p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.8</span>)
</span></span><span style="display:flex;"><span>print(result)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>Output: 
</span></span><span style="display:flex;"><span>Artificial Intelligence <span style="color:#f92672">is</span> the intelligence possessed by the sentient, intellectual, <span style="color:#f92672">or</span> creative faculties, such <span style="color:#66d9ef">as</span> the abilities of people <span style="color:#66d9ef">with</span> hands
</span></span></code></pre></div><p>Let’s look at the number of candidate words distribution when we change the <strong>p</strong> value.</p>
<p><code>nssammpler.sample_plot(prompt=SAMPLE_TEXT,p=0.8)</code></p>
<p><img loading="lazy" src="/blogs/img/decoding/p0.png" alt="p0.png"  />
</p>
<p><code>nssammpler.sample_plot(prompt*=SAMPLE_TEXT,p=0.1)</code></p>
<p><img loading="lazy" src="/blogs/img/decoding/p1.png" alt="p1.png"  />
</p>
<p>We can see from the plots that by increasing the p, the candidate words distribution converts from more uniform distribution to a skewed distribution.</p>
<p>All of the codes used in this repository can be found <a href="https://github.com/R4j4n/Text-Decoding-Strategies">HERE</a>.
From the simplicity of Greedy Search Decoding to the more sophisticated approaches like Beam Search and various sampling techniques such as Temperature Sampling, Top-K, and Nucleus Sampling, each offers unique advantages to address specific challenges. By understanding and implementing these methods, we can tailor their model&rsquo;s output to align closer with the desired outcome.</p>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<ul>
<li><a href="https://www.kaggle.com/code/sajjadayobi360/how-to-generate-text-using-language-models">https://www.kaggle.com/code/sajjadayobi360/how-to-generate-text-using-language-models</a></li>
<li><a href="https://vitalflux.com/greedy-search-vs-beam-search-decoding-concepts-examples/#:~:text=complex%20decoding%20methods.-,Drawbacks%20of%20Greedy%20Search%20Decoding%20Method,term%20implications%20of%20its%20choices">https://vitalflux.com/greedy-search-vs-beam-search-decoding-concepts-examples/#:~:text=complex decoding methods.-,Drawbacks of Greedy Search Decoding Method,term implications of its choices</a>.</li>
<li><a href="https://medium.com/@jessica_lopez/understanding-greedy-search-and-beam-search-98c1e3cd821d">https://medium.com/@jessica_lopez/understanding-greedy-search-and-beam-search-98c1e3cd821d</a></li>
<li><a href="https://towardsdatascience.com/decoding-strategies-that-you-need-to-know-for-response-generation-ba95ee0faadc#:~:text=Random%20Sampling%20with%20Temperature,1%2C%20there%20is%20no%20effect">https://towardsdatascience.com/decoding-strategies-that-you-need-to-know-for-response-generation-ba95ee0faadc#:~:text=Random Sampling with Temperature,1%2C there is no effect</a>.</li>
<li><a href="https://windysavage.github.io/Decoding-strategies-in-text-generation/">https://windysavage.github.io/Decoding-strategies-in-text-generation/</a></li>
<li><a href="https://peterchng.com/blog/2023/05/02/token-selection-strategies-top-k-top-p-and-temperature/">https://peterchng.com/blog/2023/05/02/token-selection-strategies-top-k-top-p-and-temperature/</a></li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://R4j4n.github.io/blogs/tags/natural-language-processing/">Natural Language Processing</a></li>
      <li><a href="https://R4j4n.github.io/blogs/tags/pytorch/">PyTorch</a></li>
      <li><a href="https://R4j4n.github.io/blogs/tags/large-language-models/">Large Language Models</a></li>
    </ul>
<nav class="paginav">
  <a class="next" href="https://R4j4n.github.io/blogs/posts/apdapter/">
    <span class="title">Next »</span>
    <br>
    <span>Supercharge Your LLaMA: Fine-Tuning Made Effortless and Efficient 🚀</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://R4j4n.github.io/blogs/">Rajan Ghimire</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
