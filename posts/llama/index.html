<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Understanding The Secret Sauce of LLaMA🦙 | Rajan Ghimire</title>
<meta name="keywords" content="Natural Language Processing, PyTorch, Large Language Model">
<meta name="description" content="Understanding the ins and outs of Meta&#39;s LLaMa(Open and Efficient Foundation Language Models)">
<meta name="author" content="Rajan Ghimire">
<link rel="canonical" href="https://R4j4n.github.io/blogs/posts/llama/">
<link crossorigin="anonymous" href="/blogs/assets/css/stylesheet.54ccf0f50e0bb5bc885c6d275474800d82dde88cc22f647be5b4e4ca14d7176f.css" integrity="sha256-VMzw9Q4LtbyIXG0nVHSADYLd6IzCL2R75bTkyhTXF28=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/blogs/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://R4j4n.github.io/blogs/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://R4j4n.github.io/blogs/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://R4j4n.github.io/blogs/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://R4j4n.github.io/blogs/apple-touch-icon.png">
<link rel="mask-icon" href="https://R4j4n.github.io/blogs/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css"
    integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js"
    integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY"
    crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js"
    integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>

>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false }
            ]
        });
    });
</script>
<meta property="og:title" content="Understanding The Secret Sauce of LLaMA🦙" />
<meta property="og:description" content="Understanding the ins and outs of Meta&#39;s LLaMa(Open and Efficient Foundation Language Models)" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://R4j4n.github.io/blogs/posts/llama/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-01-20T00:00:00+00:00" />
<meta property="article:modified_time" content="2023-01-20T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Understanding The Secret Sauce of LLaMA🦙"/>
<meta name="twitter:description" content="Understanding the ins and outs of Meta&#39;s LLaMa(Open and Efficient Foundation Language Models)"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://R4j4n.github.io/blogs/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Understanding The Secret Sauce of LLaMA🦙",
      "item": "https://R4j4n.github.io/blogs/posts/llama/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Understanding The Secret Sauce of LLaMA🦙",
  "name": "Understanding The Secret Sauce of LLaMA🦙",
  "description": "Understanding the ins and outs of Meta's LLaMa(Open and Efficient Foundation Language Models)",
  "keywords": [
    "Natural Language Processing", "PyTorch", "Large Language Model"
  ],
  "articleBody": " Modern Llama. Generated using Stable Diffusion v2 In the information era, a new king reigns supreme: the language model. With the internet drowning in a never-ending flood of data, there is a growing demand for intelligent machines that can not only absorb this data but also produce, analyze, and interact with it in previously unthinkable ways.\nEnter LLaMA, Meta’s Large Language Model, a monument to artificial intelligence’s current peak. But what lurks underneath the many layers of this behemoth? How can it understand the complexity of human language and respond with such amazing accuracy? If these questions have ever sparked your interest, you’ve come to the right spot.\nA large language model, or LLM, is a digital polyglot that understands and generates human-like language based on huge databases. Consider a computer being that reads nearly every book, article, and paper ever published, embodying humanity’s cumulative knowledge. That is the strength of LLMs. While there are several large models out there, Meta’s LLaMA stands out, pioneering techniques that have revolutionized the field.\nThis blog will untangle the confusing strings that makeup LLaMa. We will travel through its major components, including ROPE (Rotary Position Embedding), RMSNorm, and SwiGLU, exploring both their theoretical foundations and practical uses. Our journey will not only scratch the surface but also we’ll construct this model from the ground up, guided by the strong PyTorch Lightning’s implementation of LLaMa called lit-llama.\nI am utilizing the lit-lama implementation of LLaMa primarily due to its open-source nature, which aligns with the ethos of transparent and accessible development. While it seamlessly integrates with the original LLaMa weights distributed by Meta for research purposes, what sets lit-lama apart is its independent implementation that covers the entire spectrum from pretraining and finetuning to inference. Notably, this entire repo is provided under the Apache 2.0 license, ensuring broad usability and adaptability for a variety of research and development scenarios.\nBefore we embark on this journey, it’s crucial to have a solid understanding of the Transformer architecture, as this voyage assumes you’re well-acquainted with its nuances. You can refer to these blogs to recall the concepts of Transformer. 1. 2. 3.\nFoundations of LLaMa: A Deeper Dive 1. ROPE(Rotary Position Embedding) Transformers are widely used models for numerous NLP tasks. However, these models do not naturally comprehend the sequence order (in the context of NLP, the order of words). This is where position embeddings become crucial. Position embeddings identify the position of each token in the sequence. These are incorporated with the token embeddings before being input into the Transformer model, enabling the model to grasp the sequence order.\nThe majority of methods incorporate position data into token representations by addition operation to include either absolute or relative position information.\n$$ \\begin{align}\\mathbf{q}_m \u0026= \\mathbf{W}_q(\\mathbf{x}_m + \\mathbf{p}_m) \\\\\\mathbf{k}_n \u0026= \\mathbf{W}_k(\\mathbf{x}_n + \\mathbf{p}_n)\\\\ \\mathbf{v}_n \u0026= \\mathbf{W}_v(\\mathbf{x}_n + \\mathbf{p}_n) \\\\\\end{align} $$\nThis approach has a few drawbacks:\nThe vanilla positional encoding is designed for a fixed maximum sequence length. If you have a more extended sequence than the maximum length used during training, handling it becomes problematic. You might need to truncate, split, or find another way to fit it within the maximum length. A model trained with a particular maximum sequence length may not generalize well to sequences of very different lengths, even if they’re within the allowed range. The positional encoding for these lengths might be outside the distribution seen during training. The sinusoidal nature of the positional encoding might not always be optimal for capturing very long-term dependencies in long sequences. While self-attention theoretically allows for such connections, in practice, the model might still struggle due to the fixed nature of the encoding. Why RoPE?\nA key advantage of utilizing rotary embedding is its adaptability. In contrast to position embeddings that are restricted to a certain sequence length, RoPE can be expanded to accommodate any sequence length. This makes RoPE useful for models that must handle text of diverse lengths. Rotary embeddings equip linear self-attention with relative position encoding. This implies that models can consider the relative positions of tokens when executing self-attention. This could result in more precise predictions and a deeper understanding of the relationships between tokens. RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in the self-attention formulation.\nThe concept behind the rope is straightforward. Rather than combining the token and position embedding into a single entity by adding them together and calculating q, we should look for transformations for q at m that yield a new vector. Essentially, we rotate the affine-transformed word embedding vector by an amount equivalent to multiples of its position index.\nIn the original attention, the matrix multiplication between query and key matrices only involves the weight matrices W and the input embedding x.\n$$ \\begin{aligned} \u0026 a_{m, n}=q_m^T k_n=\\left[W_q\\left(x_m+P E(m)\\right)\\right]^T W_k\\left(x_n+P E(n)\\right) \\\\ \\end{aligned} $$\nReplace the positional encoding from the vanilla transformer.\n$$ \\begin{aligned} \u0026 a_{m, n}=q_m^T k_n=\\left[W_q x_m\\right]^T W_k x_n \\\\\\ \\end{aligned} $$\nRotate word embedding vector by an amount equivalent to multiples of its position index.\n$$ \\begin{aligned}\u0026 a_{m, n}=q_m^T k_n=\\left[R_{\\Theta, d}^m\\left(W_q x_m\\right)\\right]^T R_{\\Theta, d}^n W_k x_n \\\\\\end{aligned} $$\n$$ \\begin{aligned}\u0026 a_{m, n}=q_m^T k_n=\\left(W_q x_m\\right)^T R_{\\Theta, d}^m{ }^T R_{\\Theta, d}^n W_k x_n \\\\\\end{aligned} $$\n$$ \\begin{aligned}\u0026 a_{m, n}=q_m^T k_n=x_m^T W_q^T\\left[R_{\\Theta, d}^m{ }^T R_{\\Theta, d}^n\\right] W_k x_n\\end{aligned} $$\nHere, $R_{\\Theta, m}^d$ is a rotation matrix. In 2D this matrix is defined as:\n$$ R_{\\Theta, m}^d=\\left(\\begin{array}{cc}\\cos \\left(m \\theta_i\\right) \u0026 -\\sin \\left(m \\theta_i\\right) \\\\sin \\left(m \\theta_i\\right) \u0026 \\cos \\left(m \\theta_i\\right)\\end{array}\\right) $$\nWhere $\\theta$ is a nonzero constant.\nThe rotation matrix is a function of absolute position. Calculating the inner products of rotated queries and keys results in an attention matrix that is a function of relative position information only.\nAttention is relative but how? source\n$$ \\begin{align}\\Big \\langle RoPE\\big(x^{(1)}_m, x^{(2)}_m, m\\big), RoPE\\big(x^{(1)}_n, x^{(2)}_n, n\\big) \\Big \\rangle \u0026= \\\\(x^{(1)}_m \\cos m\\theta - x^{(2)}_m \\sin m \\theta)(x^{(1)}_n \\cos n\\theta - x^{(2)}_n \\sin n \\theta) \u0026+ \\\\(x^{(2)}_m \\cos m\\theta + x^{(1)}_m \\sin m \\theta)(x^{(2)}_n \\cos n\\theta + x^{(1)}_n \\sin n \\theta) \u0026= \\\\x^{(1)}_m x^{(1)}_n (\\cos m\\theta \\cos n\\theta + \\sin m \\theta \\sin n \\theta) \u0026+ \\\\x^{(1)}_m x^{(2)}_n (-\\cos m\\theta \\sin n\\theta + \\sin m \\theta \\cos n \\theta) \u0026+ \\\\x^{(2)}_m x^{(1)}_n (-\\sin m\\theta \\cos n\\theta + \\cos m \\theta \\sin n \\theta) \u0026+ \\\\x^{(2)}_m x^{(2)}_n (\\sin m\\theta \\sin n\\theta + \\cos m \\theta \\cos n \\theta) \u0026= \\\\ x^{(1)}_m x^{(1)}_n \\cos (m - n) \\theta +x^{(1)}_m x^{(2)}_n \\sin(m - n) \\theta \u0026+ \\\\- x^{(2)}_m x^{(1)}_n \\sin (m - n) \\theta +x^{(2)}_m x^{(1)}_n \\cos (m - n) \\theta \u0026= \\\\ \\big(x^{(1)}_m \\cos (m - n)\\theta - x^{(2)}_m \\sin (m - n) \\theta\\big) x^{(1)}_n \u0026+ \\\\\\big(x^{(2)}_m \\cos (m - n)m\\theta + x^{(1)}_m \\sin (m - n) \\theta\\big) x^{(2)}_n \u0026= \\\\ \\Big \\langle RoPE\\big(x^{(1)}_m, x^{(2)}_m, m - n\\big), RoPE\\big(x^{(1)}_n, x^{(2)}_n, 0\\big) \\Big \\rangle \\end{align} $$\nThis shows that for dot-production attention the rotary encoding gives relative attention. so,\nRotary position embedding Overview A block-diagonal matrix is a generalization of the 2D rotation matrix to a d-dimensional space. This divides the space into $d/2$ subspace each having different value of $\\Theta = {\\theta_i = 10000^{-\\frac{2(i-1)}{d}}, i \\in [1, 2, …, \\frac{d}{2}]}$\nApply Rotation to the Queries and Keys: Before computing attention scores, the goal is to rotate the query (Q) and key (K) vectors. This rotation is done using the aforementioned sinusoidal values. Given a query or key vector $v$ with two parts $v_1$ and $v_2$, the rotated vectors $v’_1$ and $v’_2$ are: The ${sin_{value}}$ and ${cos_{value}}$ will depend on the position $m$ in the given sequence and the dimension of features.\nThe following figure illustrates the above concept:\nImplementation of Rotary Position Embedding(RoPE) from original paper.Link Implementation We can see above that ${R}{\\Theta, m}^d$ is sparse. A more computationally efficient realization of a multiplication of ${R}{\\Theta, m}^d$ and $x \\in \\mathbb{R}^d$. Where vector $x$ is a query or a key.\nAs we know that in multi-headed attention, queries and keys are spitted into heads. For each head, we apply the ROPE encodings. Let’s implement ROPE for a single attention head in pure PyTorch.\nAs we can see from the above equation, the $cos$ and $sin$ values for each head will remain the same, so let’s cache these values.\nimport torch def build_cache(dim : int, max_seq_len : int): \"\"\" Given a dimension and maximum sequence length, this function returns a tensor containing position encodings obtained by multiplying position indexes with inverse frequencies. The computed encoding is then duplicated along the inverse frequency dimension. Parameters: - dim (int): Dimensionality for the position encoding, typically the model's hidden dimension size. It determines the number of theta values computed. - max_seq_len (int): The maximum sequence length for which to compute position encodings. Returns: - torch.Tensor: A tensor of shape (max_seq_len, dim) containing the position encodings. Examples: \u003e\u003e\u003e enc = build_cache(512, 100) \u003e\u003e\u003e enc.shape torch.Size([100, 512]) \"\"\" # inverse frequencies [theta_1, theta_2, ..., theta_dim/2] inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2, dtype=torch.float) / dim)) # -\u003e mθ # Compute the sequence of position indexes. pos = torch.arange(max_seq_len) # -\u003e m # Compute the product of the position index and the theta values: pos_enc = torch.einsum(\"n,d-\u003end\", pos, inv_freq) # -\u003e (mθ) # duplicate each element along inverse frequency dimension pos_enc = torch.cat([pos_enc, pos_enc], dim=-1) # -\u003e (mθ,mθ) return pos_enc This build_cache code block will get the product between the position and feature dimension present at that position. We can see this in the following figure:\ndef negative_half(input_tensor : torch.tensor): \"\"\" Applies a specific rotation to the input tensor, often referred to as a \"half rotation\" in the context of tensor operations. Given an input tensor with pairs of elements in its last dimension, this function rotates them such that for each pair (u1, u2), it outputs (-u2, u1). As a result, the sequence [u1, u2, u3, u4, ...] is transformed to [-u2, u1, -u4, u3, ...]. Parameters: - input_tensor (torch.Tensor): The input tensor to be processed. The rotation operation is applied on the last dimension of this tensor. Returns: - torch.Tensor: The rotated tensor with the same shape as the input. Examples: \u003e\u003e\u003e tensor = torch.tensor([1.0, 2.0, 3.0, 4.0]) \u003e\u003e\u003e rotated_tensor = negative_half(tensor) \u003e\u003e\u003e print(rotated_tensor) tensor([-2., 1., -4., 3.]) \"\"\" # Reshaping the tensor so that pairs [u1, u2], [u3, u4], ... are separated u = input_tensor.float().reshape(*input_tensor.shape[:-1], -1, 2) # Separating the pairs into two tensors u1 = u[..., 0] u2 = u[..., 1] # Reconstructing the tensor after rotation u_rotated = torch.stack((-u2, u1), dim=-1) # Flattening the last two dimensions to get [-u2, u1, -u4, u3, ...] u_rotated = u_rotated.view(*u_rotated.shape[:-2], -1) return u_rotated The negative_half method will shuffle the elements of $x$ in the second term of the addition is helpful. This method is basically calculating this part:\ndef rotate(input_tensor : torch.tensor, pos_enc : int): \"\"\" Applies a rotation-based position encoding to the input tensor. The function multiplies the input tensor with the cosine of the position encoding and adds the result of multiplying the negative half of the input tensor with the sine of the position encoding. This is similar to rotating vectors in a complex space using Euler's formula. Parameters: - input_tensor (torch.Tensor): The tensor to which the rotation-based position encoding will be applied. It is assumed that the second-to-last dimension represents tokens or sequence length. - pos_enc (int): Position encoding tensor with values computed elsewhere. The first dimension of this tensor should be at least as long as the number of tokens in the `input_tensor`. Returns: - torch.Tensor: The position-encoded tensor with the same shape as the `input_tensor`. Examples: \u003e\u003e\u003e tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]]) \u003e\u003e\u003e pos_enc_tensor = torch.tensor([1.0, 2.0, 3.0]) \u003e\u003e\u003e rotated_tensor = rotate(tensor, pos_enc_tensor) \u003e\u003e\u003e print(rotated_tensor) \"\"\" num_tokens = input_tensor.shape[-2] pos_enc = pos_enc[:num_tokens] return input_tensor * pos_enc.cos() + (negative_half(input_tensor) * pos_enc.sin()) Complete use case:\nimport torch.nn.functional as F batch_size = 2 num_heads = 4 seq_len = 16 head_dim = seq_len // num_heads # random queries and keys q = torch.randn(batch_size, num_heads, seq_len, head_dim) k = torch.randn(batch_size, num_heads, seq_len, head_dim) v = torch.randn(batch_size, num_heads, seq_len, head_dim) # frequency-encode positions 1 - 512 pos_enc = build_cache(dim=head_dim, max_seq_len=seq_len) # encode absolute positions into queries and keys q_rot = rotate(q, pos_enc) k_rot = rotate(k, pos_enc) # Step 1: Compute scaled dot-product attention attn_weights = torch.einsum('bhsd,bhtd-\u003ebhst', q_rot, k_rot) # dot product of queries and keys # Step 2: Scale the dot products attn_weights = attn_weights / (head_dim ** 0.5) # Step 3: Apply softmax to get the weights attn_weights = F.softmax(attn_weights, dim=-1) # Step 4: Multiply the weights by the values to get the output output = torch.einsum('bhst,bhtd-\u003ebhsd', attn_weights, v) print(output.shape) 2. Pre-normalization \u0026 RMSNorm Pre-normalization structure within a transformer block Source Traditional transformer applies layer normalization after attention and MLP(feed-forward) layer.\nLLAMA uses pre-normalization to normalize the input to each transformer sub-layer instead of normalizing the output. The main goal of pre-normalization is to improve the efficiency and stability of the training process by normalizing and reducing variation and correlation of the input features.\nIn neural networks, the result from the first layer is fed into the second layer, and the result from the second layer is fed into the third, and this process continues. When there are changes in a layer’s parameters, the distribution of inputs for the following layers also changes. This change in distribution is called internal covariate shift.\nAs per the authors of Batch Normalization :\nWe define Internal Covariate Shift as the change in the distribution of network activations due to the change in network parameters during training.\nThis shift can slow down training or cause instability. Normalization helps to stabilize the distributions of layer activations throughout training.\nWe can mitigate this issue by normalizing the input to the activation functions.\nLLAMA uses RMSNorm (Root Mean Square Layer Normalization), an extension of Layer Normalization. RMSNorm is more simpler and efficient than Layer Normalization but achieves similar performance.\nRMSNorm\nLet’s understand Layer Normalization first:\nLayer Normalization for a fully connected layer $i$, neuron $j$ and number of neurons in a layer $m$ :\nCalculate mean :\n$$ mean_{i}=\\frac{1}{m} \\sum_{l=1}^{m} \\sigma_{i l} $$\nCalculate Variance :\n$$ var_{i}=\\frac{1}{m} \\sum_{l=1}^{m}\\left(\\sigma_{i l}-\\text { mean }_{i}\\right)^{2} $$\nNormalize the feature $j$:\n$$ \\hat{\\sigma}_{i j} = \\frac{ \\sigma_ij-mean_i}{\\sqrt{var_i+\\epsilon}} $$\nShift and scale the normalized feature:\nWhere $\\gamma_{i j},\\beta_{i,j}$ are learnable parameters. Layer Normalization is successful due to it’s two properties:\nRe-centering: For each individual data sample, compute the mean of its features. Subtract this mean from each feature of the data sample. This ensures that the features have a mean of zero. Re-scaling: For the same data sample, after re-centering, compute the variance of its features. Normalize each feature by dividing it by the square root of this variance (plus a small epsilon for numerical stability). This ensures that the features have unit variance. After the normalization, the features are typically re-scaled by a learnable parameter and shifted by another learnable parameter. Specifically, if $\\hat{x}$ is the normalized activation, the final output will be $y = \\gamma \\hat{x} + \\beta$, where $\\gamma$ and $\\beta$ are learnable scaling and shifting parameters, respectively. The authors of RMSNorm theorize that the key to the success of layer normalization lies in the re-scaling. They suggest RMSNorm, a method that normalizes the input to maintain re-scaling invariance without re-centering the input.\n$$ \\operatorname{RMS}(\\boldsymbol{a})=\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} a_{i}^{2}} $$\nRMSNorm performs at a level similar to LayerNorm, but it reduces the operational time by approximately 10% to 60%.\nPyTorch implementation of RMSNorm:\nclass RMSNorm(nn.Module): def __init__(self, input_dim , eps = 1e-6) -\u003e None: super().__init__() self.g = nn.Parameter(torch.ones(input_dim)) self.eps = eps def forward(self,x): # RMS of input rms = torch.rsqrt(torch.square(x).mean(dim=-1,keepdim=True) + self.eps) # rescaling x = x * rms return x * self.g 3. SwiGLU SwiGLU is a combination of the Swish activation function and the Gated Linear Unit (GLU) concept. It was introduced in the paper “GLU Variants Improve Transformer” (Sho Takase, Naoaki Okazaki, 2020). The authors propose several variations of the standard GLU that can improve performance on machine translation tasks when used in a Transformer model.\nThe SwiGLU variant is defined as:\n$$ SwiGLU(x, x’) = x ⊙ Swish(x’) $$\nwhere ⊙ is the element-wise multiplication operation, and x’ is the transformed input (generally, a linear transformation of the input x).\nSwish The paper Swish: a Self-Gated Activation Function proposes Swish, also a smooth version of ReLU with a non-zero gradient for negative values.\nSwish is a smooth, non-monotonic function that consistently matches or outperforms ReLU.\nSimply put, Swish is an extension of the SILU activation function. SILU’s formula $f(x) = x * sigmoid(x)$. The slight modification made in the Swish formulation is the addition of a trainable β parameter, making it $f(x)=xsigmoid(\\beta x)$\nSwish has several unique characteristics that make it better than ReLU.\nFirst, Swish is a smooth, continuous function, in contrast to ReLU, which is a piecewise linear function. Swish permits a small number of negative weights to pass through, while ReLU sets all negative weights to zero. This property is vital and contributes significantly to the success of non-monotonic smooth activation functions, such as Swish, particularly when used in progressively deeper neural networks. (A non-monotonic function is a type of function that does not consistently increase or decrease in value. In other words, as you move from left to right along the x-axis, a non-monotonic function can either increase or decrease at different points, not following a single direction throughout its domain.) The trainable parameter $\\beta$ enables the activation function to be fine-tuned more effectively to optimize information propagation and push for smoother gradients. PyTorch implementation:\nimport torch import torch.nn as nn class Swish(nn.Module): def __init__(self, beta: torch.Tensor): super().__init__() self.beta = nn.Parameter(beta) def forward(self, x): return x * torch.sigmoid(self.beta * x) Gated Linear Unit (GLU) GLU (Gated Linear Units) is a layer within a neural network, rather than a strict activation function. It involves a linear transformation followed by a gating process. This gating process is controlled by a sigmoid function that manages the information flow from the linear transformation.\n$$ h_{l}(\\mathbf{X})=(\\mathbf{X} * \\mathbf{W}+\\mathbf{b}) \\otimes \\sigma(\\mathbf{X} * \\mathbf{V}+\\mathbf{c}) $$\n$\\sigma$ means the sigmoid function. So we have two sets of weights W and V, and two biases, b, and c.\nHere is the most intuitive example of GLU I found HERE.\nThe idea is simple. I want to allow the network to decide how much information should flow through a given path, like a logical gate, hence the name. How?\nIf we multiply X by 0, nothing passes. If we multiply X by 1, everything passes. If we multiply X by 0.5, half of it passes. It’s inspired by the idea of the gates of LSTMs but applied to convolutions and linear layers, but it’s the same idea.\nPyTorch implementation:\nclass GLU(nn.Module): def __init__(self, in_size) -\u003e None: super().__init__() self.linear1 = nn.Linear(in_size, in_size) self.linear2 = nn.Linear(in_size, in_size) def forward(self, X): return self.linear1(X) * torch.sigmoid(self.linear2(X)) As we now have a clear understanding of the building blocks of SwiGLU, let’s implement it on PyTorch.\nimport torch import torch.nn as nn class Swish(nn.Module): def __init__(self, beta: torch.Tensor): super().__init__() self.beta = nn.Parameter(beta) def forward(self, x): return x * torch.sigmoid(self.beta * x) class SwiGLU(nn.Module): def __init__(self, in_size, beta: torch.Tensor) -\u003e None: super().__init__() self.linear1 = nn.Linear(in_size, in_size) self.linear2 = nn.Linear(in_size, in_size) self.swish = Swish(beta) def forward(self, X): return self.linear1(X) * self.swish(self.linear2(X)) The LLaMA in PyTorch The code used below can be found : HERE\nHumans learning from LLaMa.Generated using Stable Diffusion 2.0 As mentioned earlier, ill be using a fork of lit-lama repo by Lightning AI. As we have now grasped the fundamental building blocks of LLaMa, let’s get started with the code.\nimport math from dataclasses import dataclass from typing import Optional import torch import torch.nn as nn from torch.nn import functional as F from typing_extensions import Self from utils import find_multiple llama_configs = { \"7B\": dict(n_layer=32, n_head=32, n_embd=4096), \"13B\": dict(n_layer=40, n_head=40, n_embd=5120), \"30B\": dict(n_layer=60, n_head=52, n_embd=6656), \"65B\": dict(n_layer=80, n_head=64, n_embd=8192), } These are the different variants of the LLaMa models.\n@dataclass class LLaMAConfig: block_size: int = 2048 vocab_size: int = 32000 padded_vocab_size: Optional[int] = None n_layer: int = 32 n_head: int = 32 n_embd: int = 4096 def __post_init__(self): if self.padded_vocab_size is None: self.padded_vocab_size = find_multiple(self.vocab_size, 64) @classmethod def from_name(cls, name: str) -\u003e Self: return cls(**llama_configs[name]) The LLaMAConfig class is used to store class variables. Let’s understand each of the class variables:\nblock_size : Represents the maximum sequence length the language model can process. vocab_size : Represents the size of vocabulary the large language model was trained on. n_layer : Represents total number of transformer block. n_head : Represents the total number of heads in each transformer block. n_embd : Represents the size of embedding. According to this tweet of Andrej Karpathy, it is important to find the nearest multiple of 64 for your vocab. The tweet explains: The most dramatic optimization to nanoGPT so far (~25% speedup) is to simply increase vocab size from 50257 to 50304 (nearest multiple of 64). This calculates added useless dimensions but goes down a different kernel path with much higher occupancy. Careful with your Powers of 2.\nYou can also read more about it HERE.\ndef __post_init__(self): if self.padded_vocab_size is None: self.padded_vocab_size = find_multiple(self.vocab_size, 64) This code block initializes the padded_vocab_size attribute of an object to a multiple of 64 based on the object’s vocab_size, but only if padded_vocab_size is not already set.\nclass LLaMA(nn.Module): def __init__(self, config: LLaMAConfig) -\u003e None: super().__init__() assert config.padded_vocab_size is not None self.config = config self.lm_head = nn.Linear(config.n_embd, config.padded_vocab_size, bias=False) self.transformer = nn.ModuleDict( dict( wte=nn.Embedding(config.padded_vocab_size, config.n_embd), h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]), ln_f=RMSNorm(config.n_embd), ) ) def _init_weights(self, module: nn.Module) -\u003e None: if isinstance(module, nn.Linear): torch.nn.init.normal_(module.weight, mean=0.0, std=0.02 / math.sqrt(2 * self.config.n_layer)) elif isinstance(module, nn.Embedding): torch.nn.init.normal_(module.weight, mean=0.0, std=0.02 / math.sqrt(2 * self.config.n_layer)) def forward(self, idx: torch.Tensor) -\u003e torch.Tensor: _, t = idx.size() assert ( t \u003c= self.config.block_size ), f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\" # forward the LLaMA model itself x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd) for block in self.transformer.h: x = block(x) x = self.transformer.ln_f(x) logits = self.lm_head(x) # (b, t, vocab_size) return logits @classmethod def from_name(cls, name: str) -\u003e Self: return cls(LLaMAConfig.from_name(name)) The LLaMA model provided is a PyTorch-based implementation. Below is an elaboration on the various components of the code:\nInitialization: class LLaMA(nn.Module): def __init__(self, config: LLaMAConfig) -\u003e None: super().__init__() assert config.padded_vocab_size is not None self.config = config Here, the model takes a configuration object, LLaMAConfig, during initialization. An assertion checks that the padded_vocab_size attribute is not None.\nModel Architecture: self.lm_head = nn.Linear(config.n_embd, config.padded_vocab_size, bias=False) self.transformer = nn.ModuleDict( dict( wte=nn.Embedding(config.padded_vocab_size, config.n_embd), h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]), ln_f=RMSNorm(config.n_embd), ) ) The lm_head is the final linear layer of the large language model to generate the final prediction. It maps from embeddings to the vocabulary size, which is used for predicting the next word/token. So why are we doing this? This is because we want to represent the probability distribution over the vocabulary to make the prediction. transformer is a dictionary of modules, which includes: wte: Word Token Embedding, an embedding layer for the vocabulary. Given tokens, it will generate embeddings of size config.n_embd=4096. h: A list of blocks, with each block being a segment of the transformer architecture. The number of blocks is defined by config.n_layer. ln_f: A final layer normalization, here using RMSNorm. Weight Initialization: def _init_weights(self, module: nn.Module) -\u003e None: ... This method initializes the weights of linear and embedding layers based on the model configuration.\nForward Pass: def forward(self, idx: torch.Tensor) -\u003e torch.Tensor: ... The forward method defines how input data is processed through the model to produce an output. It processes the input tensor, passes it through the transformer blocks, and eventually through the language model head to produce the logits.\nHere, idx is the shape of B,T. We haven’t converted the tokens into embedding. _, t = idx.size() : Get the sequence length.\nassert ( t \u003c= self.config.block_size ), f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\" This will check whether the input sequence is greater than the max sequence length i.e. self.config.block_size.\nx = self.transformer.wte(idx) This will convert the input of shape B,T to B,T,n_embd\nfor block in self.transformer.h: x = block(x) x = self.transformer.ln_f(x) This passes the embedding throughout n transformer blocks. I think this is the most interesting part of our entire code. We will dive deeper into it next.\nAs discussed above logits = self.lm_head(x) # (b, t, vocab_size) maps from embeddings to the vocabulary size, which is used for predicting the next word/token.\nLoad Model by Name: @classmethod def from_name(cls, name: str) -\u003e Self: return cls(LLaMAConfig.from_name(name)) This class method allows for creating a LLaMA model instance directly using a name, assuming the LLaMAConfig.from_name(name) can produce the necessary configuration from the provided name.\nclass Block(nn.Module): def __init__(self, config: LLaMAConfig) -\u003e None: super().__init__() self.rms_1 = RMSNorm(config.n_embd) self.attn = CausalSelfAttention(config) self.rms_2 = RMSNorm(config.n_embd) self.mlp = MLP(config) def forward(self, x: torch.Tensor) -\u003e torch.Tensor: x = x + self.attn(self.rms_1(x)) x = x + self.mlp(self.rms_2(x)) return x A transformer block typically consists of self-attention mechanisms followed by feed-forward neural networks. The LLaMA model has infused some variations, including the use of RMSNorm for normalization. Forward Pass:\nThe input tensor x is first normalized using the first RMSNorm instance. Post normalization, it’s fed into the CausalSelfAttention. The result is combined with the original tensor via a residual connection, a vital feature in deep networks for maintaining gradient flow. The tensor then undergoes the second RMSNorm normalization. The normalized output is processed by the MLP. As before, the resultant is added back to the tensor using a residual connection. The processed tensor, rich with information, is then returned. The Block class crystallizes a singular transformer layer’s operations within LLaMA. With the integral role of RMSNorm already understood, it becomes evident how this block combines normalization, attention, and feed-forward operations to refine the data representation at each layer. When stacked, these blocks work in concert, building upon one another to offer the powerful capabilities of the LLaMA model.\nclass CausalSelfAttention(nn.Module): def __init__(self, config: LLaMAConfig) -\u003e None: super().__init__() assert config.n_embd % config.n_head == 0 # key, query, value projections for all heads, but in a batch self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=False) # output projection self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=False) self.n_head = config.n_head self.n_embd = config.n_embd self.block_size = config.block_size self.rope_cache: Optional[torch.Tensor] = None def forward(self, x: torch.Tensor) -\u003e torch.Tensor: B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd) # calculate query, key, values for all heads in batch and move head forward to be the batch dim q, k, v = self.c_attn(x).split(self.n_embd, dim=2) head_size = C // self.n_head k = k.view(B, T, self.n_head, head_size) q = q.view(B, T, self.n_head, head_size) v = v.view(B, T, self.n_head, head_size) if self.rope_cache is None: # cache for future forward calls self.rope_cache = build_rope_cache( seq_len=self.block_size, n_elem=self.n_embd // self.n_head, dtype=x.dtype, device=x.device, ) q = apply_rope(q, self.rope_cache) k = apply_rope(k, self.rope_cache) k = k.transpose(1, 2) # (B, nh, T, hs) q = q.transpose(1, 2) # (B, nh, T, hs) v = v.transpose(1, 2) # (B, nh, T, hs) # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -\u003e (B, nh, T, T) # att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) # att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf')) # att = F.softmax(att, dim=-1) # y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -\u003e (B, nh, T, hs) # efficient attention using Flash Attention CUDA kernels y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0.0, is_causal=True) y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side # output projection y = self.c_proj(y) return y Here comes the most interesting part of our LLM. Let’s dive into each line of code in detail.\nInitialization: Here, we first ensure that the embedding size (n_embd) is divisible by the number of attention heads (n_head). This is necessary to equally distribute the embeddings across all heads.\nThe Key, Query, Value Projections: self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=False)\nThis transformation is designed to produce key, query, and value tensors, which are essential for the attention mechanism. Normally, you’d expect three separate linear transformations - one for each of key, query, and value. But here, they’re combined into a single transformation for efficiency.\nInput: config.n_embd represents the embedding size of each token in the model. Output: 3 * config.n_embd might look a bit confusing initially, but it makes perfect sense once you understand the purpose. Since we’re generating three different tensors (key, query, and value) and each has an embedding size of config.n_embd, the combined size is 3 * config.n_embd.\nForward Pass: The input tensor’s dimensions are extracted, where: B represents the batch size. T stands for the sequence length. C denotes the embedding dimensionality. The tensor x undergoes the c_attn transformation, splitting the result into query, key, and value tensors (q, k, v). These tensors are then reshaped for multi-head attention. Essentially, the embedding dimensionality is divided among the number of attention heads. If the rope cache hasn’t been built (i.e., self.rope_cache is None), it’s constructed using the build_rope_cache function. As we already discussed this cache is calculated for a single head and later applied across each head, we can see that n_elem=self.n_embd // self.n_head, this basically means for each token in the sequence, we split the token into n_head, and based on the dimension of the head, we calculate the ROPE cache. This method is pretty much similar to the one we have implemented before. We will discuss some changes in this implementation later. This cache is then applied to the q and k tensors using apply_rope which is also pretty much similar to our previous approach. The q, k, and v tensors are transposed to align them for the attention mechanism. Can you tell me why are we performing this transformation? After transposing, we have the final tensor of the shape (B, nh, T, hs). Now if we perform the operation q @ k.t, as the key is transformed, the final tensor will be of shape T,T. This T,T matrix will give us information about, given a token, what’s the relation with other tokens. I think you got an idea of why this transformation is performed. This is done basically to get the attention matrix. The main action happens in the causal self-attention mechanism. Normally, one would compute attention scores by multiplying q and k, apply a mask for causality, then use this to weigh the v tensor. Here, however, the mechanism uses the efficient F.scaled_dot_product_attention method, which leverages FlashAttention for faster attention calculations. FlashAttention is a new algorithm to speed up attention and reduce its memory footprint—without any approximation. You can read more about FlashAttention Here, Here, Here. The resultant tensor y is reshaped and then undergoes the output projection via the c_proj transformation. class MLP(nn.Module): def __init__(self, config: LLaMAConfig) -\u003e None: super().__init__() hidden_dim = 4 * config.n_embd n_hidden = int(2 * hidden_dim / 3) n_hidden = find_multiple(n_hidden, 256) self.c_fc1 = nn.Linear(config.n_embd, n_hidden, bias=False) self.c_fc2 = nn.Linear(config.n_embd, n_hidden, bias=False) self.c_proj = nn.Linear(n_hidden, config.n_embd, bias=False) def forward(self, x: torch.Tensor) -\u003e torch.Tensor: x = F.silu(self.c_fc1(x)) * self.c_fc2(x) x = self.c_proj(x) return x class RMSNorm(nn.Module): \"\"\"Root Mean Square Layer Normalization. Derived from . BSD 3-Clause License: . \"\"\" def __init__(self, size: int, dim: int = -1, eps: float = 1e-5) -\u003e None: super().__init__() self.scale = nn.Parameter(torch.ones(size)) self.eps = eps self.dim = dim def forward(self, x: torch.Tensor) -\u003e torch.Tensor: # NOTE: the original RMSNorm paper implementation is not equivalent # norm_x = x.norm(2, dim=self.dim, keepdim=True) # rms_x = norm_x * d_x ** (-1. / 2) # x_normed = x / (rms_x + self.eps) norm_x = torch.mean(x * x, dim=self.dim, keepdim=True) x_normed = x * torch.rsqrt(norm_x + self.eps) return self.scale * x_normed def build_rope_cache(seq_len: int, n_elem: int, dtype: torch.dtype, device: torch.device, base: int = 10000) -\u003e torch.Tensor: \"\"\"Enhanced Transformer with Rotary Position Embedding. Derived from: transformers/rope/__init__.py. MIT License: . \"\"\" # $\\\\Theta = {\\\\theta_i = 10000^{\\\\frac{2(i-1)}{d}}, i \\\\in [1, 2, ..., \\\\frac{d}{2}]}$ theta = 1.0 / (base ** (torch.arange(0, n_elem, 2, dtype=dtype, device=device) / n_elem)) # Create position indexes `[0, 1, ..., seq_len - 1]` seq_idx = torch.arange(seq_len, dtype=dtype, device=device) # Calculate the product of position index and $\\\\theta_i$ idx_theta = torch.outer(seq_idx, theta).float() cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)], dim=-1) # this is to mimic the behaviour of complex32, else we will get different results if dtype in (torch.float16, torch.bfloat16, torch.int8): cache = cache.half() return cache def apply_rope(x: torch.Tensor, rope_cache: torch.Tensor) -\u003e torch.Tensor: # truncate to support variable sizes T = x.size(1) rope_cache = rope_cache[:T] # cast because the reference does xshaped = x.float().reshape(*x.shape[:-1], -1, 2) # uta hami lea cos ra sine lai 2 ota use garinthiyo. Like x_rope, neg_half_x calculate gareko. rope_cache = rope_cache.view(1, xshaped.size(1), 1, xshaped.size(3), 2) x_out2 = torch.stack( [ xshaped[..., 0] * rope_cache[..., 0] - xshaped[..., 1] * rope_cache[..., 1], xshaped[..., 1] * rope_cache[..., 0] + xshaped[..., 0] * rope_cache[..., 1], ], -1, ) x_out2 = x_out2.flatten(3) return x_out2.type_as(x) The build_rope_cache function is almost identical to build_cache we implemented. Here the cos and sin values are calculated beforehand. Also, build_rope_cache has specific handling for certain data types like torch.float16, torch.bfloat16, and torch.int8, where it casts the computed cache to half precision. build_cache doesn’t handle data types in this manner.\nThe apply_rope also applies RoPE cache to query and key. But there is a slight difference in how the transformation is applied. I’ll explain what is happening in this method in detail.\nWe have two tensors: x and rope_cache.\nLet’s assume x is a 4D tensor with shape (1, 4, 2, 4) and rope_cache is a 4D tensor with shape (4, 2, 2).\nx = tensor([[[[ 0, 1, 2, 3], [ 4, 5, 6, 7]], [[ 8, 9, 10, 11], [12, 13, 14, 15]], [[16, 17, 18, 19], [20, 21, 22, 23]], [[24, 25, 26, 27], [28, 29, 30, 31]]]]) Step 1 :\nT = x.size(1) Here, T is simply the size of the second dimension of x, which is 4.\nStep 2 :\nNext, We resize rope_cache to match the size T:\nrope_cache = rope_cache[:T] This step is redundant because rope_cache already has a size of 4 in its first dimension.\nStep 3:\nThen, you reshape x to make its last dimension into two parts:\nxshaped = x.float().reshape(*x.shape[:-1], -1, 2) This breaks down as:\nConvert x into float: x.float() Reshape it: For our tensor, this converts it from shape (1, 4, 2, 4) to (1, 4, 4, 2). Given the xshaped tensor structure you provided, we can see that its shape is (1, 4, 2, 2, 2). That means you have:\n1 batch (the outermost dimension) 4 channels 2x2 spatial dimensions (height x width) 2 values for each spatial position (the innermost dimension) For instance, before reshaping, the first 2x4 matrix in x is:\n0, 1, 2, 3 4, 5, 6, 7 After reshaping, the first 4x2 matrix in xshaped would be:\n0, 1 2, 3 4, 5 6, 7 Next, you are reshaping the rope_cache:\nrope_cache = rope_cache.view(1, xshaped.size(1), 1, xshaped.size(3), 2) This converts rope_cache from shape (4, 2, 2) to (1, 4, 1, 2, 2). This reshaping is done to align the dimensions of rope_cache with xshaped for broadcasting during the subsequent operations.\nStep 3:\nThen, you perform element-wise multiplication and subtraction/addition between the reshaped x and rope_cache:\nx_out2 = torch.stack( [ xshaped[..., 0] * rope_cache[..., 0] - xshaped[..., 1] * rope_cache[..., 1], xshaped[..., 1] * rope_cache[..., 0] + xshaped[..., 0] * rope_cache[..., 1], ], -1, ) This is similar to performing rotation using sine and cosine values from rope_cache. The resulting tensor x_out2 has the same shape as xshaped, which is (1, 4, 4, 2). Rotation operation in torch.stack would work element-wise over the tensors. This means that for each position in xshaped, it uses the corresponding position in rope_cache for the rotation calculation.\nBreakdown:\nGiven a 2D rotation matrix:\n$$ R(\\theta) = \\begin{bmatrix} \\cos(\\theta) \u0026 -\\sin(\\theta) \\ \\sin(\\theta) \u0026 \\cos(\\theta) \\end{bmatrix} $$\nWhen you multiply this rotation matrix with a 2D vector $([x, y]^T)$, you get:\n$$ R(\\theta) \\cdot \\begin{bmatrix} x \\ y \\end{bmatrix} = \\begin{bmatrix} x\\cos(\\theta) - y\\sin(\\theta) \\ x\\sin(\\theta) + y\\cos(\\theta) \\end{bmatrix} $$\nNow, let’s connect this to the operations in the code:\nThe first component of the output: $x’ = x\\cos(\\theta) - y\\sin(\\theta)$ is given by: xshaped[..., 0] * rope_cache[..., 0] - xshaped[..., 1] * rope_cache[..., 1] Where:\nxshaped[..., 0] corresponds to the x component (or the first value) of our vector. xshaped[..., 1] corresponds to the y component (or the second value) of our vector. rope_cache[..., 0] is the cosine of the rotation angle. rope_cache[..., 1] is the sine of the rotation angle. The second component of the output: $y’ = x\\sin(\\theta) + y\\cos(\\theta)$ is given by: xshaped[..., 1] * rope_cache[..., 0] + xshaped[..., 0] * rope_cache[..., 1] The code is essentially applying this rotation to every pair of values in the tensor xshaped using the angles specified in rope_cache.\nThe torch.stack(..., -1) at the end stacks these computed values along the last dimension. After this operation, for every pair of x and y values in the original xshaped, you have their rotated counterparts stacked together in the resulting tensor.\nInference For inference, we will be using the pipeline provided by the lit-lama repo. It provides some helpful classes that can potentially speed up the loading and initialization of large models, especially when only parts of the model need to be accessed or when specific tensor initializations are desired. The code also seems to handle some advanced features like quantization and lazy loading of tensors.\nlet’s break down these classes:\nEmptyInitOnDevice class:\nThis class is a context manager that changes the behavior of tensor initialization to create tensors with uninitialized memory (or “empty tensors”). Additionally, it can set specific devices and data types for tensor initialization and supports specific quantization modes. When this context is active, tensors are initialized without actually assigning them any initial values, making the initialization process faster in some scenarios.\nNotYetLoadedTensor class:\nRepresents a tensor that has not yet been loaded into memory. It is essentially a placeholder that can be transformed into an actual tensor when accessed or used in computations. This class can be especially useful when dealing with large datasets or models, as it allows for lazy loading of data, only loading tensors into memory when they’re actually needed.\nLazyLoadingUnpickler class:\nCustom unpickler for lazy loading. Pickling is the process of converting a Python object into a byte stream, and unpickling is the reverse operation. The idea here is to load tensors and related objects from the pickled format only when they’re actually accessed or used.\nimport sys import time import warnings from pathlib import Path from typing import Optional import lightning as L import torch from tokenizer import Tokenizer from utils import EmptyInitOnDevice, lazy_load, llama_model_lookup @torch.no_grad() def generate( model: torch.nn.Module, idx: torch.Tensor, max_new_tokens: int, max_seq_length: int, temperature: float = 1.0, top_k: Optional[int] = None, eos_id: Optional[int] = None, ) -\u003e torch.Tensor: \"\"\"Takes a conditioning sequence (prompt) as input and continues to generate as many tokens as requested. The implementation of this function is modified from A. Karpathy's nanoGPT. Args: model: The model to use. idx: Tensor of shape (T) with indices of the prompt sequence. max_new_tokens: The number of new tokens to generate. max_seq_length: The maximum sequence length allowed. temperature: Scales the predicted logits by 1 / temperature top_k: If specified, only sample among the tokens with the k highest probabilities eos_id: If specified, stop generating any more token once the token is triggered \"\"\" # create an empty tensor of the expected final shape and fill in the current tokens T = idx.size(0) T_new = T + max_new_tokens empty = torch.empty(T_new, dtype=idx.dtype, device=idx.device) empty[:T] = idx idx = empty # generate max_new_tokens tokens for t in range(T, T_new): # ignore the not-filled-yet tokens idx_cond = idx[:t] # if the sequence context is growing too long we must crop it at max_seq_length idx_cond = idx_cond if T \u003c= max_seq_length else idx_cond[-max_seq_length:] # forward logits = model(idx_cond.view(1, -1)) logits = logits[0, -1] / temperature # optionally crop the logits to only the top k options if top_k is not None: v, _ = torch.topk(logits, min(top_k, logits.size(-1))) logits[logits \u003c v[[-1]]] = -float(\"Inf\") probs = torch.nn.functional.softmax(logits, dim=-1) idx_next = torch.multinomial(probs, num_samples=1) # concatenate the new generation idx[t] = idx_next # if token is triggered, return the output (stop generation) if idx_next == eos_id: return idx[:t + 1] # include the EOS token return idx def main( prompt: str = \"Hello, my name is\", *, num_samples: int = 1, max_new_tokens: int = 50, top_k: int = 200, temperature: float = 0.8, checkpoint_path: Optional[Path] = None, tokenizer_path: Optional[Path] = None, quantize: Optional[str] = None, ) -\u003e None: \"\"\"Generates text samples based on a pre-trained LLaMA model and tokenizer. Args: prompt: The prompt string to use for generating the samples. num_samples: The number of text samples to generate. max_new_tokens: The number of generation steps to take. top_k: The number of top most probable tokens to consider in the sampling process. temperature: A value controlling the randomness of the sampling process. Higher values result in more random samples. checkpoint_path: The checkpoint path to load. tokenizer_path: The tokenizer path to load. quantize: Whether to quantize the model and using which method: ``\"llm.int8\"``: LLM.int8() mode, ``\"gptq.int4\"``: GPTQ 4-bit mode. \"\"\" if not checkpoint_path: checkpoint_path = Path(f\"./checkpoints/lit-llama/7B/lit-llama.pth\") if not tokenizer_path: tokenizer_path = Path(\"./checkpoints/lit-llama/tokenizer.model\") assert checkpoint_path.is_file(), checkpoint_path assert tokenizer_path.is_file(), tokenizer_path fabric = L.Fabric(devices=1) dtype = torch.bfloat16 if fabric.device.type == \"cuda\" and torch.cuda.is_bf16_supported() else torch.float32 print(\"Loading model ...\", file=sys.stderr) t0 = time.time() with lazy_load(checkpoint_path) as checkpoint: name = llama_model_lookup(checkpoint) with EmptyInitOnDevice( device=fabric.device, dtype=dtype, quantization_mode=quantize ): model = LLaMA.from_name(name) model.load_state_dict(checkpoint) print(f\"Time to load model: {time.time() - t0:.02f} seconds.\", file=sys.stderr) model.eval() model = fabric.setup_module(model) tokenizer = Tokenizer(tokenizer_path) encoded_prompt = tokenizer.encode(prompt, bos=True, eos=False, device=fabric.device) L.seed_everything(1234) for i in range(num_samples): t0 = time.perf_counter() y = generate( model, encoded_prompt, max_new_tokens, model.config.block_size, # type: ignore[union-attr,arg-type] temperature=temperature, top_k=top_k, ) t = time.perf_counter() - t0 print('\\\\n\\\\n') print(tokenizer.decode(y)) print('\\\\n\\\\n') print(f\"Time for inference {i + 1}: {t:.02f} sec total, {max_new_tokens / t:.02f} tokens/sec\", file=sys.stderr) if fabric.device.type == \"cuda\": print(f\"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\", file=sys.stderr) main(\"Artificial Intelligence is the\") Loading model ... Time to load model: 17.45 seconds. You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read Global seed set to 1234 Artificial Intelligence is the ability of a computer to imitate intelligent behaviour without being programmed, such as learning in a self-directed way to do a specific task, and then not just repeating the task, but improving itself. This is different from Traditional Artificial Intelligence which is any Time for inference 1: 1.41 sec total, 35.55 tokens/sec Memory used: 13.52 GB References LLaMa Original Paper : https://arxiv.org/abs/2302.13971 https://akgeni.medium.com/llama-concepts-explained-summary-a87f0bd61964 https://kikaben.com/llama-2023-02/ https://cameronrwolfe.substack.com/p/llama-llms-for-everyone https://vinija.ai/models/LLaMA/ Implementation code https://github.com/Lightning-AI/lit-llama ROPE Original Paper : https://arxiv.org/pdf/2104.09864.pdf https://blog.eleuther.ai/rotary-embeddings/ https://nn.labml.ai/transformers/rope/index.html#:~:text=This%20is%20an%20implementation%20of,incorporates%20explicit%20relative%20position%20dependency. https://serp.ai/rotary-position-embedding/ https://medium.com/@andrew_johnson_4/understanding-rotary-position-embedding-a-key-concept-in-transformer-models-5275c6bda6d0 https://github.com/lucidrains/rotary-embedding-torch http://krasserm.github.io/2022/12/13/rotary-position-embedding/ YouTube\nhttps://youtu.be/YMcwsLGU_U8 https://youtu.be/GQPOtyITy54 Swish https://blog.paperspace.com/swish-activation-function/#:~:text=Simply%20put,%20Swish%20is%20an,Function%20Approximation%20in%20Reinforcement%20Learning%22 https://medium.com/@neuralnets/swish-activation-function-by-google-53e1ea86f820#:~:text=Swish%20is%20a%20smooth,%20non,that%20actually%20creates%20the%20difference GLU https://medium.com/deeplearningmadeeasy/glu-gated-linear-unit-21e71cd52081 RMSNorm https://akgeni.medium.com/llama-concepts-explained-summary-a87f0bd61964 https://kikaben.com/llama-2023-02/ ",
  "wordCount" : "7143",
  "inLanguage": "en",
  "datePublished": "2023-01-20T00:00:00Z",
  "dateModified": "2023-01-20T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Rajan Ghimire"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://R4j4n.github.io/blogs/posts/llama/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Rajan Ghimire",
    "logo": {
      "@type": "ImageObject",
      "url": "https://R4j4n.github.io/blogs/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://R4j4n.github.io/blogs/" accesskey="h" title="Rajan Ghimire (Alt + H)">Rajan Ghimire</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://R4j4n.github.io/blogs/" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="https://R4j4n.github.io/blogs/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://R4j4n.github.io/blogs/">Home</a>&nbsp;»&nbsp;<a href="https://R4j4n.github.io/blogs/posts/">Posts</a></div>
    <h1 class="post-title">
      Understanding The Secret Sauce of LLaMA🦙
    </h1>
    <div class="post-description">
      Understanding the ins and outs of Meta&#39;s LLaMa(Open and Efficient Foundation Language Models)
    </div>
    <div class="post-meta"><span title='2023-01-20 00:00:00 +0000 UTC'>January 20, 2023</span>&nbsp;·&nbsp;34 min&nbsp;·&nbsp;Rajan Ghimire

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#foundations-of-llama-a-deeper-dive" aria-label="Foundations of LLaMa: A Deeper Dive"><strong>Foundations of LLaMa: A Deeper Dive</strong></a><ul>
                        
                <li>
                    <a href="#1-roperotary-position-embedding" aria-label="1. ROPE(Rotary Position Embedding)">1. ROPE(Rotary Position Embedding)</a><ul>
                        
                <li>
                    <a href="#apply-rotation-to-the-queries-and-keys" aria-label="Apply Rotation to the Queries and Keys:">Apply Rotation to the Queries and Keys:</a></li>
                <li>
                    <a href="#implementation" aria-label="Implementation">Implementation</a></li></ul>
                </li>
                <li>
                    <a href="#2-pre-normalization--rmsnorm" aria-label="2. Pre-normalization &amp;amp; RMSNorm">2. <strong>Pre-normalization &amp; RMSNorm</strong></a></li>
                <li>
                    <a href="#3-swiglu" aria-label="3. SwiGLU">3. <strong><strong>SwiGLU</strong></strong></a><ul>
                        
                <li>
                    <a href="#swish" aria-label="Swish">Swish</a></li>
                <li>
                    <a href="#gated-linear-unit-glu" aria-label="Gated Linear Unit (GLU)">Gated Linear Unit (GLU)</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#the-llama-in-pytorch" aria-label="The LLaMA in PyTorch"><strong>The LLaMA in PyTorch</strong></a></li>
                <li>
                    <a href="#inference" aria-label="Inference"><strong>Inference</strong></a></li>
                <li>
                    <a href="#references" aria-label="References"><strong>References</strong></a><ul>
                        
                <li>
                    <a href="#llama" aria-label="LLaMa">LLaMa</a></li>
                <li>
                    <a href="#implementation-code" aria-label="Implementation code">Implementation code</a></li>
                <li>
                    <a href="#rope" aria-label="ROPE">ROPE</a></li>
                <li>
                    <a href="#swish-1" aria-label="Swish">Swish</a></li>
                <li>
                    <a href="#glu" aria-label="GLU">GLU</a></li>
                <li>
                    <a href="#rmsnorm" aria-label="RMSNorm">RMSNorm</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><table>
<thead>
<tr>
<th style="text-align:center"><img loading="lazy" src="/blogs/img/llama/GLTS3gFAqRajBqSNzZKq--1--8x95p.jpg" alt="GLTS3gFAqRajBqSNzZKq&amp;ndash;1&amp;ndash;8x95p.jpg"  />
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><em>Modern Llama. Generated using Stable Diffusion v2</em></td>
</tr>
</tbody>
</table>
<p>In the information era, a new king reigns supreme: the language model. With the internet drowning in a never-ending flood of data, there is a growing demand for intelligent machines that can not only absorb this data but also produce, analyze, and interact with it in previously unthinkable ways.</p>
<p>Enter LLaMA, Meta’s Large Language Model, a monument to artificial intelligence’s current peak. But what lurks underneath the many layers of this behemoth? How can it understand the complexity of human language and respond with such amazing accuracy? If these questions have ever sparked your interest, you’ve come to the right spot.</p>
<p>A large language model, or LLM, is a digital polyglot that understands and generates human-like language based on huge databases. Consider a computer being that reads nearly every book, article, and paper ever published, embodying humanity’s cumulative knowledge. That is the strength of LLMs. While there are several large models out there, Meta’s LLaMA stands out, pioneering techniques that have revolutionized the field.</p>
<p>This blog will untangle the confusing strings that makeup LLaMa. We will travel through its major components, including ROPE (Rotary Position Embedding), RMSNorm, and SwiGLU, exploring both their theoretical foundations and practical uses. Our journey will not only scratch the surface but also we&rsquo;ll construct this model from the ground up, guided by the strong PyTorch Lightning’s implementation of LLaMa called <strong><a href="https://github.com/Lightning-AI/lit-llama">lit-llama</a>.</strong></p>
<p>I am utilizing the <a href="https://github.com/Lightning-AI/lit-llama">lit-lama</a> implementation of LLaMa primarily due to its open-source nature, which aligns with the ethos of transparent and accessible development. While it seamlessly integrates with the original LLaMa weights distributed by Meta for research purposes, what sets lit-lama apart is its independent implementation that covers the entire spectrum from pretraining and finetuning to inference. Notably, this entire repo is provided under the Apache 2.0 license, ensuring broad usability and adaptability for a variety of research and development scenarios.</p>
<p>Before we embark on this journey, it&rsquo;s crucial to have a solid understanding of the Transformer architecture, as this voyage assumes you&rsquo;re well-acquainted with its nuances. You can refer to these blogs to recall the concepts of Transformer. <a href="http://jalammar.github.io/illustrated-transformer/">1.</a> <a href="https://towardsdatascience.com/transformers-141e32e69591">2.</a> <a href="https://leimao.github.io/blog/Transformer-Explained/">3.</a></p>
<h2 id="foundations-of-llama-a-deeper-dive"><strong>Foundations of LLaMa: A Deeper Dive</strong><a hidden class="anchor" aria-hidden="true" href="#foundations-of-llama-a-deeper-dive">#</a></h2>
<h3 id="1-roperotary-position-embedding">1. ROPE(Rotary Position Embedding)<a hidden class="anchor" aria-hidden="true" href="#1-roperotary-position-embedding">#</a></h3>
<p>Transformers are widely used models for numerous NLP tasks. However, these models do not naturally comprehend the sequence order (in the context of NLP, the order of words). This is where position embeddings become crucial. Position embeddings identify the position of each token in the sequence. These are incorporated with the token embeddings before being input into the Transformer model, enabling the model to grasp the sequence order.</p>
<p>The majority of methods incorporate position data into token representations by addition operation to include either absolute or relative position information.</p>
<p>$$
\begin{align}\mathbf{q}_m &amp;= \mathbf{W}_q(\mathbf{x}_m + \mathbf{p}_m) \\\mathbf{k}_n &amp;= \mathbf{W}_k(\mathbf{x}_n + \mathbf{p}_n)\\
\mathbf{v}_n &amp;= \mathbf{W}_v(\mathbf{x}_n + \mathbf{p}_n) \\\end{align}
$$</p>
<p>This approach has a few drawbacks:</p>
<ul>
<li>The vanilla positional encoding is designed for a fixed maximum sequence length. If you have a more extended sequence than the maximum length used during training, handling it becomes problematic. You might need to truncate, split, or find another way to fit it within the maximum length. A model trained with a particular maximum sequence length may not generalize well to sequences of very different lengths, even if they&rsquo;re within the allowed range. The positional encoding for these lengths might be outside the distribution seen during training.</li>
<li>The sinusoidal nature of the positional encoding might not always be optimal for capturing very long-term dependencies in long sequences. While self-attention theoretically allows for such connections, in practice, the model might still struggle due to the fixed nature of the encoding.</li>
</ul>
<p>Why RoPE?</p>
<ul>
<li>A key advantage of utilizing rotary embedding is its adaptability. In contrast to position embeddings that are restricted to a certain sequence length, RoPE can be expanded to accommodate any sequence length. This makes RoPE useful for models that must handle text of diverse lengths.</li>
<li>Rotary embeddings equip linear self-attention with relative position encoding. This implies that models can consider the relative positions of tokens when executing self-attention. This could result in more precise predictions and a deeper understanding of the relationships between tokens.</li>
</ul>
<p><em><strong>RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in the self-attention formulation.</strong></em></p>
<p>The concept behind the rope is straightforward. Rather than combining the token and position embedding into a single entity by adding them together and calculating q, we should look for transformations for q at m that yield a new vector. Essentially, we rotate the affine-transformed word embedding vector by an amount equivalent to multiples of its position index.</p>
<p>In the original attention, the matrix multiplication between query and key matrices only involves the weight matrices W and the input embedding x.</p>
<p>$$
\begin{aligned}
&amp; a_{m, n}=q_m^T k_n=\left[W_q\left(x_m+P E(m)\right)\right]^T W_k\left(x_n+P E(n)\right) \\
\end{aligned}
$$</p>
<p>Replace the positional encoding from the vanilla transformer.</p>
<p>$$
\begin{aligned}
&amp; a_{m, n}=q_m^T k_n=\left[W_q x_m\right]^T W_k x_n \\\
\end{aligned}
$$</p>
<p>Rotate word embedding vector by an amount equivalent to multiples of its position index.</p>
<p>$$
\begin{aligned}&amp; a_{m, n}=q_m^T k_n=\left[R_{\Theta, d}^m\left(W_q x_m\right)\right]^T R_{\Theta, d}^n W_k x_n \\\end{aligned}
$$</p>
<p>$$
\begin{aligned}&amp; a_{m, n}=q_m^T k_n=\left(W_q x_m\right)^T R_{\Theta, d}^m{ }^T R_{\Theta, d}^n W_k x_n \\\end{aligned}
$$</p>
<p>$$
\begin{aligned}&amp; a_{m, n}=q_m^T k_n=x_m^T W_q^T\left[R_{\Theta, d}^m{ }^T R_{\Theta, d}^n\right] W_k x_n\end{aligned}
$$</p>
<p>Here, $R_{\Theta, m}^d$ is a rotation matrix. In 2D this matrix is defined as:</p>
<p>$$
R_{\Theta, m}^d=\left(\begin{array}{cc}\cos \left(m \theta_i\right) &amp; -\sin \left(m \theta_i\right) \\sin \left(m \theta_i\right) &amp; \cos \left(m \theta_i\right)\end{array}\right)
$$</p>
<p>Where $\theta$ is a nonzero constant.</p>
<p>The rotation matrix is a function of absolute position. Calculating the inner products of rotated queries and keys results in an attention matrix that is a function of relative position information only.</p>
<p><strong>Attention is relative</strong> but how? <a href="https://nn.labml.ai/transformers/rope/index.html#:~:text=This%20is%20an%20implementation%20of,incorporates%20explicit%20relative%20position%20dependency">source</a></p>
<p>$$
\begin{align}\Big \langle RoPE\big(x^{(1)}_m, x^{(2)}_m, m\big),  RoPE\big(x^{(1)}_n, x^{(2)}_n, n\big) \Big \rangle &amp;= \\(x^{(1)}_m \cos m\theta - x^{(2)}_m \sin m \theta)(x^{(1)}_n \cos n\theta - x^{(2)}_n \sin n \theta) &amp;+ \\(x^{(2)}_m \cos m\theta + x^{(1)}_m \sin m \theta)(x^{(2)}_n \cos n\theta + x^{(1)}_n \sin n \theta) &amp;= \\x^{(1)}_m x^{(1)}_n (\cos m\theta \cos n\theta + \sin m \theta \sin n \theta) &amp;+ \\x^{(1)}_m x^{(2)}_n (-\cos m\theta \sin n\theta + \sin m \theta \cos n \theta) &amp;+ \\x^{(2)}_m x^{(1)}_n (-\sin m\theta \cos n\theta + \cos m \theta \sin n \theta) &amp;+ \\x^{(2)}_m x^{(2)}_n (\sin m\theta \sin n\theta + \cos m \theta \cos n \theta) &amp;= \\
x^{(1)}_m x^{(1)}_n \cos (m - n) \theta +x^{(1)}_m x^{(2)}_n \sin(m - n) \theta &amp;+ \\- x^{(2)}_m x^{(1)}_n \sin (m - n) \theta +x^{(2)}_m x^{(1)}_n \cos (m - n) \theta &amp;= \\
\big(x^{(1)}_m \cos (m - n)\theta - x^{(2)}_m \sin (m - n) \theta\big) x^{(1)}_n &amp;+ \\\big(x^{(2)}_m \cos (m - n)m\theta + x^{(1)}_m \sin (m - n) \theta\big) x^{(2)}_n  &amp;= \\
\Big \langle RoPE\big(x^{(1)}_m, x^{(2)}_m, m - n\big),  RoPE\big(x^{(1)}_n, x^{(2)}_n, 0\big) \Big \rangle
\end{align}
$$</p>
<p>This shows that for dot-production attention the rotary encoding gives relative attention.
so,</p>
<p><img loading="lazy" src="/blogs/img/llama/2023-08-20_15-01.png" alt="code2.png"  />
</p>
<table>
<thead>
<tr>
<th style="text-align:center"><img loading="lazy" src="/blogs/img/llama/ROPE2.png" alt="ROPE2.png"  />
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><em>Rotary position embedding Overview</em></td>
</tr>
</tbody>
</table>
<p>A block-diagonal matrix is a generalization of the 2D rotation matrix to a d-dimensional space. This divides the space into $d/2$ subspace each having different value of $\Theta = {\theta_i = 10000^{-\frac{2(i-1)}{d}}, i \in [1, 2, &hellip;, \frac{d}{2}]}$</p>
<p><img loading="lazy" src="/blogs/img/llama/2023-08-20_15-00.png" alt="code2.png"  />
</p>
<h4 id="apply-rotation-to-the-queries-and-keys">Apply Rotation to the Queries and Keys:<a hidden class="anchor" aria-hidden="true" href="#apply-rotation-to-the-queries-and-keys">#</a></h4>
<ul>
<li>Before computing attention scores, the goal is to rotate the query (Q) and key (K) vectors. This rotation is done using the aforementioned sinusoidal values.</li>
<li>Given a query or key vector $v$ with two parts $v_1$ and $v_2$, the rotated vectors $v&rsquo;_1$ and $v&rsquo;_2$ are:</li>
</ul>
<p><img loading="lazy" src="/blogs/img/llama/2023-08-20_15-03.png" alt="code2.png"  />
</p>
<p>The  ${sin_{value}}$ and ${cos_{value}}$ will depend on the position $m$ in the given sequence and the dimension of features.</p>
<p>The following figure illustrates the above concept:</p>
<table>
<thead>
<tr>
<th style="text-align:center"><img loading="lazy" src="/blogs/img/llama/2023-08-20_15-11.png" alt="Rope.png"  />
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><em>Implementation of Rotary Position Embedding(RoPE) from original paper.</em><a href="https://arxiv.org/pdf/2104.09864.pdf">Link</a></td>
</tr>
</tbody>
</table>
<h4 id="implementation">Implementation<a hidden class="anchor" aria-hidden="true" href="#implementation">#</a></h4>
<p>We can see above that ${R}{\Theta, m}^d$ <em>is sparse. A more computationally efficient realization of a
multiplication of  ${R}{\Theta, m}^d$</em>  and $x \in 	\mathbb{R}^d$. Where vector $x$ is a query or a key.</p>
<p><img loading="lazy" src="/blogs/img/llama/2023-08-20_14-58.png" alt="code2.png"  />
</p>
<p>As we know that in multi-headed attention, queries and keys are spitted into heads. For each head, we apply the ROPE encodings. Let&rsquo;s implement ROPE for a single attention head in pure PyTorch.</p>
<p>As we can see from the above equation, the $cos$ and $sin$ values for each head will remain the same, so let&rsquo;s cache these values.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">build_cache</span>(dim : int, max_seq_len : int):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Given a dimension and maximum sequence length, this function returns a tensor 
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    containing position encodings obtained by multiplying position indexes with inverse frequencies. 
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    The computed encoding is then duplicated along the inverse frequency dimension.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Parameters:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    - dim (int): Dimensionality for the position encoding, typically the model&#39;s hidden dimension size. 
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">                It determines the number of theta values computed.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    - max_seq_len (int): The maximum sequence length for which to compute position encodings.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    - torch.Tensor: A tensor of shape (max_seq_len, dim) containing the position encodings.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Examples:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &gt;&gt;&gt; enc = build_cache(512, 100)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &gt;&gt;&gt; enc.shape
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    torch.Size([100, 512])
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># inverse frequencies [theta_1, theta_2, ..., theta_dim/2]</span>
</span></span><span style="display:flex;"><span>    inv_freq <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span> <span style="color:#f92672">/</span> (<span style="color:#ae81ff">10000</span> <span style="color:#f92672">**</span> (torch<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, dim, <span style="color:#ae81ff">2</span>, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float) <span style="color:#f92672">/</span> dim))  <span style="color:#75715e"># -&gt; mθ</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Compute the sequence of position indexes.</span>
</span></span><span style="display:flex;"><span>    pos <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>arange(max_seq_len)  <span style="color:#75715e"># -&gt; m</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Compute the product of the position index and the theta values:</span>
</span></span><span style="display:flex;"><span>    pos_enc <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>einsum(<span style="color:#e6db74">&#34;n,d-&gt;nd&#34;</span>, pos, inv_freq) <span style="color:#75715e"># -&gt; (mθ)</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># duplicate each element along inverse frequency dimension</span>
</span></span><span style="display:flex;"><span>    pos_enc <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([pos_enc, pos_enc], dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>) <span style="color:#75715e"># -&gt; (mθ,mθ)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> pos_enc
</span></span></code></pre></div><p>This <code>build_cache</code> code block will get the product between the position and feature dimension present at that position. We can see this in the following figure:</p>
<p><img loading="lazy" src="/blogs/img/llama/code1.png" alt="code1.png"  />
</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">negative_half</span>(input_tensor : torch<span style="color:#f92672">.</span>tensor):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Applies a specific rotation to the input tensor, often referred to as a &#34;half rotation&#34; 
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    in the context of tensor operations.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Given an input tensor with pairs of elements in its last dimension, this function 
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    rotates them such that for each pair (u1, u2), it outputs (-u2, u1). As a result, 
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    the sequence [u1, u2, u3, u4, ...] is transformed to [-u2, u1, -u4, u3, ...].
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Parameters:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    - input_tensor (torch.Tensor): The input tensor to be processed. The rotation operation 
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">                                is applied on the last dimension of this tensor. 
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    - torch.Tensor: The rotated tensor with the same shape as the input.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Examples:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &gt;&gt;&gt; tensor = torch.tensor([1.0, 2.0, 3.0, 4.0])
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &gt;&gt;&gt; rotated_tensor = negative_half(tensor)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &gt;&gt;&gt; print(rotated_tensor)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    tensor([-2.,  1., -4.,  3.])
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Reshaping the tensor so that pairs [u1, u2], [u3, u4], ... are separated</span>
</span></span><span style="display:flex;"><span>    u <span style="color:#f92672">=</span>  input_tensor<span style="color:#f92672">.</span>float()<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">*</span>input_tensor<span style="color:#f92672">.</span>shape[:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>], <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Separating the pairs into two tensors</span>
</span></span><span style="display:flex;"><span>    u1 <span style="color:#f92672">=</span> u[<span style="color:#f92672">...</span>, <span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>    u2 <span style="color:#f92672">=</span> u[<span style="color:#f92672">...</span>, <span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Reconstructing the tensor after rotation</span>
</span></span><span style="display:flex;"><span>    u_rotated <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>stack((<span style="color:#f92672">-</span>u2, u1), dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Flattening the last two dimensions to get [-u2, u1, -u4, u3, ...]</span>
</span></span><span style="display:flex;"><span>    u_rotated <span style="color:#f92672">=</span> u_rotated<span style="color:#f92672">.</span>view(<span style="color:#f92672">*</span>u_rotated<span style="color:#f92672">.</span>shape[:<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>], <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> u_rotated
</span></span></code></pre></div><p>The <code>negative_half</code>  method will shuffle the elements of $x$ in the second term of the addition is helpful. This method is basically calculating this part:</p>
<p><img loading="lazy" src="/blogs/img/llama/code2.png" alt="code2.png"  />
</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">rotate</span>(input_tensor : torch<span style="color:#f92672">.</span>tensor, pos_enc : int):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Applies a rotation-based position encoding to the input tensor.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    The function multiplies the input tensor with the cosine of the position encoding 
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    and adds the result of multiplying the negative half of the input tensor with 
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    the sine of the position encoding. This is similar to rotating vectors in a 
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    complex space using Euler&#39;s formula.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Parameters:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    - input_tensor (torch.Tensor): The tensor to which the rotation-based position encoding 
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">                                  will be applied. It is assumed that the second-to-last 
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">                                  dimension represents tokens or sequence length.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    - pos_enc (int): Position encoding tensor with values computed elsewhere. The first 
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">                     dimension of this tensor should be at least as long as the number of 
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">                     tokens in the `input_tensor`.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">                     
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    - torch.Tensor: The position-encoded tensor with the same shape as the `input_tensor`.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Examples:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &gt;&gt;&gt; tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]])
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &gt;&gt;&gt; pos_enc_tensor = torch.tensor([1.0, 2.0, 3.0])
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &gt;&gt;&gt; rotated_tensor = rotate(tensor, pos_enc_tensor)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &gt;&gt;&gt; print(rotated_tensor)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    num_tokens <span style="color:#f92672">=</span> input_tensor<span style="color:#f92672">.</span>shape[<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>]
</span></span><span style="display:flex;"><span>    pos_enc <span style="color:#f92672">=</span> pos_enc[:num_tokens]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> input_tensor <span style="color:#f92672">*</span> pos_enc<span style="color:#f92672">.</span>cos() <span style="color:#f92672">+</span> (negative_half(input_tensor) <span style="color:#f92672">*</span> pos_enc<span style="color:#f92672">.</span>sin())
</span></span></code></pre></div><p>Complete use case:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn.functional <span style="color:#66d9ef">as</span> F
</span></span><span style="display:flex;"><span>batch_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>num_heads <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span>
</span></span><span style="display:flex;"><span>seq_len <span style="color:#f92672">=</span> <span style="color:#ae81ff">16</span>
</span></span><span style="display:flex;"><span>head_dim <span style="color:#f92672">=</span> seq_len <span style="color:#f92672">//</span> num_heads
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># random queries and keys</span>
</span></span><span style="display:flex;"><span>q <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(batch_size, num_heads, seq_len, head_dim)
</span></span><span style="display:flex;"><span>k <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(batch_size, num_heads, seq_len, head_dim)
</span></span><span style="display:flex;"><span>v <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(batch_size, num_heads, seq_len, head_dim)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># frequency-encode positions 1 - 512</span>
</span></span><span style="display:flex;"><span>pos_enc <span style="color:#f92672">=</span> build_cache(dim<span style="color:#f92672">=</span>head_dim, max_seq_len<span style="color:#f92672">=</span>seq_len)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># encode absolute positions into queries and keys</span>
</span></span><span style="display:flex;"><span>q_rot <span style="color:#f92672">=</span> rotate(q, pos_enc)
</span></span><span style="display:flex;"><span>k_rot <span style="color:#f92672">=</span> rotate(k, pos_enc)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Step 1: Compute scaled dot-product attention</span>
</span></span><span style="display:flex;"><span>attn_weights <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>einsum(<span style="color:#e6db74">&#39;bhsd,bhtd-&gt;bhst&#39;</span>, q_rot, k_rot)  <span style="color:#75715e"># dot product of queries and keys</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Step 2: Scale the dot products</span>
</span></span><span style="display:flex;"><span>attn_weights <span style="color:#f92672">=</span> attn_weights <span style="color:#f92672">/</span> (head_dim <span style="color:#f92672">**</span> <span style="color:#ae81ff">0.5</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Step 3: Apply softmax to get the weights</span>
</span></span><span style="display:flex;"><span>attn_weights <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>softmax(attn_weights, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Step 4: Multiply the weights by the values to get the output</span>
</span></span><span style="display:flex;"><span>output <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>einsum(<span style="color:#e6db74">&#39;bhst,bhtd-&gt;bhsd&#39;</span>, attn_weights, v)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(output<span style="color:#f92672">.</span>shape)
</span></span></code></pre></div><h3 id="2-pre-normalization--rmsnorm">2. <strong>Pre-normalization &amp; RMSNorm</strong><a hidden class="anchor" aria-hidden="true" href="#2-pre-normalization--rmsnorm">#</a></h3>
<table>
<thead>
<tr>
<th style="text-align:center"><img loading="lazy" src="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F446d37f0-43ab-417a-81c0-ab75b4b5aa5a_1124x840.png" alt="Rope.png"  />
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><em>Pre-normalization structure within a transformer block</em> <a href="https://cameronrwolfe.substack.com/p/llama-llms-for-everyone">Source</a></td>
</tr>
</tbody>
</table>
<p>Traditional transformer applies layer normalization after attention and MLP(feed-forward) layer.</p>
<p>LLAMA uses pre-normalization to normalize the input to each transformer sub-layer instead of normalizing the output.  The main goal of pre-normalization is to improve the efficiency and stability of the training process by normalizing and reducing variation and correlation of the input features.</p>
<p>In neural networks, the result from the first layer is fed into the second layer, and the result from the second layer is fed into the third, and this process continues. When there are changes in a layer&rsquo;s parameters, the distribution of inputs for the following layers also changes. This change in distribution is called <strong>internal covariate shift.</strong></p>
<p>As per the authors of Batch Normalization :</p>
<p><em><strong><a href="https://arxiv.org/pdf/1502.03167.pdf">We define Internal Covariate Shift as the change in the distribution of network activations due to the change in network parameters during training</a>.</strong></em></p>
<p>This shift can slow down training or cause instability. Normalization helps to stabilize the distributions of layer activations throughout training.</p>
<p>We can mitigate this issue by normalizing the input to the activation functions.</p>
<p>LLAMA uses RMSNorm (Root Mean Square Layer Normalization), an extension of Layer Normalization. RMSNorm is more simpler and efficient than Layer Normalization but achieves similar performance.</p>
<p><strong>RMSNorm</strong></p>
<p>Let&rsquo;s understand Layer Normalization first:</p>
<p>Layer Normalization for a fully connected layer $i$, neuron $j$ and number of neurons in a layer $m$ :</p>
<p>Calculate mean :</p>
<p>$$
mean_{i}=\frac{1}{m} \sum_{l=1}^{m} \sigma_{i l}
$$</p>
<p>Calculate Variance :</p>
<p>$$
var_{i}=\frac{1}{m} \sum_{l=1}^{m}\left(\sigma_{i l}-\text { mean }_{i}\right)^{2}
$$</p>
<p>Normalize the feature $j$:</p>
<p>$$
\hat{\sigma}_{i j} = \frac{ \sigma_ij-mean_i}{\sqrt{var_i+\epsilon}}
$$</p>
<p>Shift and scale the normalized feature:</p>
<p><img loading="lazy" src="/blogs/img/llama/2023-08-20_15-04.png" alt="code2.png"  />
</p>
<p>Where  $\gamma_{i j},\beta_{i,j}$  are learnable parameters. Layer Normalization is successful due to it’s two properties:</p>
<ol>
<li><strong>Re-centering</strong>:
<ul>
<li>For each individual data sample, compute the mean of its features.</li>
<li>Subtract this mean from each feature of the data sample. This ensures that the features have a mean of zero.</li>
</ul>
</li>
<li><strong>Re-scaling</strong>:
<ul>
<li>For the same data sample, after re-centering, compute the variance of its features.</li>
<li>Normalize each feature by dividing it by the square root of this variance (plus a small epsilon for numerical stability). This ensures that the features have unit variance.</li>
<li>After the normalization, the features are typically re-scaled by a learnable parameter and shifted by another learnable parameter. Specifically, if   $\hat{x}$  is the normalized activation, the final output will be $y = \gamma \hat{x} + \beta$, where $\gamma$ and $\beta$  are learnable scaling and shifting parameters, respectively.</li>
</ul>
</li>
</ol>
<p>The authors of RMSNorm theorize that the key to the success of layer normalization lies in the re-scaling. They suggest RMSNorm, a method that normalizes the input to maintain re-scaling invariance without re-centering the input.</p>
<p><img loading="lazy" src="/blogs/img/llama/2023-08-20_14-57.png" alt="code2.png"  />
</p>
<p>$$
 \operatorname{RMS}(\boldsymbol{a})=\sqrt{\frac{1}{n} \sum_{i=1}^{n} a_{i}^{2}}
$$</p>
<p>RMSNorm performs at a level similar to LayerNorm, but it reduces the operational time by approximately 10% to 60%.</p>
<p>PyTorch implementation of RMSNorm:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">RMSNorm</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, input_dim , eps <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-6</span>) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>g <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Parameter(torch<span style="color:#f92672">.</span>ones(input_dim))
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>eps <span style="color:#f92672">=</span> eps
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self,x):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># RMS of input</span>
</span></span><span style="display:flex;"><span>        rms <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>rsqrt(torch<span style="color:#f92672">.</span>square(x)<span style="color:#f92672">.</span>mean(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>,keepdim<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>) <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>eps)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># rescaling </span>
</span></span><span style="display:flex;"><span>        x  <span style="color:#f92672">=</span> x <span style="color:#f92672">*</span> rms
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>g
</span></span></code></pre></div><h3 id="3-swiglu">3. <strong><strong>SwiGLU</strong></strong><a hidden class="anchor" aria-hidden="true" href="#3-swiglu">#</a></h3>
<p>SwiGLU is a combination of the Swish activation function and the Gated Linear Unit (GLU) concept. It was introduced in the paper <strong>&ldquo;GLU Variants Improve Transformer&rdquo;</strong> (Sho Takase, Naoaki Okazaki, 2020). The authors propose several variations of the standard GLU that can improve performance on machine translation tasks when used in a Transformer model.</p>
<p>The SwiGLU variant is defined as:</p>
<p>$$
SwiGLU(x, x&rsquo;) = x ⊙ Swish(x&rsquo;)
$$</p>
<p>where ⊙ is the element-wise multiplication operation, and x&rsquo; is the transformed input (generally, a linear transformation of the input x).</p>
<h4 id="swish">Swish<a hidden class="anchor" aria-hidden="true" href="#swish">#</a></h4>
<p>The paper Swish: a Self-Gated Activation Function proposes Swish, also a smooth version of ReLU with a non-zero gradient for negative values.</p>
<p>Swish is a smooth, non-monotonic function that consistently matches or outperforms ReLU.</p>
<p>Simply put, Swish is an extension of the <strong>SILU</strong> activation function.
<strong>SILU&rsquo;s</strong> formula $f(x) = x * sigmoid(x)$. The slight modification made in the Swish formulation is the addition of a trainable
β parameter, making it $f(x)=xsigmoid(\beta x)$</p>
<p>Swish has several unique characteristics that make it better than ReLU.</p>
<ul>
<li>First, Swish is a smooth, continuous function, in contrast to ReLU, which is a piecewise linear function.</li>
<li>Swish permits a small number of negative weights to pass through, while ReLU sets all negative weights to zero. This property is vital and contributes significantly to the success of non-monotonic smooth activation functions, such as Swish, particularly when used in progressively deeper neural networks.
<em>(A non-monotonic function is a type of function that does not consistently increase or decrease in value. In other words, as you move from left to right along the x-axis, a non-monotonic function can either increase or decrease at different points, not following a single direction throughout its domain.)</em></li>
<li>The trainable parameter  $\beta$ enables the activation function to be fine-tuned more effectively to optimize information propagation and push for smoother gradients.</li>
</ul>
<p>PyTorch implementation:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Swish</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, beta: torch<span style="color:#f92672">.</span>Tensor):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>beta <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Parameter(beta)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x <span style="color:#f92672">*</span> torch<span style="color:#f92672">.</span>sigmoid(self<span style="color:#f92672">.</span>beta <span style="color:#f92672">*</span> x)
</span></span></code></pre></div><h4 id="gated-linear-unit-glu">Gated Linear Unit (GLU)<a hidden class="anchor" aria-hidden="true" href="#gated-linear-unit-glu">#</a></h4>
<p>GLU (Gated Linear Units) is a layer within a neural network, rather than a strict activation function. It involves a linear transformation followed by a gating process. This gating process is controlled by a sigmoid function that manages the information flow from the linear transformation.</p>
<p>$$
h_{l}(\mathbf{X})=(\mathbf{X} * \mathbf{W}+\mathbf{b}) \otimes \sigma(\mathbf{X} * \mathbf{V}+\mathbf{c})
$$</p>
<p>$\sigma$ means the sigmoid function. So we have two sets of weights W and V, and two biases, b, and c.</p>
<p>Here is the most intuitive example of GLU I found <a href="https://medium.com/deeplearningmadeeasy/glu-gated-linear-unit-21e71cd52081">HERE</a>.</p>
<p>The idea is simple. I want to allow the network to decide how much information should flow through a given path, like a logical gate, hence the name. How?</p>
<ul>
<li>If we multiply X by 0, nothing passes.</li>
<li>If we multiply X by 1, everything passes.</li>
<li>If we multiply X by 0.5, half of it passes.</li>
</ul>
<p>It’s inspired by the idea of the gates of LSTMs but applied to convolutions and linear layers, but it’s the same idea.</p>
<p>PyTorch implementation:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">GLU</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, in_size) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>linear1  <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(in_size, in_size)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>linear2  <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(in_size, in_size)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, X):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>linear1(X) <span style="color:#f92672">*</span> torch<span style="color:#f92672">.</span>sigmoid(self<span style="color:#f92672">.</span>linear2(X))
</span></span></code></pre></div><p>As we now have a clear understanding of the building blocks of SwiGLU, let&rsquo;s implement it on PyTorch.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Swish</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, beta: torch<span style="color:#f92672">.</span>Tensor):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>beta <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Parameter(beta)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x <span style="color:#f92672">*</span> torch<span style="color:#f92672">.</span>sigmoid(self<span style="color:#f92672">.</span>beta <span style="color:#f92672">*</span> x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SwiGLU</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, in_size, beta: torch<span style="color:#f92672">.</span>Tensor) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>linear1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(in_size, in_size)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>linear2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(in_size, in_size)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>swish <span style="color:#f92672">=</span> Swish(beta)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, X):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>linear1(X) <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>swish(self<span style="color:#f92672">.</span>linear2(X))
</span></span></code></pre></div><hr>
<h2 id="the-llama-in-pytorch"><strong>The LLaMA in PyTorch</strong><a hidden class="anchor" aria-hidden="true" href="#the-llama-in-pytorch">#</a></h2>
<p>The code used below can be found : <a href="https://github.com/R4j4n/lit-llama">HERE</a></p>
<table>
<thead>
<tr>
<th style="text-align:center"><img loading="lazy" src="/blogs/img/llama/s1ndoDYFA2jtjrbOH75n--1--9hegs.jpg" alt="s1ndoDYFA2jtjrbOH75n&amp;ndash;1&amp;ndash;9hegs.jpg"  />
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><em>Humans learning from LLaMa.Generated using Stable Diffusion 2.0</em></td>
</tr>
</tbody>
</table>
<p>As mentioned earlier, ill be using a fork of lit-lama repo by Lightning AI. As we have now grasped the fundamental building blocks of LLaMa, let&rsquo;s get started with the code.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> math
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> dataclasses <span style="color:#f92672">import</span> dataclass
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> typing <span style="color:#f92672">import</span> Optional
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> torch.nn <span style="color:#f92672">import</span> functional <span style="color:#66d9ef">as</span> F
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> typing_extensions <span style="color:#f92672">import</span> Self
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> utils <span style="color:#f92672">import</span> find_multiple
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>llama_configs <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;7B&#34;</span>: dict(n_layer<span style="color:#f92672">=</span><span style="color:#ae81ff">32</span>, n_head<span style="color:#f92672">=</span><span style="color:#ae81ff">32</span>, n_embd<span style="color:#f92672">=</span><span style="color:#ae81ff">4096</span>),
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;13B&#34;</span>: dict(n_layer<span style="color:#f92672">=</span><span style="color:#ae81ff">40</span>, n_head<span style="color:#f92672">=</span><span style="color:#ae81ff">40</span>, n_embd<span style="color:#f92672">=</span><span style="color:#ae81ff">5120</span>),
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;30B&#34;</span>: dict(n_layer<span style="color:#f92672">=</span><span style="color:#ae81ff">60</span>, n_head<span style="color:#f92672">=</span><span style="color:#ae81ff">52</span>, n_embd<span style="color:#f92672">=</span><span style="color:#ae81ff">6656</span>),
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;65B&#34;</span>: dict(n_layer<span style="color:#f92672">=</span><span style="color:#ae81ff">80</span>, n_head<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>, n_embd<span style="color:#f92672">=</span><span style="color:#ae81ff">8192</span>),
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>These are the different variants of the LLaMa models.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a6e22e">@dataclass</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">LLaMAConfig</span>:
</span></span><span style="display:flex;"><span>    block_size: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">2048</span>
</span></span><span style="display:flex;"><span>    vocab_size: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">32000</span>
</span></span><span style="display:flex;"><span>    padded_vocab_size: Optional[int] <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>    n_layer: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">32</span>
</span></span><span style="display:flex;"><span>    n_head: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">32</span>
</span></span><span style="display:flex;"><span>    n_embd: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">4096</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__post_init__</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>padded_vocab_size <span style="color:#f92672">is</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>padded_vocab_size <span style="color:#f92672">=</span> find_multiple(self<span style="color:#f92672">.</span>vocab_size, <span style="color:#ae81ff">64</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">@classmethod</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">from_name</span>(cls, name: str) <span style="color:#f92672">-&gt;</span> Self:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> cls(<span style="color:#f92672">**</span>llama_configs[name])
</span></span></code></pre></div><p>The <code>LLaMAConfig</code> class is used to store class variables.<!-- raw HTML omitted -->
Let&rsquo;s understand each of the class variables:<!-- raw HTML omitted --></p>
<ul>
<li><code>block_size</code> : Represents the maximum sequence length the language model can process.</li>
<li><code>vocab_size</code> : Represents the size of vocabulary the large language model was trained on.</li>
<li><code>n_layer</code> : Represents total number of transformer block.</li>
<li><code>n_head</code> : Represents the total number of heads in each transformer block.</li>
<li><code>n_embd</code> : Represents the size of embedding.</li>
</ul>
<p>According to <a href="https://twitter.com/karpathy/status/1621578354024677377/">this</a> tweet of <strong>Andrej Karpathy</strong>, it is important to find the nearest multiple of 64 for your vocab. The tweet explains: <!-- raw HTML omitted --></p>
<p><em>The most dramatic optimization to nanoGPT so far (~25% speedup) is to simply increase vocab size from 50257 to 50304 (nearest multiple of 64). This calculates added useless dimensions but goes down a different kernel path with much higher occupancy. Careful with your Powers of 2.</em></p>
<p>You can also read more about it <a href="https://pytorch.org/blog/accelerating-large-language-models/">HERE</a>.</p>
<pre tabindex="0"><code>
def __post_init__(self):
    if self.padded_vocab_size is None:
        self.padded_vocab_size = find_multiple(self.vocab_size, 64)
</code></pre><p>This code block initializes the padded_vocab_size attribute of an object to a multiple of 64 based on the object&rsquo;s vocab_size, but only if padded_vocab_size is not already set.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">LLaMA</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, config: LLaMAConfig) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">assert</span> config<span style="color:#f92672">.</span>padded_vocab_size <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>config <span style="color:#f92672">=</span> config
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>lm_head <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(config<span style="color:#f92672">.</span>n_embd, config<span style="color:#f92672">.</span>padded_vocab_size, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>transformer <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleDict(
</span></span><span style="display:flex;"><span>            dict(
</span></span><span style="display:flex;"><span>                wte<span style="color:#f92672">=</span>nn<span style="color:#f92672">.</span>Embedding(config<span style="color:#f92672">.</span>padded_vocab_size, config<span style="color:#f92672">.</span>n_embd),
</span></span><span style="display:flex;"><span>                h<span style="color:#f92672">=</span>nn<span style="color:#f92672">.</span>ModuleList([Block(config) <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(config<span style="color:#f92672">.</span>n_layer)]),
</span></span><span style="display:flex;"><span>                ln_f<span style="color:#f92672">=</span>RMSNorm(config<span style="color:#f92672">.</span>n_embd),
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_init_weights</span>(self, module: nn<span style="color:#f92672">.</span>Module) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> isinstance(module, nn<span style="color:#f92672">.</span>Linear):
</span></span><span style="display:flex;"><span>            torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>normal_(module<span style="color:#f92672">.</span>weight, mean<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, std<span style="color:#f92672">=</span><span style="color:#ae81ff">0.02</span> <span style="color:#f92672">/</span> math<span style="color:#f92672">.</span>sqrt(<span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>n_layer))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">elif</span> isinstance(module, nn<span style="color:#f92672">.</span>Embedding):
</span></span><span style="display:flex;"><span>            torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>normal_(module<span style="color:#f92672">.</span>weight, mean<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, std<span style="color:#f92672">=</span><span style="color:#ae81ff">0.02</span> <span style="color:#f92672">/</span> math<span style="color:#f92672">.</span>sqrt(<span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>n_layer))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, idx: torch<span style="color:#f92672">.</span>Tensor) <span style="color:#f92672">-&gt;</span> torch<span style="color:#f92672">.</span>Tensor:
</span></span><span style="display:flex;"><span>        _, t <span style="color:#f92672">=</span> idx<span style="color:#f92672">.</span>size()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">assert</span> (
</span></span><span style="display:flex;"><span>            t <span style="color:#f92672">&lt;=</span> self<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>block_size
</span></span><span style="display:flex;"><span>        ), <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Cannot forward sequence of length </span><span style="color:#e6db74">{</span>t<span style="color:#e6db74">}</span><span style="color:#e6db74">, block size is only </span><span style="color:#e6db74">{</span>self<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>block_size<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># forward the LLaMA model itself</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>transformer<span style="color:#f92672">.</span>wte(idx)  <span style="color:#75715e"># token embeddings of shape (b, t, n_embd)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> block <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>transformer<span style="color:#f92672">.</span>h:
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> block(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>transformer<span style="color:#f92672">.</span>ln_f(x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        logits <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lm_head(x)  <span style="color:#75715e"># (b, t, vocab_size)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> logits
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">@classmethod</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">from_name</span>(cls, name: str) <span style="color:#f92672">-&gt;</span> Self:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> cls(LLaMAConfig<span style="color:#f92672">.</span>from_name(name))
</span></span></code></pre></div><p>The LLaMA model provided is a PyTorch-based implementation. Below is an elaboration on the various components of the code:</p>
<ol>
<li><strong>Initialization</strong>:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">LLaMA</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, config: LLaMAConfig) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">assert</span> config<span style="color:#f92672">.</span>padded_vocab_size <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>config <span style="color:#f92672">=</span> config
</span></span></code></pre></div><p>Here, the model takes a configuration object, <code>LLaMAConfig</code>, during initialization. An assertion checks that the <code>padded_vocab_size</code> attribute is not <code>None</code>.</p>
<ol>
<li><strong>Model Architecture</strong>:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>lm_head <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(config<span style="color:#f92672">.</span>n_embd, config<span style="color:#f92672">.</span>padded_vocab_size, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>transformer <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleDict(
</span></span><span style="display:flex;"><span>            dict(
</span></span><span style="display:flex;"><span>                wte<span style="color:#f92672">=</span>nn<span style="color:#f92672">.</span>Embedding(config<span style="color:#f92672">.</span>padded_vocab_size, config<span style="color:#f92672">.</span>n_embd),
</span></span><span style="display:flex;"><span>                h<span style="color:#f92672">=</span>nn<span style="color:#f92672">.</span>ModuleList([Block(config) <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(config<span style="color:#f92672">.</span>n_layer)]),
</span></span><span style="display:flex;"><span>                ln_f<span style="color:#f92672">=</span>RMSNorm(config<span style="color:#f92672">.</span>n_embd),
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>        )
</span></span></code></pre></div><ul>
<li>The <code>lm_head</code> is the final linear layer of the large language model to generate the final prediction. It maps from embeddings to the vocabulary size, which is used for predicting the next word/token. So why are we doing this? This is because we want to represent the probability distribution over the vocabulary to make the prediction.</li>
<li><code>transformer</code> is a dictionary of modules, which includes:
<ul>
<li><code>wte</code>: Word Token Embedding, an embedding layer for the vocabulary. Given tokens, it will generate embeddings of size <code>config.n_embd=4096.</code></li>
<li><code>h</code>: A list of blocks, with each block being a segment of the transformer architecture. The number of blocks is defined by <code>config.n_layer</code>.</li>
<li><code>ln_f</code>: A final layer normalization, here using RMSNorm.</li>
</ul>
</li>
</ul>
<ol>
<li><strong>Weight Initialization</strong>:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_init_weights</span>(self, module: nn<span style="color:#f92672">.</span>Module) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">...</span>
</span></span></code></pre></div><p>This method initializes the weights of linear and embedding layers based on the model configuration.</p>
<ol>
<li><strong>Forward Pass</strong>:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, idx: torch<span style="color:#f92672">.</span>Tensor) <span style="color:#f92672">-&gt;</span> torch<span style="color:#f92672">.</span>Tensor:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">...</span>
</span></span></code></pre></div><p>The forward method defines how input data is processed through the model to produce an output. It processes the input tensor, passes it through the transformer blocks, and eventually through the language model head to produce the logits.<!-- raw HTML omitted --></p>
<p>Here, <code>idx</code> is the shape of <code>B,T</code>. We haven&rsquo;t converted the tokens into embedding.<!-- raw HTML omitted -->
<code>_, t = idx.size()</code>  : Get the sequence length.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">assert</span> (
</span></span><span style="display:flex;"><span>    t <span style="color:#f92672">&lt;=</span> self<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>block_size
</span></span><span style="display:flex;"><span>), <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Cannot forward sequence of length </span><span style="color:#e6db74">{</span>t<span style="color:#e6db74">}</span><span style="color:#e6db74">, block size is only </span><span style="color:#e6db74">{</span>self<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>block_size<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
</span></span></code></pre></div><p>This will check whether the input sequence is greater than the max sequence length i.e. <code>self.config.block_size</code>.</p>
<p><code>x = self.transformer.wte(idx)</code>  This will convert the input of shape <code>B,T</code> to <code>B,T,n_embd</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">for</span> block <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>transformer<span style="color:#f92672">.</span>h:
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> block(x)
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>transformer<span style="color:#f92672">.</span>ln_f(x)
</span></span></code></pre></div><p>This passes the embedding throughout n transformer blocks. I think this is the most interesting part of our entire code.
We will dive deeper into it next.</p>
<p>As discussed above <code>logits = self.lm_head(x)  # (b, t, vocab_size)</code> maps from embeddings to the vocabulary size, which is used for predicting the next word/token.</p>
<ol>
<li><strong>Load Model by Name</strong>:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>    <span style="color:#a6e22e">@classmethod</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">from_name</span>(cls, name: str) <span style="color:#f92672">-&gt;</span> Self:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> cls(LLaMAConfig<span style="color:#f92672">.</span>from_name(name))
</span></span></code></pre></div><p>This class method allows for creating a LLaMA model instance directly using a name, assuming the <code>LLaMAConfig.from_name(name)</code> can produce the necessary configuration from the provided name.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Block</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, config: LLaMAConfig) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>rms_1 <span style="color:#f92672">=</span> RMSNorm(config<span style="color:#f92672">.</span>n_embd)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>attn <span style="color:#f92672">=</span> CausalSelfAttention(config)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>rms_2 <span style="color:#f92672">=</span> RMSNorm(config<span style="color:#f92672">.</span>n_embd)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>mlp <span style="color:#f92672">=</span> MLP(config)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x: torch<span style="color:#f92672">.</span>Tensor) <span style="color:#f92672">-&gt;</span> torch<span style="color:#f92672">.</span>Tensor:
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>attn(self<span style="color:#f92672">.</span>rms_1(x))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>mlp(self<span style="color:#f92672">.</span>rms_2(x))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span></code></pre></div><p>A transformer block typically consists of self-attention mechanisms followed by feed-forward neural networks. The LLaMA model has infused some variations, including the use of RMSNorm for normalization.
<strong>Forward Pass:</strong></p>
<ul>
<li>The input tensor x is first normalized using the first RMSNorm instance.</li>
<li>Post normalization, it&rsquo;s fed into the CausalSelfAttention. The result is combined with the original tensor via a residual connection, a vital feature in deep networks for maintaining gradient flow.</li>
<li>The tensor then undergoes the second RMSNorm normalization.</li>
<li>The normalized output is processed by the MLP. As before, the resultant is added back to the tensor using a residual connection.</li>
<li>The processed tensor, rich with information, is then returned.</li>
</ul>
<p>The Block class crystallizes a singular transformer layer&rsquo;s operations within LLaMA. With the integral role of RMSNorm already understood, it becomes evident how this block combines normalization, attention, and feed-forward operations to refine the data representation at each layer. When stacked, these blocks work in concert, building upon one another to offer the powerful capabilities of the LLaMA model.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">CausalSelfAttention</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, config: LLaMAConfig) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">assert</span> config<span style="color:#f92672">.</span>n_embd <span style="color:#f92672">%</span> config<span style="color:#f92672">.</span>n_head <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># key, query, value projections for all heads, but in a batch</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>c_attn <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(config<span style="color:#f92672">.</span>n_embd, <span style="color:#ae81ff">3</span> <span style="color:#f92672">*</span> config<span style="color:#f92672">.</span>n_embd, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># output projection</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>c_proj <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(config<span style="color:#f92672">.</span>n_embd, config<span style="color:#f92672">.</span>n_embd, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>n_head <span style="color:#f92672">=</span> config<span style="color:#f92672">.</span>n_head
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>n_embd <span style="color:#f92672">=</span> config<span style="color:#f92672">.</span>n_embd
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>block_size <span style="color:#f92672">=</span> config<span style="color:#f92672">.</span>block_size
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>rope_cache: Optional[torch<span style="color:#f92672">.</span>Tensor] <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x: torch<span style="color:#f92672">.</span>Tensor) <span style="color:#f92672">-&gt;</span> torch<span style="color:#f92672">.</span>Tensor:
</span></span><span style="display:flex;"><span>        B, T, C <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>size()  <span style="color:#75715e"># batch size, sequence length, embedding dimensionality (n_embd)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># calculate query, key, values for all heads in batch and move head forward to be the batch dim</span>
</span></span><span style="display:flex;"><span>        q, k, v <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>c_attn(x)<span style="color:#f92672">.</span>split(self<span style="color:#f92672">.</span>n_embd, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        head_size <span style="color:#f92672">=</span> C <span style="color:#f92672">//</span> self<span style="color:#f92672">.</span>n_head
</span></span><span style="display:flex;"><span>        k <span style="color:#f92672">=</span> k<span style="color:#f92672">.</span>view(B, T, self<span style="color:#f92672">.</span>n_head, head_size)
</span></span><span style="display:flex;"><span>        q <span style="color:#f92672">=</span> q<span style="color:#f92672">.</span>view(B, T, self<span style="color:#f92672">.</span>n_head, head_size)
</span></span><span style="display:flex;"><span>        v <span style="color:#f92672">=</span> v<span style="color:#f92672">.</span>view(B, T, self<span style="color:#f92672">.</span>n_head, head_size)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>rope_cache <span style="color:#f92672">is</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># cache for future forward calls</span>
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>rope_cache <span style="color:#f92672">=</span> build_rope_cache(
</span></span><span style="display:flex;"><span>                seq_len<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>block_size,
</span></span><span style="display:flex;"><span>                n_elem<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>n_embd <span style="color:#f92672">//</span> self<span style="color:#f92672">.</span>n_head,
</span></span><span style="display:flex;"><span>                dtype<span style="color:#f92672">=</span>x<span style="color:#f92672">.</span>dtype,
</span></span><span style="display:flex;"><span>                device<span style="color:#f92672">=</span>x<span style="color:#f92672">.</span>device,
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        q <span style="color:#f92672">=</span> apply_rope(q, self<span style="color:#f92672">.</span>rope_cache)
</span></span><span style="display:flex;"><span>        k <span style="color:#f92672">=</span> apply_rope(k, self<span style="color:#f92672">.</span>rope_cache)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        k <span style="color:#f92672">=</span> k<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)  <span style="color:#75715e"># (B, nh, T, hs)</span>
</span></span><span style="display:flex;"><span>        q <span style="color:#f92672">=</span> q<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)  <span style="color:#75715e"># (B, nh, T, hs)</span>
</span></span><span style="display:flex;"><span>        v <span style="color:#f92672">=</span> v<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)  <span style="color:#75715e"># (B, nh, T, hs)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -&gt; (B, nh, T, T)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">#  att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">#  att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float(&#39;-inf&#39;))</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">#  att = F.softmax(att, dim=-1)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">#  y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -&gt; (B, nh, T, hs)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># efficient attention using Flash Attention CUDA kernels</span>
</span></span><span style="display:flex;"><span>        y <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>scaled_dot_product_attention(q, k, v, attn_mask<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>, dropout_p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, is_causal<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        y <span style="color:#f92672">=</span> y<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>contiguous()<span style="color:#f92672">.</span>view(B, T, C)  <span style="color:#75715e"># re-assemble all head outputs side by side</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># output projection</span>
</span></span><span style="display:flex;"><span>        y <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>c_proj(y)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> y
</span></span></code></pre></div><p>Here comes the most interesting part of our LLM. Let&rsquo;s dive into each line of code in detail.</p>
<ol>
<li><strong>Initialization:</strong></li>
</ol>
<ul>
<li>
<p>Here, we first ensure that the embedding size (n_embd) is divisible by the number of attention heads (n_head). This is necessary to equally distribute the embeddings across all heads.</p>
</li>
<li>
<p><strong>The Key, Query, Value Projections</strong>:<!-- raw HTML omitted -->
<code>self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=False)</code></p>
<p>This transformation is designed to produce key, query, and value tensors, which are essential for the attention mechanism. Normally, you&rsquo;d expect three separate linear transformations - one for each of key, query, and value. But here, they&rsquo;re combined into a single transformation for efficiency.</p>
<p>Input: config.n_embd represents the embedding size of each token in the model.
Output: 3 * config.n_embd might look a bit confusing initially, but it makes perfect sense once you understand the purpose. Since we&rsquo;re generating three different tensors (key, query, and value) and each has an embedding size of config.n_embd, the combined size is 3 * config.n_embd.</p>
</li>
</ul>
<ol>
<li><strong>Forward Pass</strong>:</li>
</ol>
<ul>
<li>The input tensor&rsquo;s dimensions are extracted, where:
<ul>
<li><code>B</code> represents the batch size.</li>
<li><code>T</code> stands for the sequence length.</li>
<li><code>C</code> denotes the embedding dimensionality.</li>
</ul>
</li>
<li>The tensor <code>x</code> undergoes the <code>c_attn</code> transformation, splitting the result into query, key, and value tensors (<code>q, k, v</code>).</li>
<li>These tensors are then reshaped for multi-head attention. Essentially, the embedding dimensionality is divided among the number of attention heads.</li>
<li>If the rope cache hasn&rsquo;t been built (i.e., <code>self.rope_cache is None</code>), it&rsquo;s constructed using the <code>build_rope_cache</code> function. As we already discussed this cache is calculated for a single head and later applied across each head, we can see that <code>n_elem=self.n_embd // self.n_head</code>, this basically means for each token in the sequence, we split the token into <code>n_head</code>, and based on the dimension of the head, we calculate the ROPE cache. This method is pretty much similar to the one we have implemented before. We will discuss some changes in this implementation later. This cache is then applied to the <code>q</code> and <code>k</code> tensors using <code>apply_rope</code> which is also pretty much similar to our previous approach.</li>
<li>The <code>q</code>, <code>k</code>, and <code>v</code> tensors are transposed to align them for the attention mechanism. Can you tell me why are we performing this transformation?
After transposing, we have the final tensor of the shape <code>(B, nh, T, hs)</code>. Now if we perform the operation <code>q @ k.t</code>, as the key is transformed, the final tensor will be of shape <code>T,T</code>. This <code>T,T</code> matrix will give us information about, given a token, what&rsquo;s the relation with other tokens. I think you got an idea of why this transformation is performed. This is done basically to get the attention matrix.</li>
<li>The main action happens in the causal self-attention mechanism. Normally, one would compute attention scores by multiplying <code>q</code> and <code>k</code>, apply a mask for causality, then use this to weigh the <code>v</code> tensor. Here, however, the mechanism uses the efficient <code>F.scaled_dot_product_attention</code> method, which leverages FlashAttention for faster attention calculations. FlashAttention is a new algorithm to speed up attention and reduce its memory footprint—without any approximation.
You can read more about FlashAttention <a href="https://www.adept.ai/blog/flashier-attention">Here</a>, <a href="https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad">Here</a>, <a href="https://crfm.stanford.edu/2023/07/17/flash2.html#:~:text=FlashAttention%20is%20an%20algorithm%20that,to%20linear%20in%20sequence%20length.">Here</a>.</li>
<li>The resultant tensor <code>y</code> is reshaped and then undergoes the output projection via the <code>c_proj</code> transformation.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">MLP</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, config: LLaMAConfig) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        hidden_dim <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span> <span style="color:#f92672">*</span> config<span style="color:#f92672">.</span>n_embd
</span></span><span style="display:flex;"><span>        n_hidden <span style="color:#f92672">=</span> int(<span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> hidden_dim <span style="color:#f92672">/</span> <span style="color:#ae81ff">3</span>)
</span></span><span style="display:flex;"><span>        n_hidden <span style="color:#f92672">=</span> find_multiple(n_hidden, <span style="color:#ae81ff">256</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>c_fc1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(config<span style="color:#f92672">.</span>n_embd, n_hidden, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>c_fc2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(config<span style="color:#f92672">.</span>n_embd, n_hidden, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>c_proj <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(n_hidden, config<span style="color:#f92672">.</span>n_embd, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x: torch<span style="color:#f92672">.</span>Tensor) <span style="color:#f92672">-&gt;</span> torch<span style="color:#f92672">.</span>Tensor:
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>silu(self<span style="color:#f92672">.</span>c_fc1(x)) <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>c_fc2(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>c_proj(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">RMSNorm</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Root Mean Square Layer Normalization.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Derived from &lt;https://github.com/bzhangGo/rmsnorm/blob/master/rmsnorm_torch.py&gt;. BSD 3-Clause License:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &lt;https://github.com/bzhangGo/rmsnorm/blob/master/LICENSE&gt;.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, size: int, dim: int <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, eps: float <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-5</span>) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>scale <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Parameter(torch<span style="color:#f92672">.</span>ones(size))
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>eps <span style="color:#f92672">=</span> eps
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>dim <span style="color:#f92672">=</span> dim
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x: torch<span style="color:#f92672">.</span>Tensor) <span style="color:#f92672">-&gt;</span> torch<span style="color:#f92672">.</span>Tensor:
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># NOTE: the original RMSNorm paper implementation is not equivalent</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># norm_x = x.norm(2, dim=self.dim, keepdim=True)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># rms_x = norm_x * d_x ** (-1. / 2)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x_normed = x / (rms_x + self.eps)</span>
</span></span><span style="display:flex;"><span>        norm_x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>mean(x <span style="color:#f92672">*</span> x, dim<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>dim, keepdim<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        x_normed <span style="color:#f92672">=</span> x <span style="color:#f92672">*</span> torch<span style="color:#f92672">.</span>rsqrt(norm_x <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>eps)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>scale <span style="color:#f92672">*</span> x_normed
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">build_rope_cache</span>(seq_len: int, n_elem: int, dtype: torch<span style="color:#f92672">.</span>dtype, device: torch<span style="color:#f92672">.</span>device, base: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">10000</span>) <span style="color:#f92672">-&gt;</span> torch<span style="color:#f92672">.</span>Tensor:
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Enhanced Transformer with Rotary Position Embedding.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Derived from: &lt;https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/&gt;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    transformers/rope/__init__.py. MIT License:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &lt;https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/license&gt;.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># $\\Theta = {\\theta_i = 10000^{\\frac{2(i-1)}{d}}, i \\in [1, 2, ..., \\frac{d}{2}]}$</span>
</span></span><span style="display:flex;"><span>    theta <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span> <span style="color:#f92672">/</span> (base <span style="color:#f92672">**</span> (torch<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, n_elem, <span style="color:#ae81ff">2</span>, dtype<span style="color:#f92672">=</span>dtype, device<span style="color:#f92672">=</span>device) <span style="color:#f92672">/</span> n_elem))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Create position indexes `[0, 1, ..., seq_len - 1]`</span>
</span></span><span style="display:flex;"><span>    seq_idx <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>arange(seq_len, dtype<span style="color:#f92672">=</span>dtype, device<span style="color:#f92672">=</span>device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Calculate the product of position index and $\\theta_i$</span>
</span></span><span style="display:flex;"><span>    idx_theta <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>outer(seq_idx, theta)<span style="color:#f92672">.</span>float()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    cache <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>stack([torch<span style="color:#f92672">.</span>cos(idx_theta), torch<span style="color:#f92672">.</span>sin(idx_theta)], dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># this is to mimic the behaviour of complex32, else we will get different results</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> dtype <span style="color:#f92672">in</span> (torch<span style="color:#f92672">.</span>float16, torch<span style="color:#f92672">.</span>bfloat16, torch<span style="color:#f92672">.</span>int8):
</span></span><span style="display:flex;"><span>        cache <span style="color:#f92672">=</span> cache<span style="color:#f92672">.</span>half()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> cache
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">apply_rope</span>(x: torch<span style="color:#f92672">.</span>Tensor, rope_cache: torch<span style="color:#f92672">.</span>Tensor) <span style="color:#f92672">-&gt;</span> torch<span style="color:#f92672">.</span>Tensor:
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># truncate to support variable sizes</span>
</span></span><span style="display:flex;"><span>    T <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    rope_cache <span style="color:#f92672">=</span> rope_cache[:T]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># cast because the reference does</span>
</span></span><span style="display:flex;"><span>    xshaped <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>float()<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">*</span>x<span style="color:#f92672">.</span>shape[:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>], <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># uta hami lea cos ra sine lai 2 ota use garinthiyo. Like x_rope, neg_half_x calculate gareko.</span>
</span></span><span style="display:flex;"><span>    rope_cache <span style="color:#f92672">=</span> rope_cache<span style="color:#f92672">.</span>view(<span style="color:#ae81ff">1</span>, xshaped<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">1</span>), <span style="color:#ae81ff">1</span>, xshaped<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">3</span>), <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>    x_out2 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>stack(
</span></span><span style="display:flex;"><span>        [
</span></span><span style="display:flex;"><span>            xshaped[<span style="color:#f92672">...</span>, <span style="color:#ae81ff">0</span>] <span style="color:#f92672">*</span> rope_cache[<span style="color:#f92672">...</span>, <span style="color:#ae81ff">0</span>] <span style="color:#f92672">-</span> xshaped[<span style="color:#f92672">...</span>, <span style="color:#ae81ff">1</span>] <span style="color:#f92672">*</span> rope_cache[<span style="color:#f92672">...</span>, <span style="color:#ae81ff">1</span>],
</span></span><span style="display:flex;"><span>            xshaped[<span style="color:#f92672">...</span>, <span style="color:#ae81ff">1</span>] <span style="color:#f92672">*</span> rope_cache[<span style="color:#f92672">...</span>, <span style="color:#ae81ff">0</span>] <span style="color:#f92672">+</span> xshaped[<span style="color:#f92672">...</span>, <span style="color:#ae81ff">0</span>] <span style="color:#f92672">*</span> rope_cache[<span style="color:#f92672">...</span>, <span style="color:#ae81ff">1</span>],
</span></span><span style="display:flex;"><span>        ],
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    x_out2 <span style="color:#f92672">=</span> x_out2<span style="color:#f92672">.</span>flatten(<span style="color:#ae81ff">3</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> x_out2<span style="color:#f92672">.</span>type_as(x)
</span></span></code></pre></div><p>The <code>build_rope_cache</code> function is almost identical to <code>build_cache</code> we implemented. Here the cos and sin values are calculated beforehand. Also, <code>build_rope_cache</code> has specific handling for certain data types like torch.float16, torch.bfloat16, and torch.int8, where it casts the computed cache to half precision.
<code>build_cache</code> doesn&rsquo;t handle data types in this manner.</p>
<p>The <code>apply_rope</code> also applies RoPE cache to query and key. But there is a slight difference in how the transformation is applied. I&rsquo;ll explain what is happening in this method in detail.</p>
<p>We have two tensors: <code>x</code> and <code>rope_cache</code>.</p>
<p>Let&rsquo;s assume <code>x</code> is a 4D tensor with shape <code>(1, 4, 2, 4)</code> and <code>rope_cache</code> is a 4D tensor with shape <code>(4, 2, 2)</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> tensor([[[[ <span style="color:#ae81ff">0</span>,  <span style="color:#ae81ff">1</span>,  <span style="color:#ae81ff">2</span>,  <span style="color:#ae81ff">3</span>],
</span></span><span style="display:flex;"><span>          [ <span style="color:#ae81ff">4</span>,  <span style="color:#ae81ff">5</span>,  <span style="color:#ae81ff">6</span>,  <span style="color:#ae81ff">7</span>]],
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>         [[ <span style="color:#ae81ff">8</span>,  <span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">11</span>],
</span></span><span style="display:flex;"><span>          [<span style="color:#ae81ff">12</span>, <span style="color:#ae81ff">13</span>, <span style="color:#ae81ff">14</span>, <span style="color:#ae81ff">15</span>]],
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>         [[<span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">17</span>, <span style="color:#ae81ff">18</span>, <span style="color:#ae81ff">19</span>],
</span></span><span style="display:flex;"><span>          [<span style="color:#ae81ff">20</span>, <span style="color:#ae81ff">21</span>, <span style="color:#ae81ff">22</span>, <span style="color:#ae81ff">23</span>]],
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>         [[<span style="color:#ae81ff">24</span>, <span style="color:#ae81ff">25</span>, <span style="color:#ae81ff">26</span>, <span style="color:#ae81ff">27</span>],
</span></span><span style="display:flex;"><span>          [<span style="color:#ae81ff">28</span>, <span style="color:#ae81ff">29</span>, <span style="color:#ae81ff">30</span>, <span style="color:#ae81ff">31</span>]]]])
</span></span></code></pre></div><p><strong>Step 1 :</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>T <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">1</span>)
</span></span></code></pre></div><p>Here, <code>T</code> is simply the size of the second dimension of <code>x</code>, which is 4.</p>
<p><strong>Step 2 :</strong></p>
<p>Next, We resize <code>rope_cache</code> to match the size <code>T</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>rope_cache <span style="color:#f92672">=</span> rope_cache[:T]
</span></span></code></pre></div><p>This step is redundant because <code>rope_cache</code> already has a size of 4 in its first dimension.</p>
<p><strong>Step 3:</strong></p>
<p>Then, you reshape <code>x</code> to make its last dimension into two parts:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>xshaped <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>float()<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">*</span>x<span style="color:#f92672">.</span>shape[:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>], <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)
</span></span></code></pre></div><p>This breaks down as:</p>
<ol>
<li>Convert x into float: <code>x.float()</code></li>
<li>Reshape it: For our tensor, this converts it from shape <code>(1, 4, 2, 4)</code> to <code>(1, 4, 4, 2)</code>.</li>
</ol>
<p>Given the <strong><code>xshaped</code></strong> tensor structure you provided, we can see that its shape is (1, 4, 2, 2, 2). That means you have:</p>
<ul>
<li>1 batch (the outermost dimension)</li>
<li>4 channels</li>
<li>2x2 spatial dimensions (height x width)</li>
<li>2 values for each spatial position (the innermost dimension)</li>
</ul>
<p>For instance, before reshaping, the first 2x4 matrix in <code>x</code> is:</p>
<pre tabindex="0"><code>0,  1,  2,  3
4,  5,  6,  7
</code></pre><p>After reshaping, the first 4x2 matrix in <code>xshaped</code> would be:</p>
<pre tabindex="0"><code>0,  1
2,  3
4,  5
6,  7
</code></pre><p>Next, you are reshaping the <code>rope_cache</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>rope_cache <span style="color:#f92672">=</span> rope_cache<span style="color:#f92672">.</span>view(<span style="color:#ae81ff">1</span>, xshaped<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">1</span>), <span style="color:#ae81ff">1</span>, xshaped<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">3</span>), <span style="color:#ae81ff">2</span>)
</span></span></code></pre></div><p>This converts <code>rope_cache</code> from shape <code>(4, 2, 2)</code> to <code>(1, 4, 1, 2, 2)</code>. This reshaping is done to align the dimensions of <code>rope_cache</code> with <code>xshaped</code> for broadcasting during the subsequent operations.</p>
<p><strong>Step 3:</strong></p>
<p>Then, you perform element-wise multiplication and subtraction/addition between the reshaped <code>x</code> and <code>rope_cache</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>x_out2 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>stack(
</span></span><span style="display:flex;"><span>    [
</span></span><span style="display:flex;"><span>        xshaped[<span style="color:#f92672">...</span>, <span style="color:#ae81ff">0</span>] <span style="color:#f92672">*</span> rope_cache[<span style="color:#f92672">...</span>, <span style="color:#ae81ff">0</span>] <span style="color:#f92672">-</span> xshaped[<span style="color:#f92672">...</span>, <span style="color:#ae81ff">1</span>] <span style="color:#f92672">*</span> rope_cache[<span style="color:#f92672">...</span>, <span style="color:#ae81ff">1</span>],
</span></span><span style="display:flex;"><span>        xshaped[<span style="color:#f92672">...</span>, <span style="color:#ae81ff">1</span>] <span style="color:#f92672">*</span> rope_cache[<span style="color:#f92672">...</span>, <span style="color:#ae81ff">0</span>] <span style="color:#f92672">+</span> xshaped[<span style="color:#f92672">...</span>, <span style="color:#ae81ff">0</span>] <span style="color:#f92672">*</span> rope_cache[<span style="color:#f92672">...</span>, <span style="color:#ae81ff">1</span>],
</span></span><span style="display:flex;"><span>    ],
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>This is similar to performing rotation using sine and cosine values from <code>rope_cache</code>. The resulting tensor <code>x_out2</code> has the same shape as <code>xshaped</code>, which is <code>(1, 4, 4, 2)</code>. Rotation operation in <strong><code>torch.stack</code></strong> would work element-wise over the tensors. This means that for each position in <strong><code>xshaped</code></strong>, it uses the corresponding position in <strong><code>rope_cache</code></strong> for the rotation calculation.</p>
<p><strong>Breakdown:</strong></p>
<p>Given a 2D rotation matrix:</p>
<p>$$
R(\theta) = \begin{bmatrix} \cos(\theta) &amp; -\sin(\theta) \ \sin(\theta) &amp; \cos(\theta) \end{bmatrix}
$$</p>
<p>When you multiply this rotation matrix with a 2D vector $([x, y]^T)$, you get:</p>
<p>$$
R(\theta) \cdot \begin{bmatrix} x \ y \end{bmatrix} = \begin{bmatrix} x\cos(\theta) - y\sin(\theta) \ x\sin(\theta) + y\cos(\theta) \end{bmatrix}
$$</p>
<p>Now, let&rsquo;s connect this to the operations in the code:</p>
<ul>
<li>The first component of the output:
$x&rsquo; = x\cos(\theta) - y\sin(\theta)$ is given by:
<code>xshaped[..., 0] * rope_cache[..., 0] - xshaped[..., 1] * rope_cache[..., 1]</code></li>
</ul>
<p>Where:</p>
<ul>
<li><code>xshaped[..., 0]</code> corresponds to the x component (or the first value) of our vector.</li>
<li><code>xshaped[..., 1]</code> corresponds to the y component (or the second value) of our vector.</li>
<li><code>rope_cache[..., 0]</code> is the cosine of the rotation angle.</li>
<li><code>rope_cache[..., 1]</code> is the sine of the rotation angle.</li>
<li>The second component of the output:
$y&rsquo; = x\sin(\theta) + y\cos(\theta)$ is given by:
<code>xshaped[..., 1] * rope_cache[..., 0] + xshaped[..., 0] * rope_cache[..., 1]</code></li>
</ul>
<p>The code is essentially applying this rotation to every pair of values in the tensor <code>xshaped</code> using the angles specified in <code>rope_cache</code>.</p>
<p>The <code>torch.stack(..., -1)</code> at the end stacks these computed values along the last dimension. After this operation, for every pair of x and y values in the original <code>xshaped</code>, you have their rotated counterparts stacked together in the resulting tensor.</p>
<h2 id="inference"><strong>Inference</strong><a hidden class="anchor" aria-hidden="true" href="#inference">#</a></h2>
<p>For inference, we will be using the pipeline provided by the lit-lama repo. It provides some helpful classes that can potentially speed up the loading and initialization of large models, especially when only parts of the model need to be accessed or when specific tensor initializations are desired. The code also seems to handle some advanced features like quantization and lazy loading of tensors.</p>
<p>let&rsquo;s break down these classes:</p>
<ol>
<li>
<p><strong><code>EmptyInitOnDevice</code> class</strong>:</p>
<p>This class is a context manager that changes the behavior of tensor initialization to create tensors with uninitialized memory (or &ldquo;empty tensors&rdquo;). Additionally, it can set specific devices and data types for tensor initialization and supports specific quantization modes. When this context is active, tensors are initialized without actually assigning them any initial values, making the initialization process faster in some scenarios.</p>
</li>
<li>
<p><strong><code>NotYetLoadedTensor</code> class</strong>:</p>
<p>Represents a tensor that has not yet been loaded into memory. It is essentially a placeholder that can be transformed into an actual tensor when accessed or used in computations. This class can be especially useful when dealing with large datasets or models, as it allows for lazy loading of data, only loading tensors into memory when they&rsquo;re actually needed.</p>
</li>
<li>
<p><strong><code>LazyLoadingUnpickler</code> class</strong>:</p>
<p>Custom unpickler for lazy loading. Pickling is the process of converting a Python object into a byte stream, and unpickling is the reverse operation. The idea here is to load tensors and related objects from the pickled format only when they&rsquo;re actually accessed or used.</p>
</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> sys
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> time
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> warnings
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pathlib <span style="color:#f92672">import</span> Path
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> typing <span style="color:#f92672">import</span> Optional
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> lightning <span style="color:#66d9ef">as</span> L
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> tokenizer <span style="color:#f92672">import</span>  Tokenizer
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> utils <span style="color:#f92672">import</span> EmptyInitOnDevice, lazy_load, llama_model_lookup
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a6e22e">@torch.no_grad</span>()
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">generate</span>(
</span></span><span style="display:flex;"><span>    model: torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Module,
</span></span><span style="display:flex;"><span>    idx: torch<span style="color:#f92672">.</span>Tensor,
</span></span><span style="display:flex;"><span>    max_new_tokens: int,
</span></span><span style="display:flex;"><span>    max_seq_length: int,
</span></span><span style="display:flex;"><span>    temperature: float <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span>,
</span></span><span style="display:flex;"><span>    top_k: Optional[int] <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>,
</span></span><span style="display:flex;"><span>    eos_id: Optional[int] <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>,
</span></span><span style="display:flex;"><span>) <span style="color:#f92672">-&gt;</span> torch<span style="color:#f92672">.</span>Tensor:
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Takes a conditioning sequence (prompt) as input and continues to generate as many tokens as requested.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    The implementation of this function is modified from A. Karpathy&#39;s nanoGPT.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Args:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        model: The model to use.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        idx: Tensor of shape (T) with indices of the prompt sequence.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        max_new_tokens: The number of new tokens to generate.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        max_seq_length: The maximum sequence length allowed.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        temperature: Scales the predicted logits by 1 / temperature
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        top_k: If specified, only sample among the tokens with the k highest probabilities
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        eos_id: If specified, stop generating any more token once the &lt;eos&gt; token is triggered
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># create an empty tensor of the expected final shape and fill in the current tokens</span>
</span></span><span style="display:flex;"><span>    T <span style="color:#f92672">=</span> idx<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>    T_new <span style="color:#f92672">=</span> T <span style="color:#f92672">+</span> max_new_tokens
</span></span><span style="display:flex;"><span>    empty <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>empty(T_new, dtype<span style="color:#f92672">=</span>idx<span style="color:#f92672">.</span>dtype, device<span style="color:#f92672">=</span>idx<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>    empty[:T] <span style="color:#f92672">=</span> idx
</span></span><span style="display:flex;"><span>    idx <span style="color:#f92672">=</span> empty
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># generate max_new_tokens tokens</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> range(T, T_new):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># ignore the not-filled-yet tokens</span>
</span></span><span style="display:flex;"><span>        idx_cond <span style="color:#f92672">=</span> idx[:t]
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># if the sequence context is growing too long we must crop it at max_seq_length</span>
</span></span><span style="display:flex;"><span>        idx_cond <span style="color:#f92672">=</span> idx_cond <span style="color:#66d9ef">if</span> T <span style="color:#f92672">&lt;=</span> max_seq_length <span style="color:#66d9ef">else</span> idx_cond[<span style="color:#f92672">-</span>max_seq_length:]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># forward</span>
</span></span><span style="display:flex;"><span>        logits <span style="color:#f92672">=</span> model(idx_cond<span style="color:#f92672">.</span>view(<span style="color:#ae81ff">1</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>        logits <span style="color:#f92672">=</span> logits[<span style="color:#ae81ff">0</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>] <span style="color:#f92672">/</span> temperature
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># optionally crop the logits to only the top k options</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> top_k <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            v, _ <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>topk(logits, min(top_k, logits<span style="color:#f92672">.</span>size(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)))
</span></span><span style="display:flex;"><span>            logits[logits <span style="color:#f92672">&lt;</span> v[[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]]] <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>float(<span style="color:#e6db74">&#34;Inf&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        probs <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>functional<span style="color:#f92672">.</span>softmax(logits, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        idx_next <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>multinomial(probs, num_samples<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># concatenate the new generation</span>
</span></span><span style="display:flex;"><span>        idx[t] <span style="color:#f92672">=</span> idx_next
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># if &lt;eos&gt; token is triggered, return the output (stop generation)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> idx_next <span style="color:#f92672">==</span> eos_id:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> idx[:t <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>]  <span style="color:#75715e"># include the EOS token</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> idx
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">main</span>(
</span></span><span style="display:flex;"><span>    prompt: str <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Hello, my name is&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">*</span>,
</span></span><span style="display:flex;"><span>    num_samples: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>,
</span></span><span style="display:flex;"><span>    max_new_tokens: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">50</span>,
</span></span><span style="display:flex;"><span>    top_k: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">200</span>,
</span></span><span style="display:flex;"><span>    temperature: float <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.8</span>,
</span></span><span style="display:flex;"><span>    checkpoint_path: Optional[Path] <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>,
</span></span><span style="display:flex;"><span>    tokenizer_path: Optional[Path] <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>,
</span></span><span style="display:flex;"><span>    quantize: Optional[str] <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>,
</span></span><span style="display:flex;"><span>) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Generates text samples based on a pre-trained LLaMA model and tokenizer.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Args:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        prompt: The prompt string to use for generating the samples.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        num_samples: The number of text samples to generate.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        max_new_tokens: The number of generation steps to take.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        top_k: The number of top most probable tokens to consider in the sampling process.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        temperature: A value controlling the randomness of the sampling process. Higher values result in more random
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            samples.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        checkpoint_path: The checkpoint path to load.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        tokenizer_path: The tokenizer path to load.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        quantize: Whether to quantize the model and using which method:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            ``&#34;llm.int8&#34;``: LLM.int8() mode,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            ``&#34;gptq.int4&#34;``: GPTQ 4-bit mode.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> checkpoint_path:
</span></span><span style="display:flex;"><span>        checkpoint_path <span style="color:#f92672">=</span> Path(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;./checkpoints/lit-llama/7B/lit-llama.pth&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> tokenizer_path:
</span></span><span style="display:flex;"><span>        tokenizer_path <span style="color:#f92672">=</span> Path(<span style="color:#e6db74">&#34;./checkpoints/lit-llama/tokenizer.model&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">assert</span> checkpoint_path<span style="color:#f92672">.</span>is_file(), checkpoint_path
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">assert</span> tokenizer_path<span style="color:#f92672">.</span>is_file(), tokenizer_path
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    fabric <span style="color:#f92672">=</span> L<span style="color:#f92672">.</span>Fabric(devices<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    dtype <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>bfloat16 <span style="color:#66d9ef">if</span> fabric<span style="color:#f92672">.</span>device<span style="color:#f92672">.</span>type <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;cuda&#34;</span> <span style="color:#f92672">and</span> torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>is_bf16_supported() <span style="color:#66d9ef">else</span> torch<span style="color:#f92672">.</span>float32
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;Loading model ...&#34;</span>, file<span style="color:#f92672">=</span>sys<span style="color:#f92672">.</span>stderr)
</span></span><span style="display:flex;"><span>    t0 <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>time()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> lazy_load(checkpoint_path) <span style="color:#66d9ef">as</span> checkpoint:
</span></span><span style="display:flex;"><span>        name <span style="color:#f92672">=</span> llama_model_lookup(checkpoint)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> EmptyInitOnDevice(
</span></span><span style="display:flex;"><span>                device<span style="color:#f92672">=</span>fabric<span style="color:#f92672">.</span>device, dtype<span style="color:#f92672">=</span>dtype, quantization_mode<span style="color:#f92672">=</span>quantize
</span></span><span style="display:flex;"><span>        ):
</span></span><span style="display:flex;"><span>            model <span style="color:#f92672">=</span> LLaMA<span style="color:#f92672">.</span>from_name(name)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        model<span style="color:#f92672">.</span>load_state_dict(checkpoint)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Time to load model: </span><span style="color:#e6db74">{</span>time<span style="color:#f92672">.</span>time() <span style="color:#f92672">-</span> t0<span style="color:#e6db74">:</span><span style="color:#e6db74">.02f</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> seconds.&#34;</span>, file<span style="color:#f92672">=</span>sys<span style="color:#f92672">.</span>stderr)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>eval()
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> fabric<span style="color:#f92672">.</span>setup_module(model)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    tokenizer <span style="color:#f92672">=</span> Tokenizer(tokenizer_path)
</span></span><span style="display:flex;"><span>    encoded_prompt <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>encode(prompt, bos<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, eos<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>, device<span style="color:#f92672">=</span>fabric<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    L<span style="color:#f92672">.</span>seed_everything(<span style="color:#ae81ff">1234</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(num_samples):
</span></span><span style="display:flex;"><span>        t0 <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>perf_counter()
</span></span><span style="display:flex;"><span>        y <span style="color:#f92672">=</span> generate(
</span></span><span style="display:flex;"><span>            model,
</span></span><span style="display:flex;"><span>            encoded_prompt,
</span></span><span style="display:flex;"><span>            max_new_tokens,
</span></span><span style="display:flex;"><span>            model<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>block_size,  <span style="color:#75715e"># type: ignore[union-attr,arg-type]</span>
</span></span><span style="display:flex;"><span>            temperature<span style="color:#f92672">=</span>temperature,
</span></span><span style="display:flex;"><span>            top_k<span style="color:#f92672">=</span>top_k,
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        t <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>perf_counter() <span style="color:#f92672">-</span> t0
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">n</span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">n&#39;</span>)
</span></span><span style="display:flex;"><span>        print(tokenizer<span style="color:#f92672">.</span>decode(y))
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">n</span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">n&#39;</span>)
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Time for inference </span><span style="color:#e6db74">{</span>i <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span><span style="color:#e6db74">}</span><span style="color:#e6db74">: </span><span style="color:#e6db74">{</span>t<span style="color:#e6db74">:</span><span style="color:#e6db74">.02f</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> sec total, </span><span style="color:#e6db74">{</span>max_new_tokens <span style="color:#f92672">/</span> t<span style="color:#e6db74">:</span><span style="color:#e6db74">.02f</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> tokens/sec&#34;</span>, file<span style="color:#f92672">=</span>sys<span style="color:#f92672">.</span>stderr)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> fabric<span style="color:#f92672">.</span>device<span style="color:#f92672">.</span>type <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;cuda&#34;</span>:
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Memory used: </span><span style="color:#e6db74">{</span>torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>max_memory_reserved() <span style="color:#f92672">/</span> <span style="color:#ae81ff">1e9</span><span style="color:#e6db74">:</span><span style="color:#e6db74">.02f</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> GB&#34;</span>, file<span style="color:#f92672">=</span>sys<span style="color:#f92672">.</span>stderr)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>main(<span style="color:#e6db74">&#34;Artificial Intelligence is the&#34;</span>)
</span></span></code></pre></div><pre tabindex="0"><code>Loading model ...
Time to load model: 17.45 seconds.
You are using a CUDA device (&#39;NVIDIA GeForce RTX 3090&#39;) that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision(&#39;medium&#39; | &#39;high&#39;)` which will trade-off precision for performance. For more details, read &lt;https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision&gt;
Global seed set to 1234

Artificial Intelligence is the ability of a computer to imitate intelligent behaviour without being programmed, such as learning in a self-directed way to do a specific task, and then not just repeating the task, but improving itself. This is different from Traditional Artificial Intelligence which is any

Time for inference 1: 1.41 sec total, 35.55 tokens/sec
Memory used: 13.52 GB
</code></pre><h2 id="references"><strong>References</strong><a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<h3 id="llama">LLaMa<a hidden class="anchor" aria-hidden="true" href="#llama">#</a></h3>
<hr>
<ul>
<li>Original Paper : <a href="https://arxiv.org/abs/2302.13971">https://arxiv.org/abs/2302.13971</a></li>
<li><a href="https://akgeni.medium.com/llama-concepts-explained-summary-a87f0bd61964">https://akgeni.medium.com/llama-concepts-explained-summary-a87f0bd61964</a></li>
<li><a href="https://kikaben.com/llama-2023-02/">https://kikaben.com/llama-2023-02/</a></li>
<li><a href="https://cameronrwolfe.substack.com/p/llama-llms-for-everyone">https://cameronrwolfe.substack.com/p/llama-llms-for-everyone</a></li>
<li><a href="https://vinija.ai/models/LLaMA/">https://vinija.ai/models/LLaMA/</a></li>
</ul>
<h3 id="implementation-code">Implementation code<a hidden class="anchor" aria-hidden="true" href="#implementation-code">#</a></h3>
<hr>
<ul>
<li><a href="https://github.com/Lightning-AI/lit-llama">https://github.com/Lightning-AI/lit-llama</a></li>
</ul>
<h3 id="rope">ROPE<a hidden class="anchor" aria-hidden="true" href="#rope">#</a></h3>
<hr>
<ul>
<li>Original Paper : <a href="https://arxiv.org/pdf/2104.09864.pdf">https://arxiv.org/pdf/2104.09864.pdf</a></li>
<li><a href="https://blog.eleuther.ai/rotary-embeddings/">https://blog.eleuther.ai/rotary-embeddings/</a></li>
<li><a href="https://nn.labml.ai/transformers/rope/index.html#:~:text=This%20is%20an%20implementation%20of,incorporates%20explicit%20relative%20position%20dependency">https://nn.labml.ai/transformers/rope/index.html#:~:text=This%20is%20an%20implementation%20of,incorporates%20explicit%20relative%20position%20dependency</a>.</li>
<li><a href="https://serp.ai/rotary-position-embedding/">https://serp.ai/rotary-position-embedding/</a></li>
<li><a href="https://medium.com/@andrew_johnson_4/understanding-rotary-position-embedding-a-key-concept-in-transformer-models-5275c6bda6d0">https://medium.com/@andrew_johnson_4/understanding-rotary-position-embedding-a-key-concept-in-transformer-models-5275c6bda6d0</a></li>
<li><a href="https://github.com/lucidrains/rotary-embedding-torch">https://github.com/lucidrains/rotary-embedding-torch</a></li>
<li><a href="http://krasserm.github.io/2022/12/13/rotary-position-embedding/">http://krasserm.github.io/2022/12/13/rotary-position-embedding/</a></li>
</ul>
<p><em>YouTube</em></p>
<ul>
<li><a href="https://youtu.be/YMcwsLGU_U8">https://youtu.be/YMcwsLGU_U8</a></li>
<li><a href="https://youtu.be/GQPOtyITy54">https://youtu.be/GQPOtyITy54</a></li>
</ul>
<h3 id="swish-1">Swish<a hidden class="anchor" aria-hidden="true" href="#swish-1">#</a></h3>
<hr>
<ul>
<li><a href="https://blog.paperspace.com/swish-activation-function/#:~:text=Simply%20put,%20Swish%20is%20an,Function%20Approximation%20in%20Reinforcement%20Learning%22">https://blog.paperspace.com/swish-activation-function/#:~:text=Simply%20put,%20Swish%20is%20an,Function%20Approximation%20in%20Reinforcement%20Learning%22</a></li>
<li><a href="https://medium.com/@neuralnets/swish-activation-function-by-google-53e1ea86f820#:~:text=Swish%20is%20a%20smooth,%20non,that%20actually%20creates%20the%20difference">https://medium.com/@neuralnets/swish-activation-function-by-google-53e1ea86f820#:~:text=Swish%20is%20a%20smooth,%20non,that%20actually%20creates%20the%20difference</a></li>
</ul>
<h3 id="glu">GLU<a hidden class="anchor" aria-hidden="true" href="#glu">#</a></h3>
<hr>
<ul>
<li><a href="https://medium.com/deeplearningmadeeasy/glu-gated-linear-unit-21e71cd52081">https://medium.com/deeplearningmadeeasy/glu-gated-linear-unit-21e71cd52081</a></li>
</ul>
<h3 id="rmsnorm">RMSNorm<a hidden class="anchor" aria-hidden="true" href="#rmsnorm">#</a></h3>
<hr>
<ul>
<li><a href="https://akgeni.medium.com/llama-concepts-explained-summary-a87f0bd61964">https://akgeni.medium.com/llama-concepts-explained-summary-a87f0bd61964</a></li>
<li><a href="https://kikaben.com/llama-2023-02/">https://kikaben.com/llama-2023-02/</a></li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://R4j4n.github.io/blogs/tags/natural-language-processing/">Natural Language Processing</a></li>
      <li><a href="https://R4j4n.github.io/blogs/tags/pytorch/">PyTorch</a></li>
      <li><a href="https://R4j4n.github.io/blogs/tags/large-language-model/">Large Language Model</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://R4j4n.github.io/blogs/posts/vit/">
    <span class="title">« Prev</span>
    <br>
    <span>Vision Transformer (ViT)</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://R4j4n.github.io/blogs/">Rajan Ghimire</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
